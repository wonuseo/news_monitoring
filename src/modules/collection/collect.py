"""
collect.py - Naver News Collection Module
ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ í†µí•´ ë¸Œëœë“œ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘
"""

import time
import requests
import pandas as pd
from typing import List, Dict
from pathlib import Path


# ë¸Œëœë“œ ì„¤ì • (config/brands.yamlì—ì„œ ë¡œë“œ, ì—†ìœ¼ë©´ í•˜ë“œì½”ë”© fallback)
def _load_brands():
    try:
        from src.utils.config import load_config
        cfg = load_config("brands")
        return cfg.get("our_brands", []), cfg.get("competitors", [])
    except Exception:
        return (
            ["ë¡¯ë°í˜¸í…”", "í˜¸í…”ë¡¯ë°", "L7", "ì‹œê·¸ë‹ˆì—˜"],
            ["ì‹ ë¼í˜¸í…”", "ì¡°ì„ í˜¸í…”"],
        )

OUR_BRANDS, COMPETITORS = _load_brands()

NAVER_API_URL = "https://openapi.naver.com/v1/search/news.json"


def fetch_naver(query: str, display: int, start: int, sort: str,
                naver_id: str, naver_secret: str) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ

    Args:
        query: ê²€ìƒ‰ì–´
        display: ê°€ì ¸ì˜¬ ê°œìˆ˜
        start: ì‹œì‘ ì¸ë±ìŠ¤
        sort: ì •ë ¬ ë°©ì‹ (date/sim)
        naver_id: ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ ID
        naver_secret: ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ Secret

    Returns:
        ê¸°ì‚¬ ëª©ë¡ (dict ë¦¬ìŠ¤íŠ¸)
    """
    headers = {
        "X-Naver-Client-Id": naver_id,
        "X-Naver-Client-Secret": naver_secret
    }
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }

    try:
        response = requests.get(NAVER_API_URL, headers=headers, params=params, timeout=10)

        if response.status_code == 401:
            raise RuntimeError("ë„¤ì´ë²„ API ì¸ì¦ ì‹¤íŒ¨ (401). í´ë¼ì´ì–¸íŠ¸ ID/Secretì„ í™•ì¸í•˜ì„¸ìš”.")
        elif response.status_code == 429:
            raise RuntimeError("ë„¤ì´ë²„ API ìš”ì²­ í•œë„ ì´ˆê³¼ (429). ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        elif response.status_code >= 500:
            raise RuntimeError(f"ë„¤ì´ë²„ API ì„œë²„ ì˜¤ë¥˜ ({response.status_code})")

        response.raise_for_status()
        data = response.json()
        return data.get("items", [])

    except requests.exceptions.RequestException as e:
        print(f"âš ï¸  '{query}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def fetch_naver_paginated(query: str, display: int, max_pages: int, sort: str,
                          naver_id: str, naver_secret: str, existing_links: set = None) -> List[Dict]:
    """
    í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ì—¬ëŸ¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ìˆ˜ì§‘

    ì¤‘ë³µ ë§í¬ëŠ” skipí•˜ê³  í˜ì´ì§€ ëê¹Œì§€ ê²€ì‚¬
    í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨: (A) í•´ë‹¹ í˜ì´ì§€ ìƒˆ ê¸°ì‚¬ 0ê°œ ë˜ëŠ” (B) ì—°ì† Ní˜ì´ì§€ ìƒˆ ê¸°ì‚¬ 0ê°œ

    Args:
        query: ê²€ìƒ‰ì–´
        display: í•œ í˜ì´ì§€ë‹¹ ê°€ì ¸ì˜¬ ê°œìˆ˜ (ê¸°ë³¸: 100)
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¶Œì¥: 9 for 90% ì¿¼í„° ì•ˆì „ ë§ˆì§„)
        sort: ì •ë ¬ ë°©ì‹ (date/sim)
        naver_id: ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ ID
        naver_secret: ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ Secret
        existing_links: ê¸°ì¡´ ë§í¬ set (ì¤‘ë³µ ì²´í¬ìš©)

    Returns:
        ëª¨ë“  í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ (dict ë¦¬ìŠ¤íŠ¸)
    """
    all_items = []
    existing_links = existing_links or set()
    consecutive_empty_pages = 0  # ì—°ì† ë¹ˆ í˜ì´ì§€ ì¹´ìš´í„°
    max_consecutive_empty = 3  # ì—°ì† ë¹ˆ í˜ì´ì§€ í—ˆìš© í•œê³„

    for page in range(1, max_pages + 1):
        start = (page - 1) * display + 1
        print(f"    Page {page}/{max_pages}...", end="", flush=True)

        items = fetch_naver(query, display, start, sort, naver_id, naver_secret)

        if not items:
            print(" (no more articles)")
            break

        # ì¤‘ë³µ ì²´í¬: ì¤‘ë³µì€ skip, ìƒˆ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
        new_items = []
        dup_count = 0
        for item in items:
            item_link = item.get("link", "")
            if item_link in existing_links:
                dup_count += 1
                continue  # ì¤‘ë³µì€ skip, ë‹¤ìŒ ê¸°ì‚¬ ê²€ì‚¬
            new_items.append(item)
            existing_links.add(item_link)

        all_items.extend(new_items)
        new_count = len(new_items)

        # ë¡œê·¸ ì¶œë ¥
        print(f" {new_count} new, {dup_count} dup", end="")

        # ì¤‘ë‹¨ ì¡°ê±´ A: í•´ë‹¹ í˜ì´ì§€ì—ì„œ ìƒˆ ê¸°ì‚¬ 0ê°œ
        if new_count == 0:
            consecutive_empty_pages += 1
            print(f" (ìƒˆ ê¸°ì‚¬ 0ê°œ, ì—°ì† {consecutive_empty_pages}/{max_consecutive_empty})")

            # ì¤‘ë‹¨ ì¡°ê±´ B: ì—°ì† Ní˜ì´ì§€ ìƒˆ ê¸°ì‚¬ 0ê°œ
            if consecutive_empty_pages >= max_consecutive_empty:
                print(f"    âš ï¸  ì—°ì† {consecutive_empty_pages}í˜ì´ì§€ ìƒˆ ê¸°ì‚¬ ì—†ìŒ â†’ ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
        else:
            consecutive_empty_pages = 0  # ìƒˆ ê¸°ì‚¬ ìˆìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹
            print()

        # í˜ì´ì§€ ê°„ ìš”ì²­ ì‚¬ì´ì— ë”œë ˆì´ (Rate limiting)
        if page < max_pages:
            time.sleep(0.2)

    return all_items


def collect_all_news(brands: List[str], competitors: List[str],
                     display: int, max_pages: int, sort: str,
                     naver_id: str, naver_secret: str,
                     existing_links: set = None,
                     spreadsheet = None) -> pd.DataFrame:
    """
    ëª¨ë“  ë¸Œëœë“œì™€ ê²½ìŸì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘ (ê° ë¸Œëœë“œë§ˆë‹¤ ì¦‰ì‹œ Sheets ë™ê¸°í™”)

    Args:
        existing_links: ê¸°ì¡´ ë§í¬ set (ì¤‘ë³µ ì²´í¬ìš©, Sheetsì—ì„œ ë¯¸ë¦¬ ë¡œë“œ)
        spreadsheet: Google Sheets ê°ì²´ (ì¦‰ì‹œ ë™ê¸°í™”ìš©, ì„ íƒì‚¬í•­)

    Returns:
        DataFrame with columns: query, group, title, description, pubDate, originallink, link
    """
    all_rows = []

    existing_links = existing_links or set()

    # ìš°ë¦¬ ë¸Œëœë“œ ìˆ˜ì§‘
    print(f"ğŸ“° ìš°ë¦¬ ë¸Œëœë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ {max_pages} í˜ì´ì§€)...")
    for query in brands:
        print(f"  â†’ {query}")
        items = fetch_naver_paginated(query, display, max_pages, sort, naver_id, naver_secret, existing_links)
        brand_rows = []
        for item in items:
            brand_rows.append({
                "query": query,
                "group": "OUR",
                "title": item.get("title", ""),
                "description": item.get("description", ""),
                "pubDate": item.get("pubDate", ""),
                "originallink": item.get("originallink", ""),
                "link": item.get("link", "")
            })
        all_rows.extend(brand_rows)

        # ê° ë¸Œëœë“œ ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ì¦‰ì‹œ Sheets ë™ê¸°í™”
        if brand_rows:
            _save_immediately(brand_rows, spreadsheet)

        time.sleep(0.1)  # Rate limit ë°©ì§€

    # ê²½ìŸì‚¬ ìˆ˜ì§‘
    print(f"\nğŸ“° ê²½ìŸì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ {max_pages} í˜ì´ì§€)...")
    for query in competitors:
        print(f"  â†’ {query}")
        items = fetch_naver_paginated(query, display, max_pages, sort, naver_id, naver_secret, existing_links)
        competitor_rows = []
        for item in items:
            competitor_rows.append({
                "query": query,
                "group": "COMPETITOR",
                "title": item.get("title", ""),
                "description": item.get("description", ""),
                "pubDate": item.get("pubDate", ""),
                "originallink": item.get("originallink", ""),
                "link": item.get("link", "")
            })
        all_rows.extend(competitor_rows)

        # ê° ê²½ìŸì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ì¦‰ì‹œ Sheets ë™ê¸°í™”
        if competitor_rows:
            _save_immediately(competitor_rows, spreadsheet)

        time.sleep(0.1)

    df = pd.DataFrame(all_rows)
    print(f"\nâœ… ì´ {len(df)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return df


def _save_immediately(rows: List[Dict], spreadsheet = None):
    """
    ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ ì¦‰ì‹œ Google Sheetsì— ë™ê¸°í™”

    Args:
        rows: ì €ì¥í•  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        spreadsheet: Google Sheets ê°ì²´
    """
    if not rows:
        return

    df_new = pd.DataFrame(rows)

    # Google Sheets ë™ê¸°í™” (df_new ì§ì ‘ ì „ë‹¬)
    if spreadsheet:
        try:
            from src.modules.export.sheets import sync_to_sheets
            sync_result = sync_to_sheets(df_new, spreadsheet, "raw_data")
            if sync_result['added'] > 0:
                print(f"    â˜ï¸  Sheets ë™ê¸°í™”: {sync_result['added']}ê°œ ì¶”ê°€")
        except Exception as e:
            print(f"    âš ï¸  Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}")
