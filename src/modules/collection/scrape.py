"""
scrape.py - Naver News Scraping Module
ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ BeautifulSoupë¡œ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜ ê¸°ì‚¬ ìˆ˜ì§‘
"""

import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from html import unescape
import re


def scrape_naver_news_by_date(query: str, start_date: str, end_date: str,
                              max_pages: int = 10) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ë‚ ì§œ ë²”ìœ„ë¡œ ìŠ¤í¬ë˜í•‘

    Args:
        query: ê²€ìƒ‰ì–´ (ë¸Œëœë“œëª…)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)

    Returns:
        ê¸°ì‚¬ ëª©ë¡ [{"title": ..., "description": ..., "link": ..., "pubDate": ...}, ...]
    """
    all_articles = []

    # ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError as e:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
        return []

    # ê²€ìƒ‰ URL
    base_url = "https://search.naver.com/search.naver"

    for page in range(1, max_pages + 1):
        try:
            # ë„¤ì´ë²„ ê²€ìƒ‰: ds(ì‹œì‘ì¼), de(ì¢…ë£Œì¼) íŒŒë¼ë¯¸í„° ì‚¬ìš©
            params = {
                "where": "news",
                "query": query,
                "ds": start_date.replace("-", "."),
                "de": end_date.replace("-", "."),
                "start": (page - 1) * 10 + 1,
                "news_by_date": "1"
            }

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }

            response = requests.get(base_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "lxml")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            articles = soup.find_all("li", class_="bx")

            if not articles:
                print(f"  í˜ì´ì§€ {page}: ê²°ê³¼ ì—†ìŒ")
                break

            for article in articles:
                try:
                    # ì œëª©
                    title_elem = article.find("a", class_="news_tit")
                    if not title_elem:
                        continue
                    title = title_elem.get_text(strip=True)

                    # ë§í¬
                    link = title_elem.get("href", "")

                    # ì„¤ëª…
                    desc_elem = article.find("div", class_="dsc")
                    description = ""
                    if desc_elem:
                        description = desc_elem.get_text(strip=True)

                    # ì¶œíŒ ë‚ ì§œ
                    date_elem = article.find("span", class_="sub_time")
                    pubdate_str = ""
                    if date_elem:
                        pubdate_str = date_elem.get_text(strip=True)

                    # ì¶œì²˜ (ì›ë³¸ ë§í¬ëŠ” ë„¤ì´ë²„ í¬ë¡¤ë§ìœ¼ë¡œëŠ” ì–´ë ¤ì›€)
                    source_elem = article.find("a", class_="press")
                    source = ""
                    if source_elem:
                        source = source_elem.get_text(strip=True)

                    if title and link:
                        all_articles.append({
                            "title": unescape(title),
                            "description": unescape(description),
                            "link": link,
                            "originallink": "",  # ìŠ¤í¬ë˜í•‘ìœ¼ë¡œëŠ” ì›ë³¸ ë§í¬ ë¯¸í¬í•¨
                            "pubDate": pubdate_str,
                            "source": source
                        })

                except Exception as e:
                    print(f"    ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"  í˜ì´ì§€ {page}: {len(articles)}ê°œ ê¸°ì‚¬")
            time.sleep(0.5)  # Rate limiting

        except requests.exceptions.RequestException as e:
            print(f"  í˜ì´ì§€ {page}: ìš”ì²­ ì˜¤ë¥˜ - {e}")
            break
        except Exception as e:
            print(f"  í˜ì´ì§€ {page}: ì˜¤ë¥˜ - {e}")
            break

    return all_articles


def parse_naver_date(date_str: str) -> Optional[str]:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì˜ ìƒëŒ€ ì‹œê°„ ë¬¸ìì—´ì„ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜
    ì˜ˆ: "2ì‹œê°„ ì „" â†’ ISO í˜•ì‹

    Note: ìŠ¤í¬ë˜í•‘ ì‹œì ì˜ ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•˜ë¯€ë¡œ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    """
    if not date_str or not date_str.strip():
        return None

    try:
        now = datetime.now()

        # ìƒëŒ€ ì‹œê°„ íŒŒì‹±
        if "ì´ˆ ì „" in date_str:
            secs = int(re.search(r'(\d+)', date_str).group(1))
            dt = now - timedelta(seconds=secs)
        elif "ë¶„ ì „" in date_str:
            mins = int(re.search(r'(\d+)', date_str).group(1))
            dt = now - timedelta(minutes=mins)
        elif "ì‹œê°„ ì „" in date_str:
            hours = int(re.search(r'(\d+)', date_str).group(1))
            dt = now - timedelta(hours=hours)
        elif "ì¼ ì „" in date_str:
            days = int(re.search(r'(\d+)', date_str).group(1))
            dt = now - timedelta(days=days)
        else:
            # ì ˆëŒ€ ë‚ ì§œ í˜•ì‹ ì‹œë„ (ì˜ˆ: "2026.02.07")
            dt = datetime.strptime(date_str.replace(".", "-"), "%Y-%m-%d")

        return dt.isoformat()

    except Exception as e:
        print(f"âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ '{date_str}': {e}")
        return None


def merge_api_and_scrape(df_api: pd.DataFrame, df_scrape: pd.DataFrame) -> pd.DataFrame:
    """
    API ìˆ˜ì§‘ ë°ì´í„°ì™€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³‘í•©

    Args:
        df_api: APIë¡œ ìˆ˜ì§‘í•œ DataFrame
        df_scrape: ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ìˆ˜ì§‘í•œ DataFrame

    Returns:
        ë³‘í•©ëœ DataFrame (API ë°ì´í„° ìš°ì„ )
    """
    print("ğŸ”€ APIì™€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³‘í•© ì¤‘...")

    if len(df_api) == 0:
        print("  âš ï¸  API ë°ì´í„° ì—†ìŒ, ìŠ¤í¬ë˜í•‘ ë°ì´í„°ë§Œ ì‚¬ìš©")
        df_scrape["data_source"] = "scrape"
        df_scrape["scraped_at"] = datetime.now().isoformat()
        return df_scrape

    if len(df_scrape) == 0:
        print("  âš ï¸  ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì—†ìŒ, API ë°ì´í„°ë§Œ ì‚¬ìš©")
        df_api["data_source"] = "api"
        return df_api

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ["title", "description", "link", "originallink", "pubDate"]

    # ë°ì´í„° ì†ŒìŠ¤ ë§ˆí‚¹
    df_api = df_api.copy()
    df_scrape = df_scrape.copy()

    df_api["data_source"] = "api"
    df_scrape["data_source"] = "scrape"
    df_scrape["scraped_at"] = datetime.now().isoformat()

    # originallinkê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ linkë¡œ ì •ê·œí™”
    df_api["pk"] = df_api["originallink"].where(
        df_api["originallink"].notna() & (df_api["originallink"] != ""),
        df_api["link"]
    )
    df_scrape["pk"] = df_scrape["link"]

    # ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì¤‘ APIì™€ ê²¹ì¹˜ëŠ” ê²ƒ ì œê±°
    api_links = set(df_api["pk"].dropna())
    df_scrape_new = df_scrape[~df_scrape["pk"].isin(api_links)]

    # ë³‘í•©
    df_merged = pd.concat([df_api, df_scrape_new], ignore_index=True)
    df_merged = df_merged.drop(columns=["pk"])

    print(f"  âœ… ë³‘í•© ì™„ë£Œ: API {len(df_api)}ê°œ + ìŠ¤í¬ë˜í•‘ {len(df_scrape_new)}ê°œ = ì´ {len(df_merged)}ê°œ")

    return df_merged


def collect_with_scraping(brands: List[str], competitors: List[str],
                         start_date: str, end_date: str,
                         max_pages: int = 10) -> pd.DataFrame:
    """
    APIì™€ ìŠ¤í¬ë˜í•‘ì„ ê²°í•©í•œ ì¢…í•© ìˆ˜ì§‘

    Args:
        brands: ìš°ë¦¬ ë¸Œëœë“œ ëª©ë¡
        competitors: ê²½ìŸì‚¬ ëª©ë¡
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        max_pages: í˜ì´ì§€ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜

    Returns:
        ë³‘í•©ëœ DataFrame
    """
    all_rows = []

    # ìš°ë¦¬ ë¸Œëœë“œ
    print(f"ğŸ“° ìš°ë¦¬ ë¸Œëœë“œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    for query in brands:
        print(f"  â†’ {query} ({start_date} ~ {end_date})")
        articles = scrape_naver_news_by_date(query, start_date, end_date, max_pages)
        for article in articles:
            article["query"] = query
            article["group"] = "OUR"
        all_rows.extend(articles)

    # ê²½ìŸì‚¬
    print(f"\nğŸ“° ê²½ìŸì‚¬ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    for query in competitors:
        print(f"  â†’ {query} ({start_date} ~ {end_date})")
        articles = scrape_naver_news_by_date(query, start_date, end_date, max_pages)
        for article in articles:
            article["query"] = query
            article["group"] = "COMPETITOR"
        all_rows.extend(articles)

    df = pd.DataFrame(all_rows)
    df["data_source"] = "scrape"
    df["scraped_at"] = datetime.now().isoformat()

    print(f"\nâœ… ì´ {len(df)}ê°œ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
    return df
