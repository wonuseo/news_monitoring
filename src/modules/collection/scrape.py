"""
scrape.py - Naver News Scraping Module with Playwright
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤í¬ë˜í•‘ (JavaScript ë Œë”ë§ ì§€ì›)
"""

import time
import pandas as pd
from datetime import datetime
from typing import List, Dict
from playwright.sync_api import sync_playwright


def scrape_naver_news_by_date(query: str, start_date: str, end_date: str,
                              max_pages: int = 10) -> List[Dict]:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ Naver ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤í¬ë˜í•‘ (JavaScript ë Œë”ë§ ì§€ì›)

    Args:
        query: ê²€ìƒ‰ì–´ (ë¸Œëœë“œëª…)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 10)

    Returns:
        ê¸°ì‚¬ ëª©ë¡ [{"title": ..., "description": ..., "link": ..., "pubDate": ...}, ...]
    """
    all_articles = []

    # ë‚ ì§œ ê²€ì¦
    try:
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError as e:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {e}")
        return []

    base_url = "https://search.naver.com/search.naver"

    try:
        with sync_playwright() as p:
            # ê° í˜ì´ì§€ ìš”ì²­ë§ˆë‹¤ ìƒˆ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (macOS ì•ˆì •ì„±)
            for page_num in range(1, max_pages + 1):
                browser = None
                try:
                    # ë¸Œë¼ìš°ì € ì‹¤í–‰ (í˜ì´ì§€ë§ˆë‹¤ ìƒˆë¡œ ì‹¤í–‰)
                    browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
                    page = browser.new_page()
                    page.set_viewport_size({"width": 1920, "height": 1080})

                    # URL êµ¬ì„±
                    start_idx = (page_num - 1) * 10 + 1
                    params = {
                        "where": "news",
                        "query": query,
                        "ds": start_date.replace("-", "."),
                        "de": end_date.replace("-", "."),
                        "start": start_idx,
                        "news_by_date": "1"
                    }

                    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
                    url = f"{base_url}?{query_string}"

                    print(f"  ğŸ“„ í˜ì´ì§€ {page_num} ë¡œë“œ ì¤‘...", end="", flush=True)
                    try:
                        page.goto(url, wait_until="networkidle", timeout=30000)
                    except Exception:
                        # networkidle íƒ€ì„ì•„ì›ƒ ì‹œ loadë¡œ í´ë°±
                        page.goto(url, wait_until="load", timeout=10000)

                    # JavaScript ë Œë”ë§ ëŒ€ê¸°
                    time.sleep(2)

                    # ê¸°ì‚¬ í•­ëª© ì¶”ì¶œ
                    articles_html = page.query_selector_all("li.bx div.news_wrap")
                    page_articles = 0

                    for article_element in articles_html:
                        try:
                            # ì œëª© ì¶”ì¶œ
                            title_elem = article_element.query_selector("a.news_tit")
                            if not title_elem:
                                continue
                            title = title_elem.get_attribute("title") or title_elem.text_content()
                            title = title.strip() if title else ""

                            # ë§í¬ ì¶”ì¶œ
                            link = title_elem.get_attribute("href") or ""

                            # ì„¤ëª… ì¶”ì¶œ
                            desc_elem = article_element.query_selector("div.dsc")
                            description = desc_elem.text_content().strip() if desc_elem else ""

                            # ë‚ ì§œ ì¶”ì¶œ
                            date_elem = article_element.query_selector("span.sub_time")
                            pub_date = ""
                            if date_elem:
                                pub_date = date_elem.text_content().strip()

                            # ì¶œì²˜ ì¶”ì¶œ
                            press_elem = article_element.query_selector("a.press")
                            source = press_elem.text_content().strip() if press_elem else ""

                            if title and link:
                                all_articles.append({
                                    "title": title,
                                    "description": description,
                                    "link": link,
                                    "originallink": "",
                                    "pubDate": pub_date,
                                    "source": source
                                })
                                page_articles += 1

                        except Exception:
                            continue

                    print(f" âœ“ {page_articles}ê°œ ê¸°ì‚¬")

                    # ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                    if page_articles == 0:
                        print(f"  âš ï¸  ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break

                    # Rate limiting
                    time.sleep(0.5)

                except Exception as e:
                    print(f" âŒ ì˜¤ë¥˜: {e}")
                    if page_num == 1:
                        print(f"  ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨. ìŠ¤í¬ë˜í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        break
                finally:
                    # ê° í˜ì´ì§€ ìš”ì²­ í›„ ë¸Œë¼ìš°ì € ì •ë¦¬
                    if browser:
                        try:
                            browser.close()
                        except Exception:
                            pass

    except Exception as e:
        print(f"âŒ Playwright ì˜¤ë¥˜: {e}")
        return []

    return all_articles


def merge_api_and_scrape(df_api: pd.DataFrame, df_scrape: pd.DataFrame) -> pd.DataFrame:
    """
    API ìˆ˜ì§‘ ë°ì´í„°ì™€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³‘í•©

    Args:
        df_api: APIë¡œ ìˆ˜ì§‘í•œ DataFrame
        df_scrape: ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ìˆ˜ì§‘í•œ DataFrame

    Returns:
        ë³‘í•©ëœ DataFrame (API ë°ì´í„° ìš°ì„ )
    """
    print("ğŸ”€ APIì™€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³‘í•© ì¤‘...")

    if len(df_api) == 0:
        print("  âš ï¸  API ë°ì´í„° ì—†ìŒ, ìŠ¤í¬ë˜í•‘ ë°ì´í„°ë§Œ ì‚¬ìš©")
        df_scrape["data_source"] = "scrape"
        df_scrape["scraped_at"] = datetime.now().isoformat()
        return df_scrape

    if len(df_scrape) == 0:
        print("  âš ï¸  ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì—†ìŒ, API ë°ì´í„°ë§Œ ì‚¬ìš©")
        df_api["data_source"] = "api"
        return df_api

    # ë°ì´í„° ì†ŒìŠ¤ ë§ˆí‚¹
    df_api = df_api.copy()
    df_scrape = df_scrape.copy()

    df_api["data_source"] = "api"
    df_scrape["data_source"] = "scrape"
    df_scrape["scraped_at"] = datetime.now().isoformat()

    # originallinkê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ linkë¡œ ì •ê·œí™”
    df_api["pk"] = df_api["originallink"].where(
        df_api["originallink"].notna() & (df_api["originallink"] != ""),
        df_api["link"]
    )
    df_scrape["pk"] = df_scrape["link"]

    # ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì¤‘ APIì™€ ê²¹ì¹˜ëŠ” ê²ƒ ì œê±°
    api_links = set(df_api["pk"].dropna())
    df_scrape_new = df_scrape[~df_scrape["pk"].isin(api_links)]

    # ë³‘í•©
    df_merged = pd.concat([df_api, df_scrape_new], ignore_index=True)
    df_merged = df_merged.drop(columns=["pk"])

    print(f"  âœ… ë³‘í•© ì™„ë£Œ: API {len(df_api)}ê°œ + ìŠ¤í¬ë˜í•‘ {len(df_scrape_new)}ê°œ = ì´ {len(df_merged)}ê°œ")

    return df_merged


def collect_with_scraping(brands: List[str], competitors: List[str],
                         start_date: str, end_date: str,
                         max_pages: int = 10) -> pd.DataFrame:
    """
    APIì™€ ìŠ¤í¬ë˜í•‘ì„ ê²°í•©í•œ ì¢…í•© ìˆ˜ì§‘

    Args:
        brands: ìš°ë¦¬ ë¸Œëœë“œ ëª©ë¡
        competitors: ê²½ìŸì‚¬ ëª©ë¡
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        max_pages: í˜ì´ì§€ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜

    Returns:
        ë³‘í•©ëœ DataFrame
    """
    all_rows = []

    # ìš°ë¦¬ ë¸Œëœë“œ
    print(f"ğŸ“° ìš°ë¦¬ ë¸Œëœë“œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    for query in brands:
        print(f"  â†’ {query} ({start_date} ~ {end_date})")
        articles = scrape_naver_news_by_date(query, start_date, end_date, max_pages)
        for article in articles:
            article["query"] = query
            article["group"] = "OUR"
        all_rows.extend(articles)

    # ê²½ìŸì‚¬
    print(f"\nğŸ“° ê²½ìŸì‚¬ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    for query in competitors:
        print(f"  â†’ {query} ({start_date} ~ {end_date})")
        articles = scrape_naver_news_by_date(query, start_date, end_date, max_pages)
        for article in articles:
            article["query"] = query
            article["group"] = "COMPETITOR"
        all_rows.extend(articles)

    df = pd.DataFrame(all_rows)
    df["data_source"] = "scrape"
    df["scraped_at"] = datetime.now().isoformat()

    print(f"\nâœ… ì´ {len(df)}ê°œ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
    return df
