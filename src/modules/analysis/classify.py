"""
classify.py - AI Classification Module
AIë¥¼ í†µí•œ ì¹´í…Œê³ ë¦¬í™” ë° ìœ„í—˜ë„ íŒë‹¨
1ë‹¨ê³„: ê°ì • ë¶„ì„ (ê¸ì •/ì¤‘ë¦½/ë¶€ì •)
2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
3ë‹¨ê³„: ë¶€ì • ê¸°ì‚¬ì— ëŒ€í•´ì„œë§Œ ìœ„í—˜ë„ í‰ê°€ (ìƒ/ì¤‘/í•˜)
"""

import json
import time
import random
import requests
import uuid
from datetime import datetime
from typing import Dict, List, Tuple
import pandas as pd


OPENAI_API_URL = "https://api.openai.com/v1/responses"

_error_callback = None


def set_error_callback(callback):
    """Register error logger callback: fn(message: str, data: dict)."""
    global _error_callback
    _error_callback = callback

# í•œêµ­ì–´ ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    "ë²•ë¥ /ê·œì œ",      # ë²•ì  ë¬¸ì œ, ê·œì œ, ì†Œì†¡
    "ë³´ì•ˆ/ë°ì´í„°",    # í•´í‚¹, ë°ì´í„° ìœ ì¶œ
    "ì•ˆì „/ì‚¬ê³ ",      # í™”ì¬, ì‚¬ê³ , ì•ˆì „ ë¬¸ì œ
    "ì¬ë¬´/ì‹¤ì ",      # ì‹¤ì , ì£¼ê°€, íˆ¬ì
    "ì œí’ˆ/ì„œë¹„ìŠ¤",    # ì‹ ê·œ ì„œë¹„ìŠ¤, ë¦¬ë‰´ì–¼
    "í‰íŒ/SNS",       # ì—¬ë¡ , SNS ì´ìŠˆ
    "ìš´ì˜/ê¸°íƒ€",      # ì¼ë°˜ ìš´ì˜, ì¸ì‚¬
]


def call_openai_batch(articles: List[Dict], task_type: str, openai_key: str,
                      retry: bool = True, retry_count: int = 0,
                      max_retries: int = 5, base_wait: int = 15,
                      request_id: str = None) -> Dict[int, Dict]:
    """
    OpenAI API ë°°ì¹˜ í˜¸ì¶œ
    
    Args:
        articles: ê¸°ì‚¬ ëª©ë¡ [{"title": ..., "description": ...}, ...]
        task_type: "sentiment" / "categorize" / "risk_assess"
        openai_key: OpenAI API í‚¤
        retry: ì¬ì‹œë„ ì—¬ë¶€
    
    Returns:
        {idx: {"sentiment": "ê¸ì •", ...}} í˜•íƒœì˜ ê²°ê³¼
    """
    if task_type == "sentiment":
        # 1ë‹¨ê³„: ê°ì • ë¶„ì„
        articles_text = ""
        for idx, article in enumerate(articles):
            articles_text += f"\n[{idx}]\nì œëª©: {article['title']}\nì„¤ëª…: {article['description'][:150]}\n"
        
        prompt = f"""ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ê¸°ì‚¬ë¥¼ ê¸ì •/ì¤‘ë¦½/ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

ê¸°ì‚¬ ëª©ë¡:{articles_text}

JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "results": [
    {{"id": 0, "sentiment": "ê¸ì •"}},
    {{"id": 1, "sentiment": "ë¶€ì •"}}
  ]
}}

ê°ì • ë¶„ë¥˜ ê¸°ì¤€:
- ê¸ì •: ì¢‹ì€ ì†Œì‹, ì„±ê³¼, ìˆ˜ìƒ, ì„±ì¥ ë“±
- ì¤‘ë¦½: ì¼ë°˜ ì†Œì‹, ì‚¬ì‹¤ ì „ë‹¬
- ë¶€ì •: ì‚¬ê³ , ë…¼ë€, ë¹„íŒ, ì†ì‹¤ ë“±

JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        
        schema = {
            "type": "object",
            "additionalProperties": False,
            "required": ["results"],
            "properties": {
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "additionalProperties": False,
                        "required": ["id", "sentiment"],
                        "properties": {
                            "id": {"type": "integer"},
                            "sentiment": {"type": "string", "enum": ["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"]}
                        }
                    }
                }
            }
        }
    
    elif task_type == "categorize":
        # 2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        articles_text = ""
        for idx, article in enumerate(articles):
            articles_text += f"\n[{idx}] {article['sentiment']}\nì œëª©: {article['title']}\n"
        
        prompt = f"""ë‹¹ì‹ ì€ í˜¸í…” ì—…ê³„ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê° ê¸°ì‚¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

ê¸°ì‚¬ ëª©ë¡:{articles_text}

JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "results": [
    {{"id": 0, "category": "ì œí’ˆ/ì„œë¹„ìŠ¤"}},
    {{"id": 1, "category": "ì•ˆì „/ì‚¬ê³ "}}
  ]
}}

ì¹´í…Œê³ ë¦¬ (í•˜ë‚˜ë§Œ ì„ íƒ):
- ë²•ë¥ /ê·œì œ: ë²•ì  ë¬¸ì œ, ê·œì œ, ì†Œì†¡, ì¡°ì‚¬
- ë³´ì•ˆ/ë°ì´í„°: í•´í‚¹, ì •ë³´ ìœ ì¶œ, ê°œì¸ì •ë³´ ì¹¨í•´
- ì•ˆì „/ì‚¬ê³ : í™”ì¬, ì‚¬ë§, ë¶€ìƒ, ì•ˆì „ ë¬¸ì œ
- ì¬ë¬´/ì‹¤ì : ì‹¤ì  ë°œí‘œ, ì£¼ê°€, íˆ¬ì, ì¬ë¬´ ì„±ê³¼
- ì œí’ˆ/ì„œë¹„ìŠ¤: ì‹ ê·œ ì˜¤í”ˆ, ë¦¬ë‰´ì–¼, ì œíœ´, í”„ë¡œëª¨ì…˜
- í‰íŒ/SNS: ì—¬ë¡ , SNS ì´ìŠˆ, ë¶ˆë§¤ìš´ë™, ê³ ê° ë¶ˆë§Œ
- ìš´ì˜/ê¸°íƒ€: ì¼ë°˜ ìš´ì˜, ì¸ì‚¬, ê¸°íƒ€

JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        
        schema = {
            "type": "object",
            "additionalProperties": False,
            "required": ["results"],
            "properties": {
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "additionalProperties": False,
                        "required": ["id", "category"],
                        "properties": {
                            "id": {"type": "integer"},
                            "category": {"type": "string", "enum": CATEGORIES}
                        }
                    }
                }
            }
        }
    
    else:  # risk_assess
        # 3ë‹¨ê³„: ìœ„í—˜ë„ í‰ê°€ (ë¶€ì • ê¸°ì‚¬ë§Œ)
        articles_text = ""
        for idx, article in enumerate(articles):
            articles_text += f"\n[{idx}] {article['category']}\nì œëª©: {article['title']}\n"
        
        prompt = f"""ë‹¹ì‹ ì€ í˜¸í…” ì—…ê³„ ë¦¬ìŠ¤í¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê° ë¶€ì • ê¸°ì‚¬ì˜ ìœ„í—˜ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.

ë¶€ì • ê¸°ì‚¬ ëª©ë¡:{articles_text}

JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "results": [
    {{"id": 0, "risk_level": "ìƒ", "reason": "í™”ì¬ ì‚¬ê³ "}},
    {{"id": 1, "risk_level": "ì¤‘", "reason": "ê³ ê° ë¶ˆë§Œ"}}
  ]
}}

ìœ„í—˜ë„ ê¸°ì¤€:
- ìƒ: ì¦‰ê° ëŒ€ì‘ í•„ìš”. ì‚¬ë§/í™”ì¬/ëŒ€ê·œëª¨ ìœ ì¶œ/ì˜ì—…ì •ì§€/ì§‘ë‹¨ì†Œì†¡ ë“±
- ì¤‘: ëª¨ë‹ˆí„°ë§ í•„ìš”. ê³ ê° ë¶ˆë§Œ í™•ì‚°/ê·œì œ ì¡°ì‚¬/í‰íŒ ì†ìƒ ë“±
- í•˜: ê²½ë¯¸í•œ ì˜í–¥. ì†Œê·œëª¨ ë¶ˆë§Œ/ì¼ë°˜ ë…¼ë€ ë“±

ì´ìœ (reason)ëŠ” 3-5ë‹¨ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        
        schema = {
            "type": "object",
            "additionalProperties": False,
            "required": ["results"],
            "properties": {
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "additionalProperties": False,
                        "required": ["id", "risk_level", "reason"],
                        "properties": {
                            "id": {"type": "integer"},
                            "risk_level": {"type": "string", "enum": ["ìƒ", "ì¤‘", "í•˜"]},
                            "reason": {"type": "string", "maxLength": 30}
                        }
                    }
                }
            }
        }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {openai_key}"
    }
    
    payload = {
        "model": "gpt-5-nano",
        "input": [
            {"role": "system", "content": [{"type": "input_text", "text": "ë‹¹ì‹ ì€ ì •í™•í•œ JSON ì‘ë‹µë§Œ ì œê³µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}]},
            {"role": "user", "content": [{"type": "input_text", "text": prompt}]}
        ],
        "text": {
            "format": {
                "type": "json_schema",
                "name": f"classify_{task_type}",
                "strict": True,
                "schema": schema
            }
        },
    }
    
    request_id = request_id or uuid.uuid4().hex[:8]
    current_retry = retry_count

    while True:
        try:
            response = requests.post(OPENAI_API_URL, headers=headers, json=payload, timeout=60)

            if response.status_code == 401:
                raise RuntimeError("OpenAI API ì¸ì¦ ì‹¤íŒ¨ (401). API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            elif response.status_code == 429:
                if retry and current_retry < max_retries - 1:
                    wait_time = base_wait * (2 ** current_retry)
                    retry_after = response.headers.get("Retry-After")
                    retry_after_seconds = None
                    if retry_after:
                        try:
                            retry_after_seconds = float(retry_after)
                        except ValueError:
                            retry_after_seconds = None
                    if retry_after_seconds is not None:
                        wait_time = retry_after_seconds
                    wait_time = max(wait_time, 10)
                    jitter = random.uniform(0, 6)
                    wait_time += jitter
                    if _error_callback:
                        _error_callback(
                            "OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (429)",
                            {
                                "request_id": request_id,
                                "status": 429,
                                "retry_after": retry_after_seconds,
                                "wait_time": round(wait_time, 1),
                                "attempt": current_retry + 1,
                            }
                        )
                    if retry_after_seconds is not None:
                        print(f"âš ï¸  OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (429) [req:{request_id}] Retry-After={retry_after_seconds:.1f}s, jitter={jitter:.1f}s â†’ {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (retry {current_retry + 1}/{max_retries})")
                    else:
                        print(f"âš ï¸  OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (429) [req:{request_id}] Retry-After ì—†ìŒ, jitter={jitter:.1f}s â†’ {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (retry {current_retry + 1}/{max_retries})")
                    time.sleep(wait_time)
                    current_retry += 1
                    continue
                if _error_callback:
                    _error_callback(
                        "OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (429) - ì¬ì‹œë„ ì‹¤íŒ¨",
                        {"request_id": request_id, "status": 429, "attempts": max_retries}
                    )
                raise RuntimeError("OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨)")
            elif response.status_code >= 500:
                if _error_callback:
                    _error_callback(
                        "OpenAI ì„œë²„ ì˜¤ë¥˜",
                        {"request_id": request_id, "status": response.status_code}
                    )
                raise RuntimeError(f"OpenAI ì„œë²„ ì˜¤ë¥˜ ({response.status_code})")

            response.raise_for_status()
            data = response.json()
            content = data.get("output_text", "").strip()
            if not content:
                output = data.get("output", [])
                if output:
                    contents = output[0].get("content", [])
                    for item in contents:
                        if item.get("type") == "output_text" and item.get("text"):
                            content = item["text"].strip()
                            break

            if not content:
                raise KeyError("Responses API ì‘ë‹µì—ì„œ output_textë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            parsed = json.loads(content)
            results = parsed.get("results", [])

            # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ {id: result}
            result_dict = {}
            for item in results:
                idx = item.get("id")
                if idx is not None:
                    result_dict[idx] = item

            return result_dict

        except (json.JSONDecodeError, KeyError, requests.exceptions.RequestException) as e:
            print(f"âš ï¸  ë°°ì¹˜ {task_type} ì˜¤ë¥˜ [req:{request_id}]: {e}")
            if _error_callback:
                _error_callback(
                    f"OpenAI ë°°ì¹˜ {task_type} ì˜¤ë¥˜",
                    {"request_id": request_id, "error": str(e), "attempt": current_retry + 1}
                )
            if retry and current_retry < max_retries - 1:
                print(f"ì¬ì‹œë„ ì¤‘... [req:{request_id} retry:{current_retry + 1}/{max_retries}]")
                time.sleep(2)
                current_retry += 1
                continue
            if _error_callback:
                _error_callback(
                    f"OpenAI ë°°ì¹˜ {task_type} ì‹¤íŒ¨",
                    {"request_id": request_id, "error": str(e), "attempt": current_retry + 1}
                )
            return {}


def should_classify(row: Dict, max_competitor_classify: int, 
                   competitor_count: Dict[str, int]) -> bool:
    """ë¶„ë¥˜ ëŒ€ìƒ ì—¬ë¶€ íŒë‹¨"""
    # ìš°ë¦¬ ë¸Œëœë“œëŠ” ì „ë¶€ ë¶„ë¥˜
    if row.get("group") == "OUR":
        return True
    
    # ê²½ìŸì‚¬ëŠ” ê° ë¸Œëœë“œë‹¹ ìµœì‹  Nê°œë§Œ ë¶„ë¥˜
    if row.get("group") == "COMPETITOR":
        query = row.get("query", "")
        count = competitor_count.get(query, 0)
        if count < max_competitor_classify:
            competitor_count[query] = count + 1
            return True
    
    return False


def classify_all(df: pd.DataFrame, openai_key: str, 
                max_competitor_classify: int, chunk_size: int, 
                dry_run: bool) -> pd.DataFrame:
    """
    3ë‹¨ê³„ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤
    1. ê°ì • ë¶„ì„ (ì „ì²´)
    2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì „ì²´)
    3. ìœ„í—˜ë„ í‰ê°€ (ë¶€ì • ê¸°ì‚¬ë§Œ)
    """
    df = df.copy()
    
    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["sentiment"] = ""
    df["category"] = ""
    df["risk_level"] = ""
    df["reason"] = ""
    df["classified_at"] = ""
    
    if dry_run:
        print("ğŸ”¬ DRY RUN ëª¨ë“œ: AI ë¶„ë¥˜ ìƒëµ")
        return df
    
    # ë¶„ë¥˜ ëŒ€ìƒ ìˆ˜ì§‘
    competitor_count = {}
    articles_to_classify = []
    indices_to_classify = []
    
    for idx, row in df.iterrows():
        if should_classify(row.to_dict(), max_competitor_classify, competitor_count):
            articles_to_classify.append({
                'title': row['title'],
                'description': row['description']
            })
            indices_to_classify.append(idx)
    
    if len(articles_to_classify) == 0:
        print("âš ï¸  ë¶„ë¥˜í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return df
    
    total = len(articles_to_classify)
    print(f"\nğŸ¤– AI ë¶„ë¥˜ ì‹œì‘: {total}ê°œ ê¸°ì‚¬ (ì²­í¬ í¬ê¸°: {chunk_size})")
    
    timestamp = datetime.now().isoformat()
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for chunk_start in range(0, total, chunk_size):
        chunk_end = min(chunk_start + chunk_size, total)
        chunk_articles = articles_to_classify[chunk_start:chunk_end]
        chunk_indices = indices_to_classify[chunk_start:chunk_end]
        chunk_num = (chunk_start // chunk_size) + 1
        total_chunks = (total + chunk_size - 1) // chunk_size
        
        print(f"\n--- ì²­í¬ {chunk_num}/{total_chunks} ({len(chunk_articles)}ê°œ ê¸°ì‚¬) ---")
        
        # Step 1: ê°ì • ë¶„ì„
        print("  [1/3] ê°ì • ë¶„ì„ ì¤‘...")
        sentiment_results = call_openai_batch(chunk_articles, "sentiment", openai_key)
        
        for i, idx in enumerate(chunk_indices):
            if i in sentiment_results:
                sentiment = sentiment_results[i].get("sentiment", "ì¤‘ë¦½")
                df.at[idx, "sentiment"] = sentiment
                chunk_articles[i]["sentiment"] = sentiment
            else:
                df.at[idx, "sentiment"] = "ì¤‘ë¦½"
                chunk_articles[i]["sentiment"] = "ì¤‘ë¦½"
        
        time.sleep(1)
        
        # Step 2: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        print("  [2/3] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")
        category_results = call_openai_batch(chunk_articles, "categorize", openai_key)
        
        for i, idx in enumerate(chunk_indices):
            if i in category_results:
                category = category_results[i].get("category", "ìš´ì˜/ê¸°íƒ€")
                if category not in CATEGORIES:
                    category = "ìš´ì˜/ê¸°íƒ€"
                df.at[idx, "category"] = category
                chunk_articles[i]["category"] = category
            else:
                df.at[idx, "category"] = "ìš´ì˜/ê¸°íƒ€"
                chunk_articles[i]["category"] = "ìš´ì˜/ê¸°íƒ€"
        
        time.sleep(1)
        
        # Step 3: ìœ„í—˜ë„ í‰ê°€ (ë¶€ì • ê¸°ì‚¬ë§Œ)
        negative_articles = [art for art in chunk_articles if art.get("sentiment") == "ë¶€ì •"]
        negative_indices_map = {
            i: chunk_indices[i] 
            for i, art in enumerate(chunk_articles) 
            if art.get("sentiment") == "ë¶€ì •"
        }
        
        if len(negative_articles) > 0:
            print(f"  [3/3] ìœ„í—˜ë„ í‰ê°€ ì¤‘ (ë¶€ì • ê¸°ì‚¬ {len(negative_articles)}ê°œ)...")
            risk_results = call_openai_batch(negative_articles, "risk_assess", openai_key)
            
            for i, idx in negative_indices_map.items():
                # negative_articlesì˜ ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ë§¤í•‘ í•„ìš”
                neg_idx = list(negative_indices_map.keys()).index(i)
                if neg_idx in risk_results:
                    risk_level = risk_results[neg_idx].get("risk_level", "í•˜")
                    reason = risk_results[neg_idx].get("reason", "í‰ê°€ ì‹¤íŒ¨")
                    df.at[idx, "risk_level"] = risk_level
                    df.at[idx, "reason"] = reason
                else:
                    df.at[idx, "risk_level"] = "í•˜"
                    df.at[idx, "reason"] = "í‰ê°€ ì‹¤íŒ¨"
        else:
            print("  [3/3] ë¶€ì • ê¸°ì‚¬ ì—†ìŒ, ìœ„í—˜ë„ í‰ê°€ ìƒëµ")
        
        # ê¸ì •/ì¤‘ë¦½ ê¸°ì‚¬ëŠ” ìœ„í—˜ë„ ì—†ìŒ
        for i, idx in enumerate(chunk_indices):
            if df.at[idx, "sentiment"] != "ë¶€ì •":
                df.at[idx, "risk_level"] = "-"
                df.at[idx, "reason"] = "-"
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        for idx in chunk_indices:
            df.at[idx, "classified_at"] = timestamp
        
        print(f"  âœ… ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ")
        time.sleep(1)
    
    print(f"\nâœ… AI ë¶„ë¥˜ ì™„ë£Œ: {total}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
    return df
