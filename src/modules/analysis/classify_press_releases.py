"""
classify_press_releases.py - Press Release LLM Classification with Result Sharing
ë³´ë„ìë£Œ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • -> LLM ë¶„ì„ -> í´ëŸ¬ìŠ¤í„° ë‚´ ê²°ê³¼ ê³µìœ 
"""

import json
from datetime import datetime
from typing import Dict, Optional, Tuple  # Optional kept for internal use

import pandas as pd

from .llm_engine import load_prompts, analyze_article_llm, analyze_article_negative_llm
from .task_runner import run_chunked_parallel
from src.modules.export.sheets import sync_result_to_sheets

EMPTY_PR_METRICS = {
    "pr_clusters_analyzed": 0,
    "pr_articles_propagated": 0,
    "pr_llm_success": 0,
    "pr_llm_failed": 0,
    "pr_cost_estimated": 0,
}


def select_representative_articles(df: pd.DataFrame) -> Dict[str, int]:
    """
    í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • (earliest pub_datetime)

    ì„ ì • ê¸°ì¤€:
    - cluster_idë³„ë¡œ ê°€ì¥ ë¹ ë¥¸ pub_datetime ê¸°ì‚¬ ì„ íƒ
    - pub_datetime ë™ì¼ ì‹œ DataFrame ìˆœì„œ ìš°ì„ 
    - null/NaTëŠ” í›„ìˆœìœ„

    Args:
        df: cluster_summaryì™€ cluster_idê°€ ìˆëŠ” DataFrame

    Returns:
        {cluster_id: representative_index} ë”•ì…”ë„ˆë¦¬
    """
    # ë³´ë„ìë£Œ í•„í„°ë§
    pr_mask = (
        (df["source"] == "ë³´ë„ìë£Œ") &
        (df["cluster_id"].notna()) &
        (df["cluster_id"] != "")
    )
    df_pr = df[pr_mask].copy()

    if len(df_pr) == 0:
        return {}

    # pub_datetime íŒŒì‹± (ì‹¤íŒ¨ ì‹œ NaT)
    df_pr["pub_datetime_parsed"] = pd.to_datetime(df_pr["pub_datetime"], errors="coerce")

    representatives = {}
    for cluster_id, group in df_pr.groupby("cluster_id"):
        # ë‚ ì§œìˆœ ì •ë ¬ (NaTëŠ” ë§ˆì§€ë§‰, ê°™ì€ ë‚ ì§œë©´ ì›ë³¸ ìˆœì„œ)
        sorted_group = group.sort_values(
            by=["pub_datetime_parsed"],
            na_position="last"
        )
        representative_idx = sorted_group.index[0]
        representatives[cluster_id] = representative_idx

    return representatives


def classify_press_releases(
    df: pd.DataFrame,
    openai_key: str,
    chunk_size: int = 50,
    max_workers: int = 3,
    spreadsheet=None,
    raw_df: pd.DataFrame = None,
    reasoning_collector=None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    ë³´ë„ìë£Œ LLM ë¶„ë¥˜ (ëŒ€í‘œ ê¸°ì‚¬ ë¶„ì„ -> í´ëŸ¬ìŠ¤í„° ë‚´ ê³µìœ )

    ì²˜ë¦¬ íë¦„:
    1. ë³´ë„ìë£Œ í•„í„°ë§ (source="ë³´ë„ìë£Œ", cluster_id ì¡´ì¬)
    2. í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • (earliest pub_datetime)
    3. ëŒ€í‘œ ê¸°ì‚¬ë§Œ LLM ë¶„ì„ (analyze_article_llm)
    4. ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê°™ì€ cluster_id ì „ì²´ì— ë³µì‚¬
    5. classified_at íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡

    Args:
        df: ì „ì²˜ë¦¬ëœ DataFrame (cluster_id, source ì»¬ëŸ¼ í¬í•¨)
        openai_key: OpenAI API í‚¤
        chunk_size: ì²­í¬ í¬ê¸°
        max_workers: ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        spreadsheet: Google Sheets ê°ì²´ (ì²­í¬ ë™ê¸°í™”ìš©)
        raw_df: ì›ë³¸ raw DataFrame (Sheets ë™ê¸°í™”ìš©)

    Returns:
        (df, pr_metrics) íŠœí”Œ
    """
    df = df.copy()

    if "cluster_id" not in df.columns:
        print("âš ï¸  cluster_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë³´ë„ìë£Œ ë¶„ë¥˜ ìŠ¤í‚µ")
        return df, EMPTY_PR_METRICS.copy()

    pr_mask = (
        (df["source"] == "ë³´ë„ìë£Œ") &
        (df["cluster_id"].notna()) &
        (df["cluster_id"] != "")
    )
    pr_count = pr_mask.sum()

    if pr_count == 0:
        print("â„¹ï¸  ë³´ë„ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ë¥˜ ìŠ¤í‚µ")
        return df, EMPTY_PR_METRICS.copy()

    print(f"\nğŸ“‹ ë³´ë„ìë£Œ LLM ë¶„ë¥˜ ì‹œì‘: {pr_count}ê°œ ê¸°ì‚¬")

    representatives = select_representative_articles(df)
    print(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(representatives)}ê°œ")

    try:
        prompts_config = load_prompts()
    except Exception as e:
        print(f"âš ï¸  prompts.yaml ë¡œë“œ ì‹¤íŒ¨: {e}")
        return df, EMPTY_PR_METRICS.copy()

    # 1ì°¨ ë¶„ë¥˜ ê²°ê³¼ ì»¬ëŸ¼ (prompts.yaml ë‹´ë‹¹)
    result_columns = [
        "brand_relevance",
        "brand_relevance_query_keywords",
        "sentiment_stage",
        "news_category",
        "news_keyword_summary",
        "classified_at",
    ]
    for col in result_columns:
        if col not in df.columns:
            df[col] = ""

    # 2ì°¨ ë¶„ë¥˜ ê²°ê³¼ ì»¬ëŸ¼ (negative_prompts.yaml ë‹´ë‹¹) â€” ë³„ë„ ì´ˆê¸°í™”
    for col in ["danger_level", "issue_category"]:
        if col not in df.columns:
            df[col] = ""

    timestamp = datetime.now().isoformat()
    propagated_count = 0

    cluster_to_target_indices = {}
    for idx in df[pr_mask].index:
        cluster_id = df.at[idx, "cluster_id"]
        cluster_to_target_indices.setdefault(cluster_id, []).append(idx)

    tasks = []
    for cluster_id, rep_idx in representatives.items():
        if cluster_id not in cluster_to_target_indices:
            continue

        row = df.loc[rep_idx]
        article = {
            "query": row.get("query", ""),
            "group": row.get("group", ""),
            "title": row.get("title", ""),
            "description": row.get("description", ""),
        }
        tasks.append({
            "task_id": cluster_id,
            "cluster_id": cluster_id,
            "idx": rep_idx,
            "article": article,
        })

    if len(tasks) == 0:
        print("â„¹ï¸  ë³´ë„ìë£Œ ëŒ€í‘œ ê¸°ì‚¬ ë¶„ì„ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤")
        return df, EMPTY_PR_METRICS.copy()

    print(f"  - LLM í˜¸ì¶œ ì˜ˆì •: {len(tasks)}íšŒ")
    print(f"  - ë¶„ì„ ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì›Œì»¤: {max_workers})")

    def apply_cluster_result(cluster_id: str, result: Optional[Dict]):
        nonlocal propagated_count
        target_indices = cluster_to_target_indices.get(cluster_id, [])
        if not target_indices:
            return

        for idx in target_indices:
            if result:
                df.at[idx, "brand_relevance"] = result.get("brand_relevance", "")
                keywords = result.get("brand_relevance_query_keywords", [])
                df.at[idx, "brand_relevance_query_keywords"] = json.dumps(keywords, ensure_ascii=False)
                df.at[idx, "sentiment_stage"] = result.get("sentiment_stage", "")
                df.at[idx, "news_category"] = result.get("news_category", "")
                df.at[idx, "news_keyword_summary"] = result.get("news_keyword_summary", "")
            else:
                df.at[idx, "brand_relevance"] = ""
                df.at[idx, "brand_relevance_query_keywords"] = "[]"
                df.at[idx, "sentiment_stage"] = ""
                df.at[idx, "news_category"] = ""
                df.at[idx, "news_keyword_summary"] = ""

            df.at[idx, "classified_at"] = timestamp
            propagated_count += 1

    def worker(task: Dict) -> Dict:
        cluster_id = task["cluster_id"]
        try:
            result = analyze_article_llm(task["article"], prompts_config, openai_key)
            return {
                "task_id": cluster_id,
                "cluster_id": cluster_id,
                "idx": task["idx"],
                "success": bool(result),
                "result": result,
                "error": None if result else "Empty LLM response",
                "error_type": None if result else "EmptyResponse",
            }
        except Exception as e:
            import traceback
            return {
                "task_id": cluster_id,
                "cluster_id": cluster_id,
                "idx": task["idx"],
                "success": False,
                "result": None,
                "error": str(e),
                "error_type": type(e).__name__,
                "error_trace": traceback.format_exc(),
            }

    def on_success(result: Dict):
        cluster_result = result.get("result") or {}
        # ëŒ€í‘œ ê¸°ì‚¬ reasoning ìˆ˜ì§‘ (_reasoningì€ í´ëŸ¬ìŠ¤í„° ì „íŒŒì—ì„œ ì œì™¸)
        _reasoning = cluster_result.pop("_reasoning", None)
        if reasoning_collector is not None and _reasoning:
            rep_idx = result.get("idx")
            if rep_idx is not None and "article_id" in df.columns:
                article_id = df.at[rep_idx, "article_id"]
                reasoning_collector.add_first_pass(str(article_id), timestamp, _reasoning)
        apply_cluster_result(result["cluster_id"], cluster_result if cluster_result else None)

    def on_failure(result: Dict, _chunk_fail_count: int, total_fail_count: int):
        cluster_id = result.get("cluster_id", result.get("task_id", "unknown"))
        rep_idx = result.get("idx")
        title = ""
        if rep_idx is not None and rep_idx in df.index:
            title = str(df.at[rep_idx, "title"]) if pd.notna(df.at[rep_idx, "title"]) else ""

        apply_cluster_result(cluster_id, None)
        print(f"\nâŒ ë³´ë„ìë£Œ ëŒ€í‘œê¸°ì‚¬ ë¶„ë¥˜ ì‹¤íŒ¨ [cluster={cluster_id}]:")
        print(f"   ì œëª©: {title[:80]}...")
        print(f"   ì—ëŸ¬ íƒ€ì…: {result.get('error_type', 'Unknown')}")
        print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {result.get('error', 'Unknown error')}")
        if total_fail_count == 1 and "error_trace" in result:
            print(f"   ìƒì„¸ Traceback (ì²« ì‹¤íŒ¨ë§Œ):\n{result['error_trace']}")

    def sync_callback(_chunk_num: int, _total_chunks: int, _succ: int, _fail: int):
        if spreadsheet and raw_df is not None:
            sync_result_to_sheets(df, raw_df, spreadsheet, verbose=True)

    run_stats = run_chunked_parallel(
        tasks=tasks,
        worker_fn=worker,
        on_success=on_success,
        on_failure=on_failure,
        chunk_size=chunk_size,
        max_workers=max_workers,
        progress_desc="ë³´ë„ìë£Œ ëŒ€í‘œ LLM ë¶„ì„",
        unit="í´ëŸ¬ìŠ¤í„°",
        inter_chunk_sleep=0.5,
        sync_callback=sync_callback,
    )

    print(f"âœ… ë³´ë„ìë£Œ 1ì°¨ ë¶„ë¥˜ ì™„ë£Œ: {propagated_count}ê°œ")
    print("  - í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê²°ê³¼ ê³µìœ ")
    print(f"  - ëŒ€í‘œ ê¸°ì‚¬ LLM ë¶„ì„: ì„±ê³µ {run_stats['success']}ê°œ, ì‹¤íŒ¨ {run_stats['failed']}ê°œ")

    # ========================================
    # Negative 2ì°¨ ì •ë°€ ë¶„ì„ (danger_level / issue_category)
    # ========================================
    negative_columns = ["danger_level", "issue_category"]
    negative_clusters = [
        cluster_id for cluster_id, rep_idx in representatives.items()
        if df.at[rep_idx, "sentiment_stage"] in ("ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •")
    ]

    negative_success = 0
    if negative_clusters:
        print(f"\n[ë³´ë„ìë£Œ Negative Pass] {len(negative_clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ì •ë°€ ë¶„ì„ ì¤‘...")

        negative_tasks = []
        for cluster_id in negative_clusters:
            rep_idx = representatives[cluster_id]
            row = df.loc[rep_idx]
            article = {
                "query": row.get("query", ""),
                "group": row.get("group", ""),
                "title": row.get("title", ""),
                "description": row.get("description", ""),
            }
            initial_result = {
                "brand_relevance": df.at[rep_idx, "brand_relevance"],
                "sentiment_stage": df.at[rep_idx, "sentiment_stage"],
            }
            negative_tasks.append({
                "task_id": cluster_id,
                "cluster_id": cluster_id,
                "idx": rep_idx,
                "article": article,
                "initial_result": initial_result,
            })

        def negative_worker(task: Dict) -> Dict:
            cluster_id = task["cluster_id"]
            try:
                result = analyze_article_negative_llm(
                    task["article"], task["initial_result"], openai_key
                )
                return {
                    "task_id": cluster_id,
                    "cluster_id": cluster_id,
                    "success": bool(result),
                    "result": result,
                    "error": None if result else "Empty LLM response",
                }
            except Exception as e:
                import traceback
                return {
                    "task_id": cluster_id,
                    "cluster_id": cluster_id,
                    "success": False,
                    "result": None,
                    "error": str(e),
                    "error_trace": traceback.format_exc(),
                }

        def on_negative_success(result: Dict):
            nonlocal negative_success
            cluster_id = result["cluster_id"]
            neg_result = result.get("result") or {}

            # ëŒ€í‘œ ê¸°ì‚¬ 2ì°¨ reasoning ìˆ˜ì§‘
            _neg_reasoning = neg_result.pop("_reasoning", None)
            if reasoning_collector is not None and _neg_reasoning:
                rep_idx = representatives.get(cluster_id)
                if rep_idx is not None and "article_id" in df.columns:
                    article_id = df.at[rep_idx, "article_id"]
                    reasoning_collector.update_second_pass(str(article_id), _neg_reasoning)

            for idx in cluster_to_target_indices.get(cluster_id, []):
                for col in negative_columns:
                    if col in neg_result:
                        df.at[idx, col] = neg_result[col] if neg_result[col] is not None else ""
            negative_success += 1

        def on_negative_failure(result: Dict, _chunk_fail_count: int, _total_fail_count: int):
            pass  # ì‹¤íŒ¨ ì‹œ 1ì°¨ ê²°ê³¼ ìœ ì§€

        run_chunked_parallel(
            tasks=negative_tasks,
            worker_fn=negative_worker,
            on_success=on_negative_success,
            on_failure=on_negative_failure,
            chunk_size=chunk_size,
            max_workers=max_workers,
            progress_desc="ë³´ë„ìë£Œ ë¶€ì • ê¸°ì‚¬ ì •ë°€ ë¶„ì„",
            unit="í´ëŸ¬ìŠ¤í„°",
            inter_chunk_sleep=0.5,
        )

        print(f"âœ… ë³´ë„ìë£Œ Negative Pass ì™„ë£Œ: {negative_success}/{len(negative_clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ì •ë°€ ë¶„ì„")

    pr_metrics = {
        "pr_clusters_analyzed": len(tasks),
        "pr_articles_propagated": propagated_count,
        "pr_llm_success": run_stats["success"],
        "pr_llm_failed": run_stats["failed"],
        "pr_negative_clusters": len(negative_clusters),
        "pr_negative_success": negative_success,
        "pr_cost_estimated": 0,  # Cost tracking can be added later
    }

    return df, pr_metrics
