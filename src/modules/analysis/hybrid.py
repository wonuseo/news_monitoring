"""
hybrid.py - Hybrid Analysis Orchestrator
Rule-Based + LLM í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import time
import json
import os
from datetime import datetime
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import pandas as pd
from tqdm import tqdm

from .rule_engine import load_rules, analyze_batch_rb
from .llm_engine import load_prompts, analyze_article_llm

# CSV ì“°ê¸° Lock (ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ íŒŒì¼ ì“°ê¸° ê²½í•© ë°©ì§€)
csv_write_lock = Lock()


def _process_single_article(
    idx: int,
    article: Dict,
    rb_result: Dict,
    prompts_config: dict,
    openai_key: str,
    rb_columns: List[str],
    llm_columns: List[str]
) -> Dict:
    """
    ë‹¨ì¼ ê¸°ì‚¬ LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼ í•¨ìˆ˜)

    Args:
        idx: DataFrame ì¸ë±ìŠ¤
        article: {"title": ..., "description": ...}
        rb_result: Rule-Based ë¶„ì„ ê²°ê³¼
        prompts_config: prompts.yaml ì„¤ì •
        openai_key: OpenAI API í‚¤
        rb_columns: RB ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        llm_columns: LLM ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        {"idx": idx, "success": True/False, "result": {...}, "error": ..., "error_type": ...}
    """
    try:
        llm_result = analyze_article_llm(article, rb_result, prompts_config, openai_key)
        return {
            "idx": idx,
            "success": True,
            "result": llm_result,
            "error": None,
            "error_type": None
        }
    except Exception as e:
        import traceback
        error_type = type(e).__name__
        error_msg = str(e)
        error_trace = traceback.format_exc()
        return {
            "idx": idx,
            "success": False,
            "result": None,
            "error": error_msg,
            "error_type": error_type,
            "error_trace": error_trace
        }


def classify_hybrid(
    df: pd.DataFrame,
    openai_key: str,
    chunk_size: int = 50,
    dry_run: bool = False,
    max_competitor_classify: int = 50,
    max_workers: int = 10,
    result_csv_path: Optional[str] = None
) -> pd.DataFrame:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)

    3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:
    1. Rule-Based ë¶„ì„ (ì „ì²´ ê¸°ì‚¬)
    2. LLM ë¶„ì„ (ì¡°ê±´ë¶€: ìš°ë¦¬ ë¸Œëœë“œ ì „ì²´ + ê²½ìŸì‚¬ ìƒìœ„ Nê°œ)
       - ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (max_workers ì„¤ì •)
       - ì²­í¬ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  í‘œì‹œ (tqdm)
       - ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ì¦‰ì‹œ result.csvì— append
    3. ê²°ê³¼ ë³‘í•© ë° DataFrame ë°˜í™˜

    Args:
        df: ì…ë ¥ DataFrame (title, description í•„ìˆ˜)
        openai_key: OpenAI API í‚¤
        chunk_size: LLM ë°°ì¹˜ í¬ê¸° (ì²­í¬ë‹¹ ê¸°ì‚¬ ìˆ˜)
        dry_run: Trueë©´ LLM í˜¸ì¶œ ìƒëµ (Rule-Basedë§Œ)
        max_competitor_classify: ê²½ìŸì‚¬ë³„ ìµœëŒ€ ë¶„ë¥˜ ê°œìˆ˜
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        result_csv_path: ê²°ê³¼ ì €ì¥ CSV ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)

    Returns:
        ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrame
    """
    df = df.copy()

    # ì´ˆê¸°í™”: ëª¨ë“  ê²°ê³¼ ì»¬ëŸ¼
    rb_columns = [
        "brand_mentions",
        "brand_scope_rb",
        "sentiment_rb",
        "danger_rb",
        "risk_score_rb",
        "issue_category_rb",
        "coverage_themes_rb",
        "reason_codes_rb",
        "matched_rules_rb",
        "score_breakdown_rb"
    ]

    llm_columns = [
        "sentiment_llm",
        "sentiment_llm_confidence",
        "sentiment_llm_evidence",
        "sentiment_llm_rationale",
        "sentiment_final",
        "sentiment_final_confidence",
        "sentiment_final_decision_rule",
        "sentiment_final_evidence",
        "sentiment_final_rationale",
        "danger_llm",
        "danger_llm_confidence",
        "danger_llm_evidence",
        "danger_llm_rationale",
        "danger_final",
        "danger_final_confidence",
        "danger_final_decision_rule",
        "danger_final_evidence",
        "danger_final_rationale",
        "issue_category_llm",
        "coverage_themes_llm",
        "category_llm_confidence",
        "category_llm_evidence",
        "category_llm_rationale",
        "issue_category_final",
        "coverage_themes_final",
        "category_final_confidence",
        "category_final_decision_rule",
        "category_final_evidence",
        "category_final_rationale"
    ]

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    for col in rb_columns + llm_columns:
        df[col] = None

    df["classified_at"] = ""

    # Load configs
    print("\nğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹œì‘...")
    rules = load_rules()
    prompts_config = load_prompts()

    # ========================================
    # STEP 1: Rule-Based ë¶„ì„ (ì „ì²´ ê¸°ì‚¬)
    # ========================================
    print("\n[1/3] Rule-Based ë¶„ì„ ì¤‘...")
    articles = df[["title", "description"]].fillna("").to_dict("records")
    rb_results = analyze_batch_rb(articles, rules)

    # RB ê²°ê³¼ë¥¼ DataFrameì— ë³‘í•©
    for idx, rb_result in enumerate(rb_results):
        for col in rb_columns:
            if col in rb_result:
                # JSON serializable íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                value = rb_result[col]
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, ensure_ascii=False)
                df.at[idx, col] = value

    print(f"âœ… Rule-Based ë¶„ì„ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")

    if dry_run:
        print("ğŸ”¬ DRY RUN ëª¨ë“œ: LLM ë¶„ì„ ìƒëµ")
        return df

    # ========================================
    # STEP 2: LLM ë¶„ì„ ëŒ€ìƒ ì„ íƒ
    # ========================================
    print("\n[2/3] LLM ë¶„ì„ ëŒ€ìƒ ì„ íƒ ì¤‘...")

    # ë¶„ë¥˜ ëŒ€ìƒ í•„í„°ë§ (ìš°ë¦¬ ë¸Œëœë“œ ì „ì²´ + ê²½ìŸì‚¬ ìƒìœ„ Nê°œ)
    indices_to_classify = []
    competitor_count = {}

    for idx, row in df.iterrows():
        group = row.get("group", "")

        # ìš°ë¦¬ ë¸Œëœë“œëŠ” ì „ë¶€
        if group == "OUR":
            indices_to_classify.append(idx)
        # ê²½ìŸì‚¬ëŠ” ê° ë¸Œëœë“œë‹¹ ìµœì‹  Nê°œ
        elif group == "COMPETITOR":
            query = row.get("query", "")
            count = competitor_count.get(query, 0)
            if count < max_competitor_classify:
                competitor_count[query] = count + 1
                indices_to_classify.append(idx)

    if len(indices_to_classify) == 0:
        print("âš ï¸  LLM ë¶„ì„ ëŒ€ìƒ ì—†ìŒ")
        return df

    print(f"  ì„ íƒëœ ê¸°ì‚¬: {len(indices_to_classify)}ê°œ")
    print(f"  - ìš°ë¦¬ ë¸Œëœë“œ: {sum(1 for idx in indices_to_classify if df.at[idx, 'group'] == 'OUR')}ê°œ")
    print(f"  - ê²½ìŸì‚¬: {sum(1 for idx in indices_to_classify if df.at[idx, 'group'] == 'COMPETITOR')}ê°œ")

    # ========================================
    # STEP 3: LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
    # ========================================
    print(f"\n[3/3] LLM ë¶„ì„ ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì›Œì»¤: {max_workers})...")

    timestamp = datetime.now().isoformat()
    total = len(indices_to_classify)
    total_chunks = (total + chunk_size - 1) // chunk_size

    # ì „ì²´ ì§„í–‰ë¥  í‘œì‹œìš© tqdm
    pbar = tqdm(total=total, desc="LLM ë¶„ì„", unit="ê¸°ì‚¬")

    for chunk_start in range(0, total, chunk_size):
        chunk_end = min(chunk_start + chunk_size, total)
        chunk_indices = indices_to_classify[chunk_start:chunk_end]
        chunk_num = (chunk_start // chunk_size) + 1

        # ì²­í¬ ë‚´ ê¸°ì‚¬ ì •ë³´ ì¤€ë¹„
        articles_to_process = []
        for idx in chunk_indices:
            article = {
                "title": df.at[idx, "title"],
                "description": df.at[idx, "description"]
            }

            # RB ê²°ê³¼ íŒŒì‹± (JSON string â†’ dict/list)
            rb_result = {}
            for col in rb_columns:
                value = df.at[idx, col]
                if pd.isna(value):
                    continue
                # JSON stringì´ë©´ íŒŒì‹±
                if isinstance(value, str) and (value.startswith("{") or value.startswith("[")):
                    try:
                        rb_result[col] = json.loads(value)
                    except:
                        rb_result[col] = value
                else:
                    rb_result[col] = value

            articles_to_process.append({
                "idx": idx,
                "article": article,
                "rb_result": rb_result
            })

        # ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor)
        success_count = 0
        fail_count = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_idx = {
                executor.submit(
                    _process_single_article,
                    item["idx"],
                    item["article"],
                    item["rb_result"],
                    prompts_config,
                    openai_key,
                    rb_columns,
                    llm_columns
                ): item["idx"]
                for item in articles_to_process
            }

            # Process completed tasks
            for future in as_completed(future_to_idx):
                result = future.result()
                idx = result["idx"]

                if result["success"]:
                    # LLM ê²°ê³¼ë¥¼ DataFrameì— ë³‘í•©
                    llm_result = result["result"]
                    for col in llm_columns:
                        if col in llm_result:
                            value = llm_result[col]
                            # JSON serializable íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                            if isinstance(value, (dict, list)):
                                value = json.dumps(value, ensure_ascii=False)
                            df.at[idx, col] = value

                    df.at[idx, "classified_at"] = timestamp
                    success_count += 1

                    # ì„±ê³µí•œ ê¸°ì‚¬ ì¦‰ì‹œ CSVì— ì €ì¥ (Lock ì‚¬ìš©)
                    if result_csv_path:
                        try:
                            with csv_write_lock:
                                row_df = df.loc[[idx]].copy()
                                # íŒŒì¼ì´ ì—†ìœ¼ë©´ header í¬í•¨, ìˆìœ¼ë©´ appendë§Œ
                                file_exists = os.path.exists(result_csv_path)
                                row_df.to_csv(
                                    result_csv_path,
                                    mode='a' if file_exists else 'w',
                                    header=not file_exists,
                                    index=False,
                                    encoding='utf-8-sig'
                                )
                        except Exception as csv_err:
                            print(f"âš ï¸  CSV ì €ì¥ ì‹¤íŒ¨ [idx={idx}]: {csv_err}")
                else:
                    # Fallback: RB ê²°ê³¼ ì‚¬ìš©
                    df.at[idx, "sentiment_final"] = df.at[idx, "sentiment_rb"]
                    df.at[idx, "sentiment_final_decision_rule"] = f"LLM failed: {result['error']}"
                    df.at[idx, "danger_final"] = df.at[idx, "danger_rb"]
                    df.at[idx, "danger_final_decision_rule"] = f"LLM failed: {result['error']}"
                    df.at[idx, "classified_at"] = timestamp
                    fail_count += 1

                    # ì‹¤íŒ¨ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
                    title = df.at[idx, 'title'] if pd.notna(df.at[idx, 'title']) else ""
                    print(f"\nâŒ ë¶„ë¥˜ ì‹¤íŒ¨ [idx={idx}]:")
                    print(f"   ì œëª©: {title[:80]}...")
                    print(f"   ì—ëŸ¬ íƒ€ì…: {result.get('error_type', 'Unknown')}")
                    print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {result['error']}")
                    # Tracebackì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì²« ì‹¤íŒ¨ë§Œ ì¶œë ¥
                    if fail_count == 1 and 'error_trace' in result:
                        print(f"   ìƒì„¸ Traceback (ì²« ì‹¤íŒ¨ë§Œ):\n{result['error_trace']}")

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                pbar.update(1)

        pbar.set_postfix({"ì²­í¬": f"{chunk_num}/{total_chunks}", "ì„±ê³µ": success_count, "ì‹¤íŒ¨": fail_count})

        # ì²­í¬ ì™„ë£Œ í†µê³„ ì¶œë ¥
        chunk_total = success_count + fail_count
        success_rate = (success_count / chunk_total * 100) if chunk_total > 0 else 0
        print(f"\n  ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ: ì„±ê³µ {success_count}/{chunk_total} ({success_rate:.1f}%)")

        # ì²­í¬ ê°„ ì§§ì€ ëŒ€ê¸° (rate limiting)
        if chunk_num < total_chunks:
            time.sleep(0.5)

    pbar.close()

    # ì „ì²´ í†µê³„ ì¶œë ¥
    total_processed = sum(1 for idx in indices_to_classify if pd.notna(df.at[idx, "classified_at"]))
    total_success = sum(1 for idx in indices_to_classify
                       if pd.notna(df.at[idx, "classified_at"])
                       and "LLM failed" not in str(df.at[idx, "sentiment_final_decision_rule"]))
    total_failed = total_processed - total_success
    overall_success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0

    print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ:")
    print(f"   ì´ ì²˜ë¦¬: {total_processed}ê°œ ê¸°ì‚¬")
    print(f"   ì„±ê³µ: {total_success}ê°œ ({overall_success_rate:.1f}%)")
    print(f"   ì‹¤íŒ¨: {total_failed}ê°œ ({100-overall_success_rate:.1f}%)")

    return df


def get_classification_stats(df: pd.DataFrame) -> Dict:
    """
    ë¶„ë¥˜ í†µê³„ ìƒì„±

    Args:
        df: ë¶„ì„ ê²°ê³¼ DataFrame

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    stats = {}

    # Sentiment distribution (Final)
    if "sentiment_final" in df.columns:
        sentiment_counts = df["sentiment_final"].value_counts().to_dict()
        stats["sentiment_final"] = sentiment_counts

    # Danger distribution (Final)
    if "danger_final" in df.columns:
        danger_counts = df["danger_final"].value_counts().to_dict()
        stats["danger_final"] = danger_counts

    # Brand scope distribution
    if "brand_scope_rb" in df.columns:
        scope_counts = df["brand_scope_rb"].value_counts().to_dict()
        stats["brand_scope_rb"] = scope_counts

    # Issue category distribution
    if "issue_category_rb" in df.columns:
        category_counts = df["issue_category_rb"].value_counts().to_dict()
        stats["issue_category_rb"] = category_counts

    # Average confidence scores
    if "sentiment_final_confidence" in df.columns:
        avg_sentiment_conf = df["sentiment_final_confidence"].apply(
            lambda x: float(x) if pd.notna(x) and x != "" else 0
        ).mean()
        stats["avg_sentiment_confidence"] = round(avg_sentiment_conf, 3)

    if "danger_final_confidence" in df.columns:
        avg_danger_conf = df["danger_final_confidence"].apply(
            lambda x: float(x) if pd.notna(x) and x != "" else 0
        ).mean()
        stats["avg_danger_confidence"] = round(avg_danger_conf, 3)

    return stats


def print_classification_stats(stats: Dict):
    """
    ë¶„ë¥˜ í†µê³„ ì¶œë ¥

    Args:
        stats: get_classification_stats() ê²°ê³¼
    """
    print("\nğŸ“Š ë¶„ë¥˜ í†µê³„:")

    if "sentiment_final" in stats:
        print("\n  Sentiment (Final):")
        for sentiment, count in stats["sentiment_final"].items():
            print(f"    - {sentiment}: {count}ê°œ")

    if "danger_final" in stats:
        print("\n  Danger (Final):")
        for danger, count in stats["danger_final"].items():
            print(f"    - {danger}: {count}ê°œ")

    if "brand_scope_rb" in stats:
        print("\n  Brand Scope:")
        for scope, count in stats["brand_scope_rb"].items():
            print(f"    - {scope}: {count}ê°œ")

    if "issue_category_rb" in stats:
        print("\n  Issue Category:")
        for category, count in sorted(stats["issue_category_rb"].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    - {category}: {count}ê°œ")

    if "avg_sentiment_confidence" in stats:
        print(f"\n  í‰ê·  Sentiment ì‹ ë¢°ë„: {stats['avg_sentiment_confidence']:.2%}")

    if "avg_danger_confidence" in stats:
        print(f"  í‰ê·  Danger ì‹ ë¢°ë„: {stats['avg_danger_confidence']:.2%}")
