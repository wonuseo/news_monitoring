"""
source_verifier.py - Source Verification & Topic Grouping (ì§„ì…ì )

ì„¸ ëª¨ë“ˆì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ì–‡ì€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°:
  - cluster_verifier   : Part A  â€” ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦
  - cross_query_merger : Part A-2 â€” Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©
  - topic_grouper      : Part B  â€” ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”

Source Labels:
  - ë³´ë„ìë£Œ: ë¸Œëœë“œ ê³µì‹ ë°°í¬ ë³´ë„ìë£Œ (LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ìœ¼ë¡œ íŒë‹¨)
  - ìœ ì‚¬ì£¼ì œ: ë…ë¦½ ê¸°ì‚¬, ê°™ì€ ì£¼ì œ (í´ëŸ¬ìŠ¤í„°ëì§€ë§Œ ë³´ë„ìë£Œ ê¸°ì¤€ ë¯¸ë‹¬)
  - ì¼ë°˜ê¸°ì‚¬: ë…ë¦½ ê¸°ì‚¬ (ê¸°ë³¸ê°’)
"""

import os
from typing import Dict, Tuple

import pandas as pd

from .cluster_verifier import llm_verify_cluster, verify_press_release_clusters
from .cross_query_merger import merge_cross_query_clusters
from .topic_grouper import discover_topic_groups, llm_verify_topic_similarity
from ._sv_common import llm_judge_component_representative, determine_verified_source

__all__ = [
    "verify_and_regroup_sources",
    "llm_verify_cluster",
    "verify_press_release_clusters",
    "merge_cross_query_clusters",
    "discover_topic_groups",
    "llm_verify_topic_similarity",
    "llm_judge_component_representative",
    "determine_verified_source",
]


def verify_and_regroup_sources(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Entry point: Part A â†’ A-2 â†’ B ìˆœì„œ ì‹¤í–‰ + ì¼ê´€ì„± ê²€ì¦.

    Args:
        df: ë¶„ë¥˜ ì™„ë£Œëœ DataFrame
        openai_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ fallback)

    Returns:
        (df, combined_stats) íŠœí”Œ
    """
    if openai_key is None:
        openai_key = os.getenv("OPENAI_API_KEY")

    print("\nğŸ“‹ Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì‹œì‘")

    # Part A: ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦
    df, verify_stats = verify_press_release_clusters(df, openai_key=openai_key)
    print(f"  Part A: ë³´ë„ìë£Œ ê²€ì¦ - {verify_stats['sv_clusters_verified']}ê°œ í´ëŸ¬ìŠ¤í„° ê²€ì¦")
    if verify_stats["sv_clusters_verified"] > 0:
        print(f"    - ë³´ë„ìë£Œ ìœ ì§€: {verify_stats['sv_kept_press_release']}ê°œ")
        print(f"    - ìœ ì‚¬ì£¼ì œ ì¬ë¶„ë¥˜: {verify_stats['sv_reclassified_similar_topic']}ê°œ")

    # Part A-2: Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©
    df, cross_stats = merge_cross_query_clusters(df, openai_key=openai_key)
    print(f"  Part A-2: Cross-query ë³‘í•© - {cross_stats['sv_cross_merged_groups']}ê°œ ê·¸ë£¹, "
          f"{cross_stats['sv_cross_merged_articles']}ê°œ ê¸°ì‚¬")

    # Part B: ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”
    df, topic_stats = discover_topic_groups(df, openai_key=openai_key)
    print(f"  Part B: ì£¼ì œ ê·¸ë£¹ ë°œê²¬ - {topic_stats['sv_new_topic_groups']}ê°œ ê·¸ë£¹, "
          f"{topic_stats['sv_new_topic_articles']}ê°œ ê¸°ì‚¬")
    if topic_stats.get("sv_llm_verified", 0) > 0 or topic_stats.get("sv_llm_rejected", 0) > 0:
        print(f"    - LLM ê²½ê³„ì„  ê²€ì¦: {topic_stats['sv_llm_verified']}ê°œ ì—°ê²°, "
              f"{topic_stats['sv_llm_rejected']}ê°œ ê±°ë¶€")

    combined = {**verify_stats, **cross_stats, **topic_stats}

    # ì¼ê´€ì„± ê²€ì¦
    if "cluster_id" in df.columns and "cluster_summary" in df.columns:
        # Rule 1: source=="ì¼ë°˜ê¸°ì‚¬" â†’ cluster_id="", cluster_summary=""
        general_mask = df["source"] == "ì¼ë°˜ê¸°ì‚¬"
        r1_count = (general_mask & (df["cluster_id"].astype(str).str.strip() != "")).sum()
        if r1_count > 0:
            df.loc[general_mask, "cluster_id"] = ""
            df.loc[general_mask, "cluster_summary"] = ""
            print(f"  ì¼ê´€ì„± Rule 1: ì¼ë°˜ê¸°ì‚¬ {r1_count}ê±´ì˜ cluster_id/cluster_summary ì´ˆê¸°í™”")

        # Rule 2: ê°™ì€ cluster_id â†’ ê°™ì€ cluster_summary (ì²« ë¹„ì–´ìˆì§€ ì•Šì€ ê°’ ì „íŒŒ)
        clustered = df["cluster_id"].astype(str).str.strip() != ""
        r2_count = 0
        if clustered.any():
            for cid, cgroup in df[clustered].groupby("cluster_id"):
                summaries = cgroup["cluster_summary"].dropna().astype(str)
                summaries = summaries[summaries.str.strip() != ""]
                if len(summaries) > 0:
                    first_val = summaries.iloc[0]
                    mismatch = clustered & (df["cluster_id"] == cid) & (df["cluster_summary"] != first_val)
                    fix_count = mismatch.sum()
                    if fix_count > 0:
                        df.loc[mismatch, "cluster_summary"] = first_val
                        r2_count += fix_count
        if r2_count > 0:
            print(f"  ì¼ê´€ì„± Rule 2: {r2_count}ê±´ì˜ cluster_summary ì „íŒŒ")

        # Rule 3: cluster_id ìˆëŠ”ë° cluster_summary ì—†ìœ¼ë©´ ê²½ê³ 
        r3_mask = clustered & (df["cluster_summary"].astype(str).str.strip() == "")
        r3_count = r3_mask.sum()
        if r3_count > 0:
            print(f"  ì¼ê´€ì„± Rule 3: {r3_count}ê±´ cluster_summary ëˆ„ë½ (summarize_clusters í˜¸ì¶œ ì˜ˆì •)")

    if "source" in df.columns:
        source_dist = df["source"].value_counts().to_dict()
        print(f"  Source ë¶„í¬: {source_dist}")

    print("âœ… Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì™„ë£Œ")
    return df, combined
