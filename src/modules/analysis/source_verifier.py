"""
source_verifier.py - Source Verification & Topic Grouping

LLM ë¶„ë¥˜ ê²°ê³¼ + LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ì„ í™œìš©í•œ ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° ê²€ì¦ ë° ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”.

Source Labels:
- ë³´ë„ìë£Œ: ë¸Œëœë“œ ê³µì‹ ë°°í¬ ë³´ë„ìë£Œ (LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ìœ¼ë¡œ íŒë‹¨)
- ìœ ì‚¬ì£¼ì œ: ë…ë¦½ ê¸°ì‚¬, ê°™ì€ ì£¼ì œ (í´ëŸ¬ìŠ¤í„°ëì§€ë§Œ ë³´ë„ìë£Œ ê¸°ì¤€ ë¯¸ë‹¬)
- ì¼ë°˜ê¸°ì‚¬: ë…ë¦½ ê¸°ì‚¬ (ê¸°ë³¸ê°’)
"""

import re
import os
from collections import deque
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

from src.modules.analysis.llm_engine import (
    load_source_verifier_prompts,
    render_prompt,
    call_openai_structured,
)
from src.utils.openai_client import load_api_models


PR_CATEGORIES = {
    "PR/ë³´ë„ìë£Œ", "ìƒí’ˆ/ì˜¤í¼ë§", "ì œíœ´/íŒŒíŠ¸ë„ˆì‹­",
    "ë¸Œëœë“œ/ë§ˆì¼€íŒ…", "ì´ë²¤íŠ¸/í”„ë¡œëª¨ì…˜", "ì‹œì„¤/ì˜¤í”ˆ",
    "ì‚¬ì—…/ì‹¤ì ", "ESG/ì‚¬íšŒ",
}

# Topic grouping thresholds
TOPIC_JACCARD_LOW_THRESHOLD = 0.35   # ì´í•˜: í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ
TOPIC_JACCARD_HIGH_THRESHOLD = 0.50  # ì´ìƒ: í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ
# 0.35 ~ 0.50 ì‚¬ì´: LLM ê²€ì¦ í•„ìš” (ê²½ê³„ì„  ì¼€ì´ìŠ¤)

# Cross-query merge thresholds (STEP 2 ëŒ€ë¹„ ì•½ê°„ ì™„í™”, ë‚ ì§œ ì œì•½ ì œê±°)
CROSS_TITLE_COS_THRESHOLD = 0.65
CROSS_TITLE_JAC_THRESHOLD = 0.15
CROSS_DESC_COS_THRESHOLD = 0.55
CROSS_DESC_JAC_THRESHOLD = 0.08
# LLM ê²½ê³„ì„  ë²”ìœ„
CROSS_TITLE_COS_BORDERLINE = (0.50, 0.65)  # [low, high)
CROSS_DESC_COS_BORDERLINE = (0.40, 0.55)   # [low, high)


def determine_verified_source(
    brand_relevance: str,
    sentiment_stage: str,
    news_category: str,
    date_spread_days: float,
) -> str:
    """
    ê·œì¹™ ê¸°ë°˜ source ê²€ì¦ (LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ fallback).

    ë³´ë„ìë£Œ ìœ ì§€ ì¡°ê±´ (AND):
    - brand_relevance=="ê´€ë ¨"
    - sentiment_stage NOT IN ["ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •"]
    - sentiment_stage=="ê¸ì •" OR (sentiment_stage=="ì¤‘ë¦½" AND news_category in PR_CATEGORIES)

    Edge case:
    - brand_relevance=="íŒë‹¨ í•„ìš”" â†’ ë³´ë„ìë£Œ ìœ ì§€
    - ë¹ˆ ê°’ (LLM ì‹¤íŒ¨) â†’ ë³´ë„ìë£Œ ìœ ì§€

    Returns: "ë³´ë„ìë£Œ" / "ìœ ì‚¬ì£¼ì œ"
    """
    # Edge case: LLM ë¯¸ë¶„ë¥˜ ë˜ëŠ” íŒë‹¨ í•„ìš” â†’ ë³´ë„ìë£Œ ìœ ì§€
    if not brand_relevance or brand_relevance == "íŒë‹¨ í•„ìš”":
        return "ë³´ë„ìë£Œ"

    # ë³´ë„ìë£Œ ìœ ì§€ ì¡°ê±´
    if brand_relevance == "ê´€ë ¨":
        # ë¶€ì •ì€ ì ˆëŒ€ ë³´ë„ìë£Œ ì•„ë‹˜ (ëª…ì‹œì  ì²´í¬)
        if sentiment_stage in ["ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •"]:
            return "ìœ ì‚¬ì£¼ì œ"

        if sentiment_stage == "ê¸ì •":
            return "ë³´ë„ìë£Œ"
        if sentiment_stage == "ì¤‘ë¦½" and news_category in PR_CATEGORIES:
            return "ë³´ë„ìë£Œ"

    # ë³´ë„ìë£Œ ê¸°ì¤€ ë¯¸ë‹¬ â†’ ìœ ì‚¬ì£¼ì œ
    return "ìœ ì‚¬ì£¼ì œ"


def _get_sv_model() -> str:
    """source_verification ëª¨ë¸ ë¡œë“œ."""
    api_models = load_api_models()
    return api_models.get("source_verification", "gpt-4o-mini")


def llm_verify_cluster(
    cluster_df: pd.DataFrame,
    query: str,
    press_release_group: str,
    openai_key: str,
) -> Optional[str]:
    """
    LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ ë³´ë„ìë£Œ/ìœ ì‚¬ì£¼ì œ ê²€ì¦ (1 cluster = 1 API call).

    Args:
        cluster_df: í´ëŸ¬ìŠ¤í„° ë‚´ ê¸°ì‚¬ DataFrame
        query: ê²€ìƒ‰ ë¸Œëœë“œ
        press_release_group: í´ëŸ¬ìŠ¤í„° ìš”ì•½
        openai_key: OpenAI API í‚¤

    Returns:
        "ë³´ë„ìë£Œ" / "ìœ ì‚¬ì£¼ì œ" ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    prompts = load_source_verifier_prompts()
    cv_config = prompts.get("cluster_verification", {})

    system_prompt = cv_config.get("system", "")
    user_template = cv_config.get("user_prompt_template", "")
    response_schema = cv_config.get("response_schema", {})

    if not system_prompt or not user_template:
        return None

    # ê¸°ì‚¬ ì œëª© ëª©ë¡ (ìµœëŒ€ 10ê°œ)
    titles = cluster_df["title"].tolist() if "title" in cluster_df.columns else []
    titles_display = titles[:10]
    titles_list = "\n".join(f"- {t}" for t in titles_display)
    if len(titles) > 10:
        titles_list += f"\n- ... ì™¸ {len(titles) - 10}ê°œ"

    # ëŒ€í‘œ ê¸°ì‚¬ LLM ë¶„ë¥˜ ê²°ê³¼ (ì²« ë²ˆì§¸ ë¶„ë¥˜ ì™„ë£Œ ê¸°ì‚¬)
    classified = cluster_df[cluster_df["brand_relevance"].astype(str).str.strip() != ""]
    if len(classified) > 0:
        rep = classified.iloc[0]
    else:
        rep = cluster_df.iloc[0]

    brand_relevance = str(rep.get("brand_relevance", ""))
    sentiment_stage = str(rep.get("sentiment_stage", ""))
    news_category = str(rep.get("news_category", ""))

    context = {
        "query": query,
        "press_release_group": press_release_group if press_release_group else "ì—†ìŒ",
        "article_count": str(len(cluster_df)),
        "titles_list": titles_list,
        "brand_relevance": brand_relevance,
        "sentiment_stage": sentiment_stage,
        "news_category": news_category,
    }

    user_prompt = render_prompt(user_template, context)
    model = _get_sv_model()

    try:
        result = call_openai_structured(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=response_schema,
            openai_key=openai_key,
            model=model,
            label="í´ëŸ¬ìŠ¤í„°ê²€ì¦",
            schema_name="cluster_verification_result",
        )

        if result and "source_type" in result:
            return result["source_type"]
        return None

    except Exception as e:
        print(f"  âš ï¸  LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return None


def verify_press_release_clusters(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Part A: í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ LLM ê²€ì¦ìœ¼ë¡œ ë³´ë„ìë£Œ/ìœ ì‚¬ì£¼ì œ êµ¬ë¶„.

    ê° cluster_idë³„ë¡œ LLMì´ ë³´ë„ìë£Œì¸ì§€ ìœ ì‚¬ì£¼ì œì¸ì§€ íŒë‹¨.
    LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback (determine_verified_source).

    Returns:
        (df, stats) íŠœí”Œ
    """
    df = df.copy()
    stats = {
        "sv_clusters_verified": 0,
        "sv_kept_press_release": 0,
        "sv_reclassified_similar_topic": 0,
    }

    if "source" not in df.columns:
        return df, stats

    pr_mask = df["source"] == "ë³´ë„ìë£Œ"
    if pr_mask.sum() == 0:
        return df, stats

    if "cluster_id" not in df.columns:
        return df, stats

    # cluster_idë³„ ê·¸ë£¹í•‘
    pr_df = df[pr_mask]
    cluster_groups = pr_df.groupby("cluster_id", dropna=False)
    stats["sv_clusters_verified"] = len(cluster_groups)

    for cluster_id, cluster_df in cluster_groups:
        verified_source = None

        # LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹œë„
        if openai_key:
            query = str(cluster_df["query"].iloc[0]) if "query" in cluster_df.columns else ""
            prg = str(cluster_df["press_release_group"].iloc[0]) if "press_release_group" in cluster_df.columns else ""
            verified_source = llm_verify_cluster(cluster_df, query, prg, openai_key)

        # LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback (ëŒ€í‘œ ê¸°ì‚¬ ê¸°ì¤€)
        if verified_source is None:
            rep = cluster_df.iloc[0]
            verified_source = determine_verified_source(
                brand_relevance=str(rep.get("brand_relevance", "")),
                sentiment_stage=str(rep.get("sentiment_stage", "")),
                news_category=str(rep.get("news_category", "")),
                date_spread_days=0,
            )

        # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ê¸°ì‚¬ì— ê²°ê³¼ ì ìš©
        for idx in cluster_df.index:
            df.at[idx, "source"] = verified_source

        if verified_source == "ë³´ë„ìë£Œ":
            stats["sv_kept_press_release"] += len(cluster_df)
        else:
            stats["sv_reclassified_similar_topic"] += len(cluster_df)

    return df, stats


def _tokenize_summary(summary: str) -> set:
    """news_keyword_summaryë¥¼ ê³µë°±ìœ¼ë¡œ í† í°í™”."""
    if not summary or not isinstance(summary, str):
        return set()
    return set(summary.strip().split())


def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """Jaccard similarity between two sets."""
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0


def llm_verify_topic_similarity(
    summary_a: str,
    summary_b: str,
    title_a: Optional[str] = None,
    title_b: Optional[str] = None,
    openai_key: str = None,
) -> bool:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê¸°ì‚¬ê°€ ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ”ì§€ íŒë‹¨.

    ê²½ê³„ì„  ì¼€ì´ìŠ¤ (Jaccard 0.35~0.50)ì—ì„œë§Œ í˜¸ì¶œí•˜ì—¬ ë¹„ìš© ìµœì†Œí™”.
    YAML prompt + Responses API (call_openai_structured) ì‚¬ìš©.

    Args:
        summary_a: ê¸°ì‚¬ Aì˜ news_keyword_summary
        summary_b: ê¸°ì‚¬ Bì˜ news_keyword_summary
        title_a: ê¸°ì‚¬ Aì˜ ì œëª© (optional)
        title_b: ê¸°ì‚¬ Bì˜ ì œëª© (optional)
        openai_key: OpenAI API í‚¤

    Returns:
        True if ê°™ì€ ì£¼ì œ, False otherwise
    """
    if not openai_key:
        openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print(f"  âš ï¸  OPENAI_API_KEY ì—†ìŒ, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    prompts = load_source_verifier_prompts()
    ts_config = prompts.get("topic_similarity", {})

    system_prompt = ts_config.get("system", "")
    user_template = ts_config.get("user_prompt_template", "")
    response_schema = ts_config.get("response_schema", {})

    if not system_prompt or not user_template:
        print(f"  âš ï¸  topic_similarity prompt ì—†ìŒ, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    # Context êµ¬ì„±
    context_a = f"ì œëª©: {title_a}\nìš”ì•½: {summary_a}" if title_a else f"ìš”ì•½: {summary_a}"
    context_b = f"ì œëª©: {title_b}\nìš”ì•½: {summary_b}" if title_b else f"ìš”ì•½: {summary_b}"

    context = {
        "context_a": context_a,
        "context_b": context_b,
    }

    user_prompt = render_prompt(user_template, context)
    model = _get_sv_model()

    try:
        result = call_openai_structured(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=response_schema,
            openai_key=openai_key,
            model=model,
            label="ì£¼ì œìœ ì‚¬ë„",
            schema_name="topic_similarity_result",
        )

        if result and "same_topic" in result:
            return result["same_topic"]

        print(f"  âš ï¸  LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    except Exception as e:
        print(f"  âš ï¸  LLM ê²€ì¦ ì‹¤íŒ¨: {e}, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False


def discover_topic_groups(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Part B: ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ ì¤‘ ê°™ì€ ì£¼ì œ ê·¸ë£¹ ë°œê²¬.

    news_keyword_summary í† í° Jaccard ìœ ì‚¬ë„ + news_category ì¼ì¹˜ +
    ê²½ê³„ì„  ì¼€ì´ìŠ¤ LLM ê²€ì¦ìœ¼ë¡œ ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ë¥¼ ê·¸ë£¹í™”.

    ì•Œê³ ë¦¬ì¦˜:
    1. ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ í•„í„°
    2. queryë³„ ê·¸ë£¹í•‘
    3. news_category ì¼ì¹˜ í•„ìˆ˜
    4. Jaccard similarity 3ë‹¨ê³„ ë¶„ë¥˜:
       - >= 0.50: í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ â†’ ì¦‰ì‹œ ì—°ê²°
       - < 0.35: í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ â†’ ê±´ë„ˆëœ€
       - 0.35 ~ 0.50: ê²½ê³„ì„  ì¼€ì´ìŠ¤ â†’ LLM ê²€ì¦
    5. BFS connected components â†’ 2+ ë©¤ë²„ë§Œ
    6. cluster_id format: "{query}_t{counter:05d}"
    7. source: ìœ ì‚¬ì£¼ì œ

    Returns:
        (df, stats) íŠœí”Œ
    """
    df = df.copy()
    stats = {
        "sv_new_topic_groups": 0,
        "sv_new_topic_articles": 0,
        "sv_llm_verified": 0,  # LLMì´ ê°™ì€ ì£¼ì œë¡œ íŒë‹¨í•œ ìŒ
        "sv_llm_rejected": 0,  # LLMì´ ë‹¤ë¥¸ ì£¼ì œë¡œ íŒë‹¨í•œ ìŒ
    }

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required = {"source", "news_keyword_summary", "news_category", "pub_datetime"}
    if not required.issubset(df.columns):
        return df, stats

    # ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ë§Œ ëŒ€ìƒ (ë¹„ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì œì™¸)
    general_mask = df["source"] == "ì¼ë°˜ê¸°ì‚¬"

    # news_categoryê°€ "ë¹„ê´€ë ¨"ì¸ ê¸°ì‚¬ëŠ” ì£¼ì œ ê·¸ë£¹í™”ì—ì„œ ì œì™¸
    if "news_category" in df.columns:
        general_mask = general_mask & (df["news_category"] != "ë¹„ê´€ë ¨")

    if general_mask.sum() < 2:
        return df, stats

    # cluster_id ì»¬ëŸ¼ ë³´ì¥
    if "cluster_id" not in df.columns:
        df["cluster_id"] = ""

    # query ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ
    if "query" not in df.columns:
        df["query"] = ""

    df_general = df[general_mask].copy()

    # queryë³„ ì²˜ë¦¬
    for query, q_group in df_general.groupby("query", dropna=False):
        if len(q_group) < 2:
            continue

        indices = q_group.index.tolist()

        # í† í°í™” + ì¹´í…Œê³ ë¦¬ ìºì‹± (nan ê°’ ëª…ì‹œì  ì²˜ë¦¬)
        token_cache = {}
        cat_cache = {}
        for idx in indices:
            # news_keyword_summary ì²˜ë¦¬ (nan â†’ ë¹ˆ set)
            summary_val = df.at[idx, "news_keyword_summary"]
            if pd.isna(summary_val):
                token_cache[idx] = set()
            else:
                token_cache[idx] = _tokenize_summary(str(summary_val))

            # news_category ì²˜ë¦¬ (nan â†’ None)
            cat_val = df.at[idx, "news_category"]
            if pd.isna(cat_val) or cat_val == "":
                cat_cache[idx] = None
            else:
                cat_cache[idx] = str(cat_val)

        # ìœ íš¨ í† í°ì´ ìˆëŠ” ê¸°ì‚¬ë§Œ (LLM ë¶„ë¥˜ ì„±ê³µí•œ ê¸°ì‚¬)
        valid_indices = [i for i in indices if len(token_cache[i]) > 0 and cat_cache[i] is not None]
        if len(valid_indices) < 2:
            continue

        # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (category ì¼ì¹˜ + Jaccard + LLM ê²½ê³„ì„  ê²€ì¦)
        adjacency = {i: [] for i in valid_indices}
        llm_verified_count = 0
        llm_rejected_count = 0

        for i in range(len(valid_indices)):
            for j in range(i + 1, len(valid_indices)):
                idx_a, idx_b = valid_indices[i], valid_indices[j]
                # news_category ì¼ì¹˜ í•„ìˆ˜ (None ì²´í¬ ë¶ˆí•„ìš” - valid_indicesì—ì„œ ì´ë¯¸ í•„í„°ë§ë¨)
                if cat_cache[idx_a] != cat_cache[idx_b]:
                    continue

                sim = _jaccard_similarity(token_cache[idx_a], token_cache[idx_b])

                # í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ (high threshold ì´ìƒ)
                if sim >= TOPIC_JACCARD_HIGH_THRESHOLD:
                    adjacency[idx_a].append(idx_b)
                    adjacency[idx_b].append(idx_a)
                # í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ (low threshold ì´í•˜)
                elif sim < TOPIC_JACCARD_LOW_THRESHOLD:
                    continue
                # ê²½ê³„ì„  ì¼€ì´ìŠ¤ (0.35 ~ 0.50): LLM ê²€ì¦
                else:
                    summary_a = str(df.at[idx_a, "news_keyword_summary"])
                    summary_b = str(df.at[idx_b, "news_keyword_summary"])
                    title_a = str(df.at[idx_a, "title"]) if "title" in df.columns else None
                    title_b = str(df.at[idx_b, "title"]) if "title" in df.columns else None

                    is_same = llm_verify_topic_similarity(
                        summary_a, summary_b, title_a, title_b,
                        openai_key=openai_key,
                    )

                    if is_same:
                        adjacency[idx_a].append(idx_b)
                        adjacency[idx_b].append(idx_a)
                        llm_verified_count += 1
                    else:
                        llm_rejected_count += 1

        # LLM ê²€ì¦ í†µê³„ ëˆ„ì  ë° ì¶œë ¥
        stats["sv_llm_verified"] += llm_verified_count
        stats["sv_llm_rejected"] += llm_rejected_count
        if llm_verified_count > 0 or llm_rejected_count > 0:
            print(f"    Query '{query}': LLM ê²½ê³„ì„  ê²€ì¦ {llm_verified_count}ê°œ ì—°ê²°, {llm_rejected_count}ê°œ ê±°ë¶€")

        # BFS connected components
        visited = set()
        components = []
        for start in valid_indices:
            if start in visited:
                continue
            component = []
            queue = deque([start])
            while queue:
                node = queue.popleft()
                if node in visited:
                    continue
                visited.add(node)
                component.append(node)
                for neighbor in adjacency[node]:
                    if neighbor not in visited:
                        queue.append(neighbor)
            if len(component) >= 2:
                components.append(component)

        # query ë¬¸ìì—´ ì •ë¦¬ (íŒŒì´í”„ í¬í•¨ ì‹œ ì²« ë²ˆì§¸)
        query_str = str(query) if query else "unknown"
        query_prefix = query_str.split("|")[0] if "|" in query_str else query_str

        # ê¸°ì¡´ topic cluster counter í™•ì¸
        existing_topic_ids = df["cluster_id"][
            df["cluster_id"].str.startswith(f"{query_prefix}_t", na=False)
        ]
        if len(existing_topic_ids) > 0:
            max_num = max(
                int(tid.split("_t")[-1])
                for tid in existing_topic_ids
                if tid.split("_t")[-1].isdigit()
            )
            counter = max_num + 1
        else:
            counter = 1

        # í´ëŸ¬ìŠ¤í„° í• ë‹¹
        for component in components:
            cid = f"{query_prefix}_t{counter:05d}"
            counter += 1

            for idx in component:
                df.at[idx, "cluster_id"] = cid
                df.at[idx, "source"] = "ìœ ì‚¬ì£¼ì œ"

            stats["sv_new_topic_groups"] += 1
            stats["sv_new_topic_articles"] += len(component)

    return df, stats


def _clean_html(s: str) -> str:
    """HTML íƒœê·¸ ì œê±° (press_release_detector ë°©ì‹ ì¬í™œìš©)."""
    if not isinstance(s, str):
        return ""
    s = re.sub(r"<[^>]+>", " ", s)
    s = s.replace("&quot;", " ").replace("&amp;", "&")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _tokenize_simple(s: str, min_token_len: int = 2) -> set:
    """ê°„ë‹¨ í† í°í™” â†’ set (Jaccardìš©)."""
    if not isinstance(s, str) or not s.strip():
        return set()
    s = s.lower()
    toks = re.findall(r"[ê°€-í£a-z0-9]+", s)
    return {t for t in toks if len(t) >= min_token_len}


def merge_cross_query_clusters(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©: ì„œë¡œ ë‹¤ë¥¸ queryë¡œ ìˆ˜ì§‘ëœ ë™ì¼ ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„°ë¥¼ ë³‘í•©.

    Part A(í´ëŸ¬ìŠ¤í„° ê²€ì¦) ì´í›„, Part B(ì£¼ì œ ê·¸ë£¹í™”) ì´ì „ì— ì‹¤í–‰.
    STEP 2(press_release_detector)ì—ì„œ queryë³„ë¡œ ë¶„ë¦¬ëœ í´ëŸ¬ìŠ¤í„° + ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ë¥¼
    TF-IDF cosine + Jaccard ìœ ì‚¬ë„ë¡œ cross-query ë³‘í•©.

    ì•Œê³ ë¦¬ì¦˜:
    1. í›„ë³´ ìˆ˜ì§‘: ê° clusterë³„ ëŒ€í‘œ ê¸°ì‚¬ + ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬
    2. TF-IDF char n-gram ë²¡í„°í™” (cross-query)
    3. Skip mask: ê°™ì€ cluster, ê°™ì€ query+ë¯¸í´ëŸ¬ìŠ¤í„° ìŒ ì œì™¸
    4. Auto-merge + LLM ê²½ê³„ì„  ê²€ì¦
    5. BFS connected components â†’ ë³‘í•© ì²˜ë¦¬

    Returns:
        (df, stats) íŠœí”Œ
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    df = df.copy()
    stats = {
        "sv_cross_merged_groups": 0,
        "sv_cross_merged_articles": 0,
    }

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required = {"source", "cluster_id", "title", "description"}
    if not required.issubset(df.columns):
        return df, stats

    # â”€â”€â”€ 1. í›„ë³´ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    candidates: List[Dict] = []  # {repr_idx, member_indices, cluster_id, query, source}

    # 1a. ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ê¸°ì‚¬ (ê°€ì¥ ë¹ ë¥¸ pub_datetime)
    clustered_mask = df["cluster_id"].astype(str).str.strip() != ""
    if clustered_mask.any():
        for cid, cgroup in df[clustered_mask].groupby("cluster_id"):
            if "pub_datetime" in df.columns:
                repr_idx = cgroup["pub_datetime"].astype(str).idxmin()
            else:
                repr_idx = cgroup.index[0]
            candidates.append({
                "repr_idx": repr_idx,
                "member_indices": cgroup.index.tolist(),
                "cluster_id": str(cid),
                "query": str(cgroup["query"].iloc[0]) if "query" in cgroup.columns else "",
                "source": str(cgroup["source"].iloc[0]),
            })

    # 1b. ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ (ë¹„ê´€ë ¨ ì œì™¸)
    unclustered_mask = (~clustered_mask) & (df["source"] == "ì¼ë°˜ê¸°ì‚¬")
    if "news_category" in df.columns:
        unclustered_mask = unclustered_mask & (df["news_category"] != "ë¹„ê´€ë ¨")
    for idx in df[unclustered_mask].index:
        candidates.append({
            "repr_idx": idx,
            "member_indices": [idx],
            "cluster_id": "",
            "query": str(df.at[idx, "query"]) if "query" in df.columns else "",
            "source": "ì¼ë°˜ê¸°ì‚¬",
        })

    n = len(candidates)
    if n < 2:
        return df, stats

    # â”€â”€â”€ 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ + TF-IDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    repr_indices = [c["repr_idx"] for c in candidates]
    title_texts = [_clean_html(str(df.at[idx, "title"])) for idx in repr_indices]
    desc_texts = [_clean_html(str(df.at[idx, "description"])) for idx in repr_indices]

    # Jaccardìš© token set
    title_toksets = [_tokenize_simple(t) for t in title_texts]
    desc_toksets = [_tokenize_simple(d) for d in desc_texts]

    # TF-IDF ë²¡í„°í™”
    try:
        vec_title = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=1)
        vec_desc = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=1)

        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬: ê³µë°± í•˜ë‚˜ë¼ë„ ë„£ì–´ì•¼ TfidfVectorizerê°€ ì‘ë™
        title_corpus = [t if t.strip() else " " for t in title_texts]
        desc_corpus = [d if d.strip() else " " for d in desc_texts]

        X_title = vec_title.fit_transform(title_corpus)
        X_desc = vec_desc.fit_transform(desc_corpus)

        sim_title = cosine_similarity(X_title)
        sim_desc = cosine_similarity(X_desc)
    except Exception as e:
        print(f"  âš ï¸  Cross-query TF-IDF ì‹¤íŒ¨: {e}")
        return df, stats

    # â”€â”€â”€ 3-5. Skip mask + ìœ ì‚¬ë„ ê¸°ë°˜ adjacency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    adjacency = {i: [] for i in range(n)}
    llm_calls = 0

    for i in range(n):
        for j in range(i + 1, n):
            ci, cj = candidates[i], candidates[j]

            # Skip: ê°™ì€ clusterì— ì†í•œ ìŒ
            if ci["cluster_id"] and ci["cluster_id"] == cj["cluster_id"]:
                continue

            # Skip: ê°™ì€ queryì´ë©´ì„œ ë‘˜ ë‹¤ ë¯¸í´ëŸ¬ìŠ¤í„° (STEP 2ì—ì„œ ì´ë¯¸ ì²˜ë¦¬)
            if ci["query"] == cj["query"] and not ci["cluster_id"] and not cj["cluster_id"]:
                continue

            t_cos = sim_title[i, j]
            d_cos = sim_desc[i, j]
            t_jac = _jaccard_similarity(title_toksets[i], title_toksets[j])
            d_jac = _jaccard_similarity(desc_toksets[i], desc_toksets[j])

            # Auto-merge: title ê¸°ì¤€
            if t_cos >= CROSS_TITLE_COS_THRESHOLD and t_jac >= CROSS_TITLE_JAC_THRESHOLD:
                adjacency[i].append(j)
                adjacency[j].append(i)
                continue

            # Auto-merge: description ê¸°ì¤€
            if d_cos >= CROSS_DESC_COS_THRESHOLD and d_jac >= CROSS_DESC_JAC_THRESHOLD:
                adjacency[i].append(j)
                adjacency[j].append(i)
                continue

            # LLM ê²½ê³„ì„  ê²€ì¦
            title_borderline = (CROSS_TITLE_COS_BORDERLINE[0] <= t_cos < CROSS_TITLE_COS_BORDERLINE[1])
            desc_borderline = (CROSS_DESC_COS_BORDERLINE[0] <= d_cos < CROSS_DESC_COS_BORDERLINE[1])

            if title_borderline or desc_borderline:
                # descë¥¼ summaryë¡œ ì „ë‹¬ (llm_verify_topic_similarity ì¬í™œìš©)
                title_a = str(df.at[ci["repr_idx"], "title"]) if "title" in df.columns else None
                title_b = str(df.at[cj["repr_idx"], "title"]) if "title" in df.columns else None
                desc_a = desc_texts[i][:200]
                desc_b = desc_texts[j][:200]

                is_same = llm_verify_topic_similarity(
                    summary_a=desc_a, summary_b=desc_b,
                    title_a=title_a, title_b=title_b,
                    openai_key=openai_key,
                )
                llm_calls += 1

                if is_same:
                    adjacency[i].append(j)
                    adjacency[j].append(i)

    # â”€â”€â”€ 6. BFS connected components â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    visited = set()
    components = []
    for start in range(n):
        if start in visited:
            continue
        component = []
        queue = deque([start])
        while queue:
            node = queue.popleft()
            if node in visited:
                continue
            visited.add(node)
            component.append(node)
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    queue.append(neighbor)
        if len(component) >= 2:
            components.append(component)

    # â”€â”€â”€ 7. ë³‘í•© ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cross_counter = 1
    for component in components:
        comp_candidates = [candidates[ci] for ci in component]

        # target cluster_id: ê¸°ì¡´ cluster_id ì¤‘ ê°€ì¥ ì‘ì€ ê²ƒ
        existing_cids = sorted([c["cluster_id"] for c in comp_candidates if c["cluster_id"]])
        if existing_cids:
            target_cid = existing_cids[0]
        else:
            target_cid = f"cross_m{cross_counter:05d}"
            cross_counter += 1

        # target source: component ë‚´ "ë³´ë„ìë£Œ" ìˆìœ¼ë©´ "ë³´ë„ìë£Œ", ì•„ë‹ˆë©´ "ìœ ì‚¬ì£¼ì œ"
        sources_in_comp = {c["source"] for c in comp_candidates}
        target_source = "ë³´ë„ìë£Œ" if "ë³´ë„ìë£Œ" in sources_in_comp else "ìœ ì‚¬ì£¼ì œ"

        # target press_release_group: ê¸°ì¡´ ê°’ ì¤‘ ì²« ë²ˆì§¸
        target_prg = ""
        if "press_release_group" in df.columns:
            for c in comp_candidates:
                for midx in c["member_indices"]:
                    val = str(df.at[midx, "press_release_group"]).strip()
                    if val and val != "nan":
                        target_prg = val
                        break
                if target_prg:
                    break

        # ëª¨ë“  member_indicesì— ëŒ€í•´ ì—…ë°ì´íŠ¸
        total_members = 0
        for c in comp_candidates:
            for midx in c["member_indices"]:
                df.at[midx, "cluster_id"] = target_cid
                df.at[midx, "source"] = target_source
                if target_prg and "press_release_group" in df.columns:
                    df.at[midx, "press_release_group"] = target_prg
                total_members += 1

        stats["sv_cross_merged_groups"] += 1
        stats["sv_cross_merged_articles"] += total_members

    if llm_calls > 0:
        print(f"    Cross-query LLM ê²½ê³„ì„  ê²€ì¦: {llm_calls}íšŒ í˜¸ì¶œ")

    return df, stats


def verify_and_regroup_sources(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Entry point: Part A (ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦) + Part B (ì£¼ì œ ê·¸ë£¹ ë°œê²¬) ì‹¤í–‰.

    Args:
        df: ë¶„ë¥˜ ì™„ë£Œëœ DataFrame
        openai_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ fallback)

    Returns:
        (df, combined_stats) íŠœí”Œ
    """
    if openai_key is None:
        openai_key = os.getenv("OPENAI_API_KEY")

    print("\nğŸ“‹ Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì‹œì‘")

    # Part A: ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦
    df, verify_stats = verify_press_release_clusters(df, openai_key=openai_key)
    print(f"  Part A: ë³´ë„ìë£Œ ê²€ì¦ - {verify_stats['sv_clusters_verified']}ê°œ í´ëŸ¬ìŠ¤í„° ê²€ì¦")
    if verify_stats["sv_clusters_verified"] > 0:
        print(f"    - ë³´ë„ìë£Œ ìœ ì§€: {verify_stats['sv_kept_press_release']}ê°œ")
        print(f"    - ìœ ì‚¬ì£¼ì œ ì¬ë¶„ë¥˜: {verify_stats['sv_reclassified_similar_topic']}ê°œ")

    # Part A-2: Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©
    df, cross_stats = merge_cross_query_clusters(df, openai_key=openai_key)
    print(f"  Part A-2: Cross-query ë³‘í•© - {cross_stats['sv_cross_merged_groups']}ê°œ ê·¸ë£¹, "
          f"{cross_stats['sv_cross_merged_articles']}ê°œ ê¸°ì‚¬")

    # Part B: ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”
    df, topic_stats = discover_topic_groups(df, openai_key=openai_key)
    print(f"  Part B: ì£¼ì œ ê·¸ë£¹ ë°œê²¬ - {topic_stats['sv_new_topic_groups']}ê°œ ê·¸ë£¹, "
          f"{topic_stats['sv_new_topic_articles']}ê°œ ê¸°ì‚¬")
    if topic_stats.get("sv_llm_verified", 0) > 0 or topic_stats.get("sv_llm_rejected", 0) > 0:
        print(f"    - LLM ê²½ê³„ì„  ê²€ì¦: {topic_stats['sv_llm_verified']}ê°œ ì—°ê²°, "
              f"{topic_stats['sv_llm_rejected']}ê°œ ê±°ë¶€")

    # í†µí•© í†µê³„
    combined = {**verify_stats, **cross_stats, **topic_stats}

    # Source ë¶„í¬ ì¶œë ¥
    if "source" in df.columns:
        source_dist = df["source"].value_counts().to_dict()
        print(f"  Source ë¶„í¬: {source_dist}")

    print("âœ… Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì™„ë£Œ")
    return df, combined
