"""
source_verifier.py - Source Verification & Topic Grouping

LLM ë¶„ë¥˜ ê²°ê³¼ + LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ì„ í™œìš©í•œ ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° ê²€ì¦ ë° ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”.

Source Labels:
- ë³´ë„ìë£Œ: ë¸Œëœë“œ ê³µì‹ ë°°í¬ ë³´ë„ìë£Œ (LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ìœ¼ë¡œ íŒë‹¨)
- ìœ ì‚¬ì£¼ì œ: ë…ë¦½ ê¸°ì‚¬, ê°™ì€ ì£¼ì œ (í´ëŸ¬ìŠ¤í„°ëì§€ë§Œ ë³´ë„ìë£Œ ê¸°ì¤€ ë¯¸ë‹¬)
- ì¼ë°˜ê¸°ì‚¬: ë…ë¦½ ê¸°ì‚¬ (ê¸°ë³¸ê°’)
"""

import re
import os
from collections import deque
from typing import Dict, List, Tuple, Optional
from urllib.parse import urlparse

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.modules.analysis.llm_engine import (
    load_source_verifier_prompts,
    load_prompts,
    analyze_article_llm,
    render_prompt,
    call_openai_structured,
)
from src.utils.openai_client import load_api_models


PR_CATEGORIES = {
    "PR/ë³´ë„ìë£Œ", "ìƒí’ˆ/ì˜¤í¼ë§", "ì œíœ´/íŒŒíŠ¸ë„ˆì‹­",
    "ë¸Œëœë“œ/ë§ˆì¼€íŒ…", "ì´ë²¤íŠ¸/í”„ë¡œëª¨ì…˜", "ì‹œì„¤/ì˜¤í”ˆ",
    "ì‚¬ì—…/ì‹¤ì ", "ESG/ì‚¬íšŒ",
}

# Topic grouping thresholds
TOPIC_JACCARD_LOW_THRESHOLD = 0.35   # ì´í•˜: í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ
TOPIC_JACCARD_HIGH_THRESHOLD = 0.50  # ì´ìƒ: í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ
# 0.35 ~ 0.50 ì‚¬ì´: LLM ê²€ì¦ í•„ìš” (ê²½ê³„ì„  ì¼€ì´ìŠ¤)

# Cross-query merge thresholds (STEP 2 ëŒ€ë¹„ ì•½ê°„ ì™„í™”, ë‚ ì§œ ì œì•½ ì œê±°)
CROSS_TITLE_COS_THRESHOLD = 0.65
CROSS_TITLE_JAC_THRESHOLD = 0.15
CROSS_DESC_COS_THRESHOLD = 0.55
CROSS_DESC_JAC_THRESHOLD = 0.08
# LLM ê²½ê³„ì„  ë²”ìœ„
CROSS_TITLE_COS_BORDERLINE = (0.58, 0.65)  # [low, high)
CROSS_TITLE_JAC_BORDERLINE_MIN = 0.20
CROSS_DESC_COS_BORDERLINE = (0.48, 0.55)   # [low, high)
CROSS_DESC_JAC_BORDERLINE_MIN = 0.12
CROSS_BORDERLINE_DAY_HINT_MAX = 1  # Î”days <= 1


_article_prompts_cache: Optional[dict] = None


def _get_article_prompts() -> Optional[dict]:
    """article ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ìºì‹œ ë¡œë“œ."""
    global _article_prompts_cache
    if _article_prompts_cache is not None:
        return _article_prompts_cache

    try:
        _article_prompts_cache = load_prompts()
    except Exception as e:
        print(f"  âš ï¸  article prompts ë¡œë“œ ì‹¤íŒ¨: {e}")
        _article_prompts_cache = None

    return _article_prompts_cache


def _build_article_payload(df: pd.DataFrame, idx: int) -> Dict[str, str]:
    """DataFrame row indexë¥¼ analyze_article_llm ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜."""
    return {
        "query": str(df.at[idx, "query"]) if "query" in df.columns else "",
        "group": str(df.at[idx, "group"]) if "group" in df.columns else "",
        "title": str(df.at[idx, "title"]) if "title" in df.columns else "",
        "description": str(df.at[idx, "description"]) if "description" in df.columns else "",
    }


def _select_representative_index(df: pd.DataFrame, row_indices: List[int]) -> Optional[int]:
    """
    ì»´í¬ë„ŒíŠ¸ ëŒ€í‘œ ê¸°ì‚¬ ì„ íƒ:
    1) ê°€ì¥ ì´ë¥¸ pub_datetime
    2) ë™ì¼/ê²°ì¸¡ì´ë©´ ì •ë³´ëŸ‰(title+description ê¸¸ì´) ë†’ì€ ê¸°ì‚¬
    """
    if not row_indices:
        return None

    has_pub = "pub_datetime" in df.columns
    has_title = "title" in df.columns
    has_desc = "description" in df.columns

    best_idx = None
    best_key = None

    for idx in row_indices:
        dt = pd.to_datetime(df.at[idx, "pub_datetime"], errors="coerce", utc=True) if has_pub else pd.NaT
        has_dt = 0 if pd.notna(dt) else 1
        dt_key = dt.value if pd.notna(dt) else float("inf")
        info_len = (
            len(str(df.at[idx, "title"])) if has_title else 0
        ) + (
            len(str(df.at[idx, "description"])) if has_desc else 0
        )
        key = (has_dt, dt_key, -info_len, idx)
        if best_key is None or key < best_key:
            best_key = key
            best_idx = idx

    return best_idx


def _extract_media_key(df: pd.DataFrame, idx: int) -> str:
    """same_media íŒë³„ìš© í‚¤ ì¶”ì¶œ (media_domain ìš°ì„ , ì—†ìœ¼ë©´ link ë„ë©”ì¸)."""
    if "media_domain" in df.columns:
        media_domain = str(df.at[idx, "media_domain"]).strip().lower()
        if media_domain and media_domain != "nan":
            return media_domain

    if "link" in df.columns:
        link = str(df.at[idx, "link"]).strip()
        if link and link != "nan":
            try:
                return urlparse(link).netloc.lower()
            except Exception:
                return ""

    return ""


def llm_judge_component_representative(
    article: Dict[str, str],
    openai_key: str,
    mode: str = "cross_query_borderline",
) -> bool:
    """
    ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ ëŒ€í‘œ ê¸°ì‚¬ 1ê±´ë§Œ LLM ë¶„ë¥˜í•´ ì»´í¬ë„ŒíŠ¸ ìŠ¹ì¸ ì—¬ë¶€ ê²°ì •.

    mode:
      - press_release_borderline: ë³´ë„ìë£Œì„± ì—„ê²© ê²Œì´íŠ¸
      - cross_query_borderline: cross-query ë³‘í•© ê²Œì´íŠ¸
      - topic_group_borderline: ì¼ë°˜ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹ ê²Œì´íŠ¸
    """
    if not openai_key:
        return False

    prompts = _get_article_prompts()
    if not prompts:
        return False

    try:
        result = analyze_article_llm(article, prompts, openai_key)
    except Exception as e:
        print(f"  âš ï¸  ì»´í¬ë„ŒíŠ¸ ëŒ€í‘œê¸°ì‚¬ LLM ì‹¤íŒ¨: {e}")
        return False

    if not result:
        return False

    brand_rel = str(result.get("brand_relevance", "")).strip()
    sentiment = str(result.get("sentiment_stage", "")).strip()
    news_category = str(result.get("news_category", "")).strip()

    if mode == "press_release_borderline":
        if brand_rel != "ê´€ë ¨":
            return False
        if sentiment in {"ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •"}:
            return False
        return news_category in PR_CATEGORIES

    if mode == "cross_query_borderline":
        return brand_rel in {"ê´€ë ¨", "ì–¸ê¸‰"} and news_category != "ë¹„ê´€ë ¨"

    # topic_group_borderline
    return brand_rel in {"ê´€ë ¨", "ì–¸ê¸‰"} and news_category != "ë¹„ê´€ë ¨"


def determine_verified_source(
    brand_relevance: str,
    sentiment_stage: str,
    news_category: str,
    date_spread_days: float,
) -> str:
    """
    ê·œì¹™ ê¸°ë°˜ source ê²€ì¦ (LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹¤íŒ¨ ì‹œ fallback).

    ë³´ë„ìë£Œ ìœ ì§€ ì¡°ê±´ (AND):
    - brand_relevance=="ê´€ë ¨"
    - sentiment_stage NOT IN ["ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •"]
    - sentiment_stage=="ê¸ì •" OR (sentiment_stage=="ì¤‘ë¦½" AND news_category in PR_CATEGORIES)

    Edge case:
    - brand_relevance=="íŒë‹¨ í•„ìš”" â†’ ë³´ë„ìë£Œ ìœ ì§€
    - ë¹ˆ ê°’ (LLM ì‹¤íŒ¨) â†’ ë³´ë„ìë£Œ ìœ ì§€

    Returns: "ë³´ë„ìë£Œ" / "ìœ ì‚¬ì£¼ì œ"
    """
    # Edge case: LLM ë¯¸ë¶„ë¥˜ ë˜ëŠ” íŒë‹¨ í•„ìš” â†’ ë³´ë„ìë£Œ ìœ ì§€
    if not brand_relevance or brand_relevance == "íŒë‹¨ í•„ìš”":
        return "ë³´ë„ìë£Œ"

    # ë³´ë„ìë£Œ ìœ ì§€ ì¡°ê±´
    if brand_relevance == "ê´€ë ¨":
        # ë¶€ì •ì€ ì ˆëŒ€ ë³´ë„ìë£Œ ì•„ë‹˜ (ëª…ì‹œì  ì²´í¬)
        if sentiment_stage in ["ë¶€ì • í›„ë³´", "ë¶€ì • í™•ì •"]:
            return "ìœ ì‚¬ì£¼ì œ"

        if sentiment_stage == "ê¸ì •":
            return "ë³´ë„ìë£Œ"
        if sentiment_stage == "ì¤‘ë¦½" and news_category in PR_CATEGORIES:
            return "ë³´ë„ìë£Œ"

    # ë³´ë„ìë£Œ ê¸°ì¤€ ë¯¸ë‹¬ â†’ ìœ ì‚¬ì£¼ì œ
    return "ìœ ì‚¬ì£¼ì œ"


def _get_sv_model() -> str:
    """source_verification ëª¨ë¸ ë¡œë“œ."""
    api_models = load_api_models()
    return api_models.get("source_verification", "gpt-4o-mini")


def llm_verify_cluster(
    cluster_df: pd.DataFrame,
    query: str,
    cluster_summary: str,
    openai_key: str,
) -> Optional[str]:
    """
    LLMìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ ë³´ë„ìë£Œ/ìœ ì‚¬ì£¼ì œ ê²€ì¦ (1 cluster = 1 API call).

    Args:
        cluster_df: í´ëŸ¬ìŠ¤í„° ë‚´ ê¸°ì‚¬ DataFrame
        query: ê²€ìƒ‰ ë¸Œëœë“œ
        cluster_summary: í´ëŸ¬ìŠ¤í„° ìš”ì•½
        openai_key: OpenAI API í‚¤

    Returns:
        "ë³´ë„ìë£Œ" / "ìœ ì‚¬ì£¼ì œ" ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    prompts = load_source_verifier_prompts()
    cv_config = prompts.get("cluster_verification", {})

    system_prompt = cv_config.get("system", "")
    user_template = cv_config.get("user_prompt_template", "")
    response_schema = cv_config.get("response_schema", {})

    if not system_prompt or not user_template:
        return None

    # ê¸°ì‚¬ ì œëª© ëª©ë¡ (ìµœëŒ€ 10ê°œ)
    titles = cluster_df["title"].tolist() if "title" in cluster_df.columns else []
    titles_display = titles[:10]
    titles_list = "\n".join(f"- {t}" for t in titles_display)
    if len(titles) > 10:
        titles_list += f"\n- ... ì™¸ {len(titles) - 10}ê°œ"

    # ëŒ€í‘œ ê¸°ì‚¬ LLM ë¶„ë¥˜ ê²°ê³¼ (ì²« ë²ˆì§¸ ë¶„ë¥˜ ì™„ë£Œ ê¸°ì‚¬)
    classified = cluster_df[cluster_df["brand_relevance"].astype(str).str.strip() != ""]
    if len(classified) > 0:
        rep = classified.iloc[0]
    else:
        rep = cluster_df.iloc[0]

    brand_relevance = str(rep.get("brand_relevance", ""))
    sentiment_stage = str(rep.get("sentiment_stage", ""))
    news_category = str(rep.get("news_category", ""))

    context = {
        "query": query,
        "cluster_summary": cluster_summary if cluster_summary else "ì—†ìŒ",
        "article_count": str(len(cluster_df)),
        "titles_list": titles_list,
        "brand_relevance": brand_relevance,
        "sentiment_stage": sentiment_stage,
        "news_category": news_category,
    }

    user_prompt = render_prompt(user_template, context)
    model = _get_sv_model()

    try:
        result = call_openai_structured(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=response_schema,
            openai_key=openai_key,
            model=model,
            label="í´ëŸ¬ìŠ¤í„°ê²€ì¦",
            schema_name="cluster_verification_result",
        )

        if result and "source_type" in result:
            return result["source_type"]
        return None

    except Exception as e:
        print(f"  âš ï¸  LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return None


def verify_press_release_clusters(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Part A: í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ LLM ê²€ì¦ìœ¼ë¡œ ë³´ë„ìë£Œ/ìœ ì‚¬ì£¼ì œ êµ¬ë¶„.

    ê° cluster_idë³„ë¡œ LLMì´ ë³´ë„ìë£Œì¸ì§€ ìœ ì‚¬ì£¼ì œì¸ì§€ íŒë‹¨.
    LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback (determine_verified_source).

    Returns:
        (df, stats) íŠœí”Œ
    """
    df = df.copy()
    stats = {
        "sv_clusters_verified": 0,
        "sv_kept_press_release": 0,
        "sv_reclassified_similar_topic": 0,
    }

    if "source" not in df.columns:
        return df, stats

    pr_mask = df["source"] == "ë³´ë„ìë£Œ"
    if pr_mask.sum() == 0:
        return df, stats

    if "cluster_id" not in df.columns:
        return df, stats

    # cluster_idë³„ ê·¸ë£¹í•‘
    pr_df = df[pr_mask]
    cluster_groups = list(pr_df.groupby("cluster_id", dropna=False))
    stats["sv_clusters_verified"] = len(cluster_groups)
    total_cl = len(cluster_groups)

    pbar = tqdm(total=total_cl, desc="    PR í´ëŸ¬ìŠ¤í„° ê²€ì¦", unit="í´ëŸ¬ìŠ¤í„°", leave=False)

    for cl_idx, (cluster_id, cluster_df) in enumerate(cluster_groups, 1):
        print(f"    [{cl_idx}/{total_cl}] í´ëŸ¬ìŠ¤í„° '{cluster_id}' ({len(cluster_df)}ê°œ ê¸°ì‚¬) ê²€ì¦ ì¤‘...", end=" ")
        verified_source = None

        # LLM í´ëŸ¬ìŠ¤í„° ê²€ì¦ ì‹œë„
        if openai_key:
            query = str(cluster_df["query"].iloc[0]) if "query" in cluster_df.columns else ""
            cs = str(cluster_df["cluster_summary"].iloc[0]) if "cluster_summary" in cluster_df.columns else ""
            verified_source = llm_verify_cluster(cluster_df, query, cs, openai_key)

        # LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback (ëŒ€í‘œ ê¸°ì‚¬ ê¸°ì¤€)
        if verified_source is None:
            rep = cluster_df.iloc[0]
            verified_source = determine_verified_source(
                brand_relevance=str(rep.get("brand_relevance", "")),
                sentiment_stage=str(rep.get("sentiment_stage", "")),
                news_category=str(rep.get("news_category", "")),
                date_spread_days=0,
            )

        # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ê¸°ì‚¬ì— ê²°ê³¼ ì ìš©
        for idx in cluster_df.index:
            df.at[idx, "source"] = verified_source

        if verified_source == "ë³´ë„ìë£Œ":
            stats["sv_kept_press_release"] += len(cluster_df)
            print(f"â†’ ë³´ë„ìë£Œ")
        else:
            stats["sv_reclassified_similar_topic"] += len(cluster_df)
            print(f"â†’ ìœ ì‚¬ì£¼ì œ")

        pbar.set_postfix({
            "ë³´ë„ìë£Œìœ ì§€": stats["sv_kept_press_release"],
            "ìœ ì‚¬ì£¼ì œì „í™˜": stats["sv_reclassified_similar_topic"],
        })
        pbar.update(1)

    pbar.close()

    return df, stats


def _tokenize_summary(summary: str) -> set:
    """news_keyword_summaryë¥¼ ê³µë°±ìœ¼ë¡œ í† í°í™”."""
    if not summary or not isinstance(summary, str):
        return set()
    return set(summary.strip().split())


def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """Jaccard similarity between two sets."""
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0


def llm_verify_topic_similarity(
    summary_a: str,
    summary_b: str,
    title_a: Optional[str] = None,
    title_b: Optional[str] = None,
    openai_key: str = None,
) -> bool:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê¸°ì‚¬ê°€ ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ”ì§€ íŒë‹¨.

    ê²½ê³„ì„  ì¼€ì´ìŠ¤ (Jaccard 0.35~0.50)ì—ì„œë§Œ í˜¸ì¶œí•˜ì—¬ ë¹„ìš© ìµœì†Œí™”.
    YAML prompt + Responses API (call_openai_structured) ì‚¬ìš©.

    Args:
        summary_a: ê¸°ì‚¬ Aì˜ news_keyword_summary
        summary_b: ê¸°ì‚¬ Bì˜ news_keyword_summary
        title_a: ê¸°ì‚¬ Aì˜ ì œëª© (optional)
        title_b: ê¸°ì‚¬ Bì˜ ì œëª© (optional)
        openai_key: OpenAI API í‚¤

    Returns:
        True if ê°™ì€ ì£¼ì œ, False otherwise
    """
    if not openai_key:
        openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print(f"  âš ï¸  OPENAI_API_KEY ì—†ìŒ, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    prompts = load_source_verifier_prompts()
    ts_config = prompts.get("topic_similarity", {})

    system_prompt = ts_config.get("system", "")
    user_template = ts_config.get("user_prompt_template", "")
    response_schema = ts_config.get("response_schema", {})

    if not system_prompt or not user_template:
        print(f"  âš ï¸  topic_similarity prompt ì—†ìŒ, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    # Context êµ¬ì„±
    context_a = f"ì œëª©: {title_a}\nìš”ì•½: {summary_a}" if title_a else f"ìš”ì•½: {summary_a}"
    context_b = f"ì œëª©: {title_b}\nìš”ì•½: {summary_b}" if title_b else f"ìš”ì•½: {summary_b}"

    context = {
        "context_a": context_a,
        "context_b": context_b,
    }

    user_prompt = render_prompt(user_template, context)
    model = _get_sv_model()

    try:
        result = call_openai_structured(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=response_schema,
            openai_key=openai_key,
            model=model,
            label="ì£¼ì œìœ ì‚¬ë„",
            schema_name="topic_similarity_result",
        )

        if result and "same_topic" in result:
            return result["same_topic"]

        print(f"  âš ï¸  LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False

    except Exception as e:
        print(f"  âš ï¸  LLM ê²€ì¦ ì‹¤íŒ¨: {e}, ë³´ìˆ˜ì ìœ¼ë¡œ False ë°˜í™˜")
        return False


def discover_topic_groups(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Part B: ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ ì¤‘ ê°™ì€ ì£¼ì œ ê·¸ë£¹ ë°œê²¬.

    news_keyword_summary í† í° Jaccard ìœ ì‚¬ë„ + news_category ì¼ì¹˜ +
    ê²½ê³„ì„  ì¼€ì´ìŠ¤ LLM ê²€ì¦ìœ¼ë¡œ ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ë¥¼ ê·¸ë£¹í™”.

    ì•Œê³ ë¦¬ì¦˜:
    1. ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ í•„í„°
    2. queryë³„ ê·¸ë£¹í•‘
    3. news_category ì¼ì¹˜ í•„ìˆ˜
    4. Jaccard similarity 3ë‹¨ê³„ ë¶„ë¥˜:
       - >= 0.50: í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ â†’ ì¦‰ì‹œ ì—°ê²°
       - < 0.35: í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ â†’ ê±´ë„ˆëœ€
       - 0.35 ~ 0.50: ê²½ê³„ì„  ì¼€ì´ìŠ¤ â†’ LLM ê²€ì¦
    5. BFS connected components â†’ 2+ ë©¤ë²„ë§Œ
    6. cluster_id format: "{query}_t{counter:05d}"
    7. source: ìœ ì‚¬ì£¼ì œ

    Returns:
        (df, stats) íŠœí”Œ
    """
    df = df.copy()
    stats = {
        "sv_new_topic_groups": 0,
        "sv_new_topic_articles": 0,
        "sv_llm_verified": 0,  # LLMì´ ê°™ì€ ì£¼ì œë¡œ ìŠ¹ì¸í•œ ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ ìˆ˜
        "sv_llm_rejected": 0,  # LLMì´ ê±°ë¶€í•œ ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ ìˆ˜
    }

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required = {"source", "news_keyword_summary", "news_category", "pub_datetime"}
    if not required.issubset(df.columns):
        return df, stats

    # ë¹„í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ë§Œ ëŒ€ìƒ (ë¹„ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì œì™¸)
    general_mask = df["source"] == "ì¼ë°˜ê¸°ì‚¬"

    # news_categoryê°€ "ë¹„ê´€ë ¨"ì¸ ê¸°ì‚¬ëŠ” ì£¼ì œ ê·¸ë£¹í™”ì—ì„œ ì œì™¸
    if "news_category" in df.columns:
        general_mask = general_mask & (df["news_category"] != "ë¹„ê´€ë ¨")

    if general_mask.sum() < 2:
        return df, stats

    # cluster_id ì»¬ëŸ¼ ë³´ì¥
    if "cluster_id" not in df.columns:
        df["cluster_id"] = ""

    # query ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ
    if "query" not in df.columns:
        df["query"] = ""

    df_general = df[general_mask].copy()

    # queryë³„ ì²˜ë¦¬
    query_groups = list(df_general.groupby("query", dropna=False))
    total_queries = len(query_groups)

    for q_idx, (query, q_group) in enumerate(query_groups, 1):
        if len(q_group) < 2:
            continue

        print(f"    [{q_idx}/{total_queries}] Query '{query}' ({len(q_group)}ê°œ ì¼ë°˜ê¸°ì‚¬) ì£¼ì œ ê·¸ë£¹ íƒìƒ‰ ì¤‘...")
        indices = q_group.index.tolist()

        # í† í°í™” + ì¹´í…Œê³ ë¦¬ ìºì‹± (nan ê°’ ëª…ì‹œì  ì²˜ë¦¬)
        token_cache = {}
        cat_cache = {}
        for idx in indices:
            # news_keyword_summary ì²˜ë¦¬ (nan â†’ ë¹ˆ set)
            summary_val = df.at[idx, "news_keyword_summary"]
            if pd.isna(summary_val):
                token_cache[idx] = set()
            else:
                token_cache[idx] = _tokenize_summary(str(summary_val))

            # news_category ì²˜ë¦¬ (nan â†’ None)
            cat_val = df.at[idx, "news_category"]
            if pd.isna(cat_val) or cat_val == "":
                cat_cache[idx] = None
            else:
                cat_cache[idx] = str(cat_val)

        # ìœ íš¨ í† í°ì´ ìˆëŠ” ê¸°ì‚¬ë§Œ (LLM ë¶„ë¥˜ ì„±ê³µí•œ ê¸°ì‚¬)
        valid_indices = [i for i in indices if len(token_cache[i]) > 0 and cat_cache[i] is not None]
        if len(valid_indices) < 2:
            continue

        # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (category ì¼ì¹˜ + Jaccard)
        # ê²½ê³„ì„ (0.35~0.50)ì€ pair-level LLM ëŒ€ì‹  ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ LLM 1íšŒ ê²€ì¦
        adjacency = {i: [] for i in valid_indices}
        borderline_edges = set()
        llm_verified_count = 0
        llm_rejected_count = 0
        n_valid = len(valid_indices)
        total_pairs = n_valid * (n_valid - 1) // 2

        # tqdm progress bar
        pbar = tqdm(total=total_pairs, desc=f"      [{q_idx}/{total_queries}] ì£¼ì œ ê·¸ë£¹ íƒìƒ‰", unit="ìŒ", leave=False)

        for i in range(n_valid):
            for j in range(i + 1, n_valid):
                idx_a, idx_b = valid_indices[i], valid_indices[j]
                # news_category ì¼ì¹˜ í•„ìˆ˜ (None ì²´í¬ ë¶ˆí•„ìš” - valid_indicesì—ì„œ ì´ë¯¸ í•„í„°ë§ë¨)
                if cat_cache[idx_a] != cat_cache[idx_b]:
                    pbar.update(1)
                    continue

                sim = _jaccard_similarity(token_cache[idx_a], token_cache[idx_b])

                # í™•ì‹¤íˆ ê°™ì€ ì£¼ì œ (high threshold ì´ìƒ)
                if sim >= TOPIC_JACCARD_HIGH_THRESHOLD:
                    adjacency[idx_a].append(idx_b)
                    adjacency[idx_b].append(idx_a)
                # í™•ì‹¤íˆ ë‹¤ë¥¸ ì£¼ì œ (low threshold ì´í•˜)
                elif sim < TOPIC_JACCARD_LOW_THRESHOLD:
                    pass
                # ê²½ê³„ì„  ì¼€ì´ìŠ¤ (0.35 ~ 0.50): ê²½ê³„ì„  ê·¸ë˜í”„ì— ì €ì¥
                else:
                    a, b = (idx_a, idx_b) if idx_a < idx_b else (idx_b, idx_a)
                    borderline_edges.add((a, b))

                pbar.update(1)

        pbar.close()

        # ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ êµ¬ì„±
        if borderline_edges:
            borderline_adj = {idx: [] for idx in valid_indices}
            for a, b in borderline_edges:
                borderline_adj[a].append(b)
                borderline_adj[b].append(a)

            borderline_components = []
            border_visited = set()
            for start in valid_indices:
                if start in border_visited or len(borderline_adj[start]) == 0:
                    continue
                component = []
                queue = deque([start])
                while queue:
                    node = queue.popleft()
                    if node in border_visited:
                        continue
                    border_visited.add(node)
                    component.append(node)
                    for neighbor in borderline_adj[node]:
                        if neighbor not in border_visited:
                            queue.append(neighbor)
                if len(component) >= 2:
                    borderline_components.append(component)

            if borderline_components and openai_key:
                comp_pbar = tqdm(
                    total=len(borderline_components),
                    desc=f"      [{q_idx}/{total_queries}] ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ ê²€ì¦",
                    unit="ì»´í¬ë„ŒíŠ¸",
                    leave=False,
                )
                for component in borderline_components:
                    rep_idx = _select_representative_index(df, component)
                    if rep_idx is None:
                        comp_pbar.update(1)
                        continue

                    is_same = llm_judge_component_representative(
                        _build_article_payload(df, rep_idx),
                        openai_key=openai_key,
                        mode="topic_group_borderline",
                    )

                    if is_same:
                        llm_verified_count += 1
                        comp_size = len(component)
                        for a_pos in range(comp_size):
                            for b_pos in range(a_pos + 1, comp_size):
                                a_idx = component[a_pos]
                                b_idx = component[b_pos]
                                if b_idx not in adjacency[a_idx]:
                                    adjacency[a_idx].append(b_idx)
                                if a_idx not in adjacency[b_idx]:
                                    adjacency[b_idx].append(a_idx)
                    else:
                        llm_rejected_count += 1

                    comp_pbar.update(1)
                comp_pbar.close()
            elif borderline_components:
                # API í‚¤ê°€ ì—†ìœ¼ë©´ ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ë¯¸ì—°ê²°
                llm_rejected_count += len(borderline_components)

        # LLM ê²€ì¦ í†µê³„ ëˆ„ì  ë° ì¶œë ¥
        stats["sv_llm_verified"] += llm_verified_count
        stats["sv_llm_rejected"] += llm_rejected_count
        if llm_verified_count > 0 or llm_rejected_count > 0:
            print(
                f"    Query '{query}': LLM ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ ê²€ì¦ "
                f"{llm_verified_count}ê°œ ìŠ¹ì¸, {llm_rejected_count}ê°œ ê±°ë¶€"
            )

        # BFS connected components
        visited = set()
        components = []
        for start in valid_indices:
            if start in visited:
                continue
            component = []
            queue = deque([start])
            while queue:
                node = queue.popleft()
                if node in visited:
                    continue
                visited.add(node)
                component.append(node)
                for neighbor in adjacency[node]:
                    if neighbor not in visited:
                        queue.append(neighbor)
            if len(component) >= 2:
                components.append(component)

        # query ë¬¸ìì—´ ì •ë¦¬ (íŒŒì´í”„ í¬í•¨ ì‹œ ì²« ë²ˆì§¸)
        query_str = str(query) if query else "unknown"
        query_prefix = query_str.split("|")[0] if "|" in query_str else query_str

        # ê¸°ì¡´ topic cluster counter í™•ì¸
        existing_topic_ids = df["cluster_id"][
            df["cluster_id"].str.startswith(f"{query_prefix}_t", na=False)
        ]
        if len(existing_topic_ids) > 0:
            max_num = max(
                int(tid.split("_t")[-1])
                for tid in existing_topic_ids
                if tid.split("_t")[-1].isdigit()
            )
            counter = max_num + 1
        else:
            counter = 1

        # í´ëŸ¬ìŠ¤í„° í• ë‹¹
        for component in components:
            cid = f"{query_prefix}_t{counter:05d}"
            counter += 1

            for idx in component:
                df.at[idx, "cluster_id"] = cid
                df.at[idx, "source"] = "ìœ ì‚¬ì£¼ì œ"

            stats["sv_new_topic_groups"] += 1
            stats["sv_new_topic_articles"] += len(component)

    return df, stats


def _clean_html(s: str) -> str:
    """HTML íƒœê·¸ ì œê±° (press_release_detector ë°©ì‹ ì¬í™œìš©)."""
    if not isinstance(s, str):
        return ""
    s = re.sub(r"<[^>]+>", " ", s)
    s = s.replace("&quot;", " ").replace("&amp;", "&")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _tokenize_simple(s: str, min_token_len: int = 2) -> set:
    """ê°„ë‹¨ í† í°í™” â†’ set (Jaccardìš©)."""
    if not isinstance(s, str) or not s.strip():
        return set()
    s = s.lower()
    toks = re.findall(r"[ê°€-í£a-z0-9]+", s)
    return {t for t in toks if len(t) >= min_token_len}


def merge_cross_query_clusters(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©: ì„œë¡œ ë‹¤ë¥¸ queryë¡œ ìˆ˜ì§‘ëœ ë™ì¼ ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„°ë¥¼ ë³‘í•©.

    Part A(í´ëŸ¬ìŠ¤í„° ê²€ì¦) ì´í›„, Part B(ì£¼ì œ ê·¸ë£¹í™”) ì´ì „ì— ì‹¤í–‰.
    STEP 2(press_release_detector)ì—ì„œ queryë³„ë¡œ ë¶„ë¦¬ëœ í´ëŸ¬ìŠ¤í„° + ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ë¥¼
    TF-IDF cosine + Jaccard ìœ ì‚¬ë„ë¡œ cross-query ë³‘í•©.

    ì•Œê³ ë¦¬ì¦˜:
    1. í›„ë³´ ìˆ˜ì§‘: ê° clusterë³„ ëŒ€í‘œ ê¸°ì‚¬ + ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬
    2. TF-IDF char n-gram ë²¡í„°í™” (cross-query)
    3. Skip mask: ê°™ì€ cluster, ê°™ì€ query+ë¯¸í´ëŸ¬ìŠ¤í„° ìŒ ì œì™¸
    4. Auto-merge + LLM ê²½ê³„ì„  ê²€ì¦
    5. BFS connected components â†’ ë³‘í•© ì²˜ë¦¬

    Returns:
        (df, stats) íŠœí”Œ
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    df = df.copy()
    stats = {
        "sv_cross_merged_groups": 0,
        "sv_cross_merged_articles": 0,
    }

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required = {"source", "cluster_id", "title", "description"}
    if not required.issubset(df.columns):
        return df, stats

    # â”€â”€â”€ 1. í›„ë³´ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    candidates: List[Dict] = []  # {repr_idx, member_indices, cluster_id, query, source}

    # 1a. ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ê¸°ì‚¬ (ê°€ì¥ ë¹ ë¥¸ pub_datetime)
    clustered_mask = df["cluster_id"].astype(str).str.strip() != ""
    if clustered_mask.any():
        for cid, cgroup in df[clustered_mask].groupby("cluster_id"):
            if "pub_datetime" in df.columns:
                repr_idx = cgroup["pub_datetime"].astype(str).idxmin()
            else:
                repr_idx = cgroup.index[0]
            candidates.append({
                "repr_idx": repr_idx,
                "member_indices": cgroup.index.tolist(),
                "cluster_id": str(cid),
                "query": str(cgroup["query"].iloc[0]) if "query" in cgroup.columns else "",
                "source": str(cgroup["source"].iloc[0]),
            })

    # 1b. ë¯¸í´ëŸ¬ìŠ¤í„° ì¼ë°˜ê¸°ì‚¬ (ë¹„ê´€ë ¨ ì œì™¸)
    unclustered_mask = (~clustered_mask) & (df["source"] == "ì¼ë°˜ê¸°ì‚¬")
    if "news_category" in df.columns:
        unclustered_mask = unclustered_mask & (df["news_category"] != "ë¹„ê´€ë ¨")
    for idx in df[unclustered_mask].index:
        candidates.append({
            "repr_idx": idx,
            "member_indices": [idx],
            "cluster_id": "",
            "query": str(df.at[idx, "query"]) if "query" in df.columns else "",
            "source": "ì¼ë°˜ê¸°ì‚¬",
        })

    n = len(candidates)
    if n < 2:
        return df, stats

    # â”€â”€â”€ 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ + TF-IDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    repr_indices = [c["repr_idx"] for c in candidates]
    title_texts = [_clean_html(str(df.at[idx, "title"])) for idx in repr_indices]
    desc_texts = [_clean_html(str(df.at[idx, "description"])) for idx in repr_indices]
    repr_media_keys = [_extract_media_key(df, idx) for idx in repr_indices]
    if "pub_datetime" in df.columns:
        repr_pub_dt = [pd.to_datetime(df.at[idx, "pub_datetime"], errors="coerce", utc=True) for idx in repr_indices]
    else:
        repr_pub_dt = [pd.NaT for _ in repr_indices]

    # Jaccardìš© token set
    title_toksets = [_tokenize_simple(t) for t in title_texts]
    desc_toksets = [_tokenize_simple(d) for d in desc_texts]

    # TF-IDF ë²¡í„°í™”
    try:
        vec_title = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=1)
        vec_desc = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=1)

        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬: ê³µë°± í•˜ë‚˜ë¼ë„ ë„£ì–´ì•¼ TfidfVectorizerê°€ ì‘ë™
        title_corpus = [t if t.strip() else " " for t in title_texts]
        desc_corpus = [d if d.strip() else " " for d in desc_texts]

        X_title = vec_title.fit_transform(title_corpus)
        X_desc = vec_desc.fit_transform(desc_corpus)

        sim_title = cosine_similarity(X_title)
        sim_desc = cosine_similarity(X_desc)
    except Exception as e:
        print(f"  âš ï¸  Cross-query TF-IDF ì‹¤íŒ¨: {e}")
        return df, stats

    # â”€â”€â”€ 3-5. Skip mask + ìœ ì‚¬ë„ ê¸°ë°˜ adjacency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    adjacency = {i: [] for i in range(n)}
    borderline_pair_scores: Dict[Tuple[int, int], float] = {}
    total_pairs = n * (n - 1) // 2

    # tqdm progress bar
    print(f"    Cross-query í›„ë³´ {n}ê°œ, ë¹„êµ ìŒ {total_pairs}ê°œ ì²˜ë¦¬ ì¤‘...")
    pbar = tqdm(total=total_pairs, desc="    Cross-query ë³‘í•©", unit="ìŒ", leave=False)

    for i in range(n):
        for j in range(i + 1, n):
            ci, cj = candidates[i], candidates[j]

            # Skip: ê°™ì€ clusterì— ì†í•œ ìŒ
            if ci["cluster_id"] and ci["cluster_id"] == cj["cluster_id"]:
                pbar.update(1)
                continue

            # Skip: ê°™ì€ queryì´ë©´ì„œ ë‘˜ ë‹¤ ë¯¸í´ëŸ¬ìŠ¤í„° (STEP 2ì—ì„œ ì´ë¯¸ ì²˜ë¦¬)
            if ci["query"] == cj["query"] and not ci["cluster_id"] and not cj["cluster_id"]:
                pbar.update(1)
                continue

            t_cos = sim_title[i, j]
            d_cos = sim_desc[i, j]
            t_jac = _jaccard_similarity(title_toksets[i], title_toksets[j])
            d_jac = _jaccard_similarity(desc_toksets[i], desc_toksets[j])

            # Auto-merge: title ê¸°ì¤€
            if t_cos >= CROSS_TITLE_COS_THRESHOLD and t_jac >= CROSS_TITLE_JAC_THRESHOLD:
                adjacency[i].append(j)
                adjacency[j].append(i)
                pbar.update(1)
                continue

            # Auto-merge: description ê¸°ì¤€
            if d_cos >= CROSS_DESC_COS_THRESHOLD and d_jac >= CROSS_DESC_JAC_THRESHOLD:
                adjacency[i].append(j)
                adjacency[j].append(i)
                pbar.update(1)
                continue

            # ê²½ê³„ì„  í›„ë³´ ìˆ˜ì§‘ (pair-level LLM ì œê±°)
            title_borderline = (
                CROSS_TITLE_COS_BORDERLINE[0] <= t_cos < CROSS_TITLE_COS_BORDERLINE[1]
                and t_jac >= CROSS_TITLE_JAC_BORDERLINE_MIN
            )
            desc_borderline = (
                CROSS_DESC_COS_BORDERLINE[0] <= d_cos < CROSS_DESC_COS_BORDERLINE[1]
                and d_jac >= CROSS_DESC_JAC_BORDERLINE_MIN
            )

            if title_borderline or desc_borderline:
                # strong hint gate: same_media OR Î”days <= 1
                same_media = (
                    bool(repr_media_keys[i])
                    and bool(repr_media_keys[j])
                    and repr_media_keys[i] == repr_media_keys[j]
                )
                day_hint = False
                if pd.notna(repr_pub_dt[i]) and pd.notna(repr_pub_dt[j]):
                    day_diff = abs((repr_pub_dt[i] - repr_pub_dt[j]).total_seconds()) / 86400.0
                    day_hint = day_diff <= CROSS_BORDERLINE_DAY_HINT_MAX

                if same_media or day_hint:
                    title_score = (0.7 * float(t_cos) + 0.3 * float(t_jac)) if title_borderline else -1.0
                    desc_score = (0.7 * float(d_cos) + 0.3 * float(d_jac)) if desc_borderline else -1.0
                    score = max(title_score, desc_score)
                    key = (i, j)
                    prev = borderline_pair_scores.get(key)
                    if prev is None or score > prev:
                        borderline_pair_scores[key] = score

            pbar.update(1)

    pbar.close()

    # ê²½ê³„ì„  ê·¸ë˜í”„ ì»´í¬ë„ŒíŠ¸ êµ¬ì„± í›„, ì»´í¬ë„ŒíŠ¸ë‹¹ ëŒ€í‘œê¸°ì‚¬ 1íšŒ LLM ê²€ì¦
    llm_calls = 0
    llm_component_accepted = 0
    if borderline_pair_scores:
        borderline_adj = {i: [] for i in range(n)}
        for i, j in borderline_pair_scores.keys():
            borderline_adj[i].append(j)
            borderline_adj[j].append(i)

        borderline_components = []
        border_visited = set()
        for start in range(n):
            if start in border_visited or len(borderline_adj[start]) == 0:
                continue
            component = []
            queue = deque([start])
            while queue:
                node = queue.popleft()
                if node in border_visited:
                    continue
                border_visited.add(node)
                component.append(node)
                for neighbor in borderline_adj[node]:
                    if neighbor not in border_visited:
                        queue.append(neighbor)
            if len(component) >= 2:
                borderline_components.append(component)

        if borderline_components and openai_key:
            comp_pbar = tqdm(
                total=len(borderline_components),
                desc="    Cross-query ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸",
                unit="ì»´í¬ë„ŒíŠ¸",
                leave=False,
            )
            for component in borderline_components:
                rep_row_indices = [candidates[ci]["repr_idx"] for ci in component]
                rep_idx = _select_representative_index(df, rep_row_indices)
                if rep_idx is None:
                    comp_pbar.update(1)
                    continue

                llm_calls += 1
                should_merge = llm_judge_component_representative(
                    _build_article_payload(df, rep_idx),
                    openai_key=openai_key,
                    mode="cross_query_borderline",
                )

                if should_merge:
                    llm_component_accepted += 1
                    comp_size = len(component)
                    for a in range(comp_size):
                        for b in range(a + 1, comp_size):
                            i, j = component[a], component[b]
                            if j not in adjacency[i]:
                                adjacency[i].append(j)
                            if i not in adjacency[j]:
                                adjacency[j].append(i)

                comp_pbar.update(1)
            comp_pbar.close()
        elif borderline_components:
            print("    âš ï¸  OPENAI_API_KEY ì—†ìŒ: Cross-query ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ LLM ê²€ì¦ ìŠ¤í‚µ")

    # â”€â”€â”€ 6. BFS connected components â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    visited = set()
    components = []
    for start in range(n):
        if start in visited:
            continue
        component = []
        queue = deque([start])
        while queue:
            node = queue.popleft()
            if node in visited:
                continue
            visited.add(node)
            component.append(node)
            for neighbor in adjacency[node]:
                if neighbor not in visited:
                    queue.append(neighbor)
        if len(component) >= 2:
            components.append(component)

    # â”€â”€â”€ 7. ë³‘í•© ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cross_counter = 1
    for component in components:
        comp_candidates = [candidates[ci] for ci in component]

        # target cluster_id: ê¸°ì¡´ cluster_id ì¤‘ ê°€ì¥ ì‘ì€ ê²ƒ
        existing_cids = sorted([c["cluster_id"] for c in comp_candidates if c["cluster_id"]])
        if existing_cids:
            target_cid = existing_cids[0]
        else:
            target_cid = f"cross_m{cross_counter:05d}"
            cross_counter += 1

        # target source: component ë‚´ "ë³´ë„ìë£Œ" ìˆìœ¼ë©´ "ë³´ë„ìë£Œ", ì•„ë‹ˆë©´ "ìœ ì‚¬ì£¼ì œ"
        sources_in_comp = {c["source"] for c in comp_candidates}
        target_source = "ë³´ë„ìë£Œ" if "ë³´ë„ìë£Œ" in sources_in_comp else "ìœ ì‚¬ì£¼ì œ"

        # target cluster_summary: ê¸°ì¡´ ê°’ ì¤‘ ì²« ë²ˆì§¸
        target_cs = ""
        if "cluster_summary" in df.columns:
            for c in comp_candidates:
                for midx in c["member_indices"]:
                    val = str(df.at[midx, "cluster_summary"]).strip()
                    if val and val != "nan":
                        target_cs = val
                        break
                if target_cs:
                    break

        # ëª¨ë“  member_indicesì— ëŒ€í•´ ì—…ë°ì´íŠ¸
        total_members = 0
        for c in comp_candidates:
            for midx in c["member_indices"]:
                df.at[midx, "cluster_id"] = target_cid
                df.at[midx, "source"] = target_source
                if target_cs and "cluster_summary" in df.columns:
                    df.at[midx, "cluster_summary"] = target_cs
                total_members += 1

        stats["sv_cross_merged_groups"] += 1
        stats["sv_cross_merged_articles"] += total_members

    if llm_calls > 0:
        print(
            f"    Cross-query ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸ LLM ê²€ì¦: "
            f"{llm_calls}íšŒ í˜¸ì¶œ, {llm_component_accepted}ê°œ ì»´í¬ë„ŒíŠ¸ ìŠ¹ì¸"
        )

    return df, stats


def verify_and_regroup_sources(
    df: pd.DataFrame,
    openai_key: str = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    Entry point: Part A (ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦) + Part B (ì£¼ì œ ê·¸ë£¹ ë°œê²¬) ì‹¤í–‰.

    Args:
        df: ë¶„ë¥˜ ì™„ë£Œëœ DataFrame
        openai_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ fallback)

    Returns:
        (df, combined_stats) íŠœí”Œ
    """
    if openai_key is None:
        openai_key = os.getenv("OPENAI_API_KEY")

    print("\nğŸ“‹ Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì‹œì‘")

    # Part A: ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„° LLM ê²€ì¦
    df, verify_stats = verify_press_release_clusters(df, openai_key=openai_key)
    print(f"  Part A: ë³´ë„ìë£Œ ê²€ì¦ - {verify_stats['sv_clusters_verified']}ê°œ í´ëŸ¬ìŠ¤í„° ê²€ì¦")
    if verify_stats["sv_clusters_verified"] > 0:
        print(f"    - ë³´ë„ìë£Œ ìœ ì§€: {verify_stats['sv_kept_press_release']}ê°œ")
        print(f"    - ìœ ì‚¬ì£¼ì œ ì¬ë¶„ë¥˜: {verify_stats['sv_reclassified_similar_topic']}ê°œ")

    # Part A-2: Cross-query í´ëŸ¬ìŠ¤í„° ë³‘í•©
    df, cross_stats = merge_cross_query_clusters(df, openai_key=openai_key)
    print(f"  Part A-2: Cross-query ë³‘í•© - {cross_stats['sv_cross_merged_groups']}ê°œ ê·¸ë£¹, "
          f"{cross_stats['sv_cross_merged_articles']}ê°œ ê¸°ì‚¬")

    # Part B: ë¹„í´ëŸ¬ìŠ¤í„° ê¸°ì‚¬ ì£¼ì œ ê·¸ë£¹í™”
    df, topic_stats = discover_topic_groups(df, openai_key=openai_key)
    print(f"  Part B: ì£¼ì œ ê·¸ë£¹ ë°œê²¬ - {topic_stats['sv_new_topic_groups']}ê°œ ê·¸ë£¹, "
          f"{topic_stats['sv_new_topic_articles']}ê°œ ê¸°ì‚¬")
    if topic_stats.get("sv_llm_verified", 0) > 0 or topic_stats.get("sv_llm_rejected", 0) > 0:
        print(f"    - LLM ê²½ê³„ì„  ê²€ì¦: {topic_stats['sv_llm_verified']}ê°œ ì—°ê²°, "
              f"{topic_stats['sv_llm_rejected']}ê°œ ê±°ë¶€")

    # í†µí•© í†µê³„
    combined = {**verify_stats, **cross_stats, **topic_stats}

    # ì¼ê´€ì„± ê²€ì¦
    if "cluster_id" in df.columns and "cluster_summary" in df.columns:
        # Rule 1: source=="ì¼ë°˜ê¸°ì‚¬" â†’ cluster_id="", cluster_summary=""
        general_mask = df["source"] == "ì¼ë°˜ê¸°ì‚¬"
        r1_count = (general_mask & (df["cluster_id"].astype(str).str.strip() != "")).sum()
        if r1_count > 0:
            df.loc[general_mask, "cluster_id"] = ""
            df.loc[general_mask, "cluster_summary"] = ""
            print(f"  ì¼ê´€ì„± Rule 1: ì¼ë°˜ê¸°ì‚¬ {r1_count}ê±´ì˜ cluster_id/cluster_summary ì´ˆê¸°í™”")

        # Rule 2: ê°™ì€ cluster_id â†’ ê°™ì€ cluster_summary (ì²« ë¹„ì–´ìˆì§€ ì•Šì€ ê°’ ì „íŒŒ)
        clustered = df["cluster_id"].astype(str).str.strip() != ""
        r2_count = 0
        if clustered.any():
            for cid, cgroup in df[clustered].groupby("cluster_id"):
                summaries = cgroup["cluster_summary"].dropna().astype(str)
                summaries = summaries[summaries.str.strip() != ""]
                if len(summaries) > 0:
                    first_val = summaries.iloc[0]
                    mismatch = clustered & (df["cluster_id"] == cid) & (df["cluster_summary"] != first_val)
                    fix_count = mismatch.sum()
                    if fix_count > 0:
                        df.loc[mismatch, "cluster_summary"] = first_val
                        r2_count += fix_count
        if r2_count > 0:
            print(f"  ì¼ê´€ì„± Rule 2: {r2_count}ê±´ì˜ cluster_summary ì „íŒŒ")

        # Rule 3: cluster_id ìˆëŠ”ë° cluster_summary ì—†ìœ¼ë©´ ê²½ê³  (summarize_clusters í˜¸ì¶œë¡œ í•´ê²°ë¨)
        r3_mask = clustered & (df["cluster_summary"].astype(str).str.strip() == "")
        r3_count = r3_mask.sum()
        if r3_count > 0:
            print(f"  ì¼ê´€ì„± Rule 3: {r3_count}ê±´ cluster_summary ëˆ„ë½ (summarize_clusters í˜¸ì¶œ ì˜ˆì •)")

    # Source ë¶„í¬ ì¶œë ¥
    if "source" in df.columns:
        source_dist = df["source"].value_counts().to_dict()
        print(f"  Source ë¶„í¬: {source_dist}")

    print("âœ… Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™” ì™„ë£Œ")
    return df, combined
