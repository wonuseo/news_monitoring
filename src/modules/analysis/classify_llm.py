"""
classify_llm.py - LLM-Only Classification Orchestrator
ë³‘ë ¬ ì²˜ë¦¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""

import json
from datetime import datetime
from typing import Dict, Optional
import pandas as pd

from .llm_engine import load_prompts, analyze_article_llm
from .llm_orchestrator import run_chunked_parallel
from .result_writer import save_result_to_csv_incremental, sync_result_to_sheets
from src.utils.text_cleaning import clean_bom


def _process_single_article(
    idx: int,
    article: Dict,
    prompts_config: dict,
    openai_key: str
) -> Dict:
    """
    ë‹¨ì¼ ê¸°ì‚¬ LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼ í•¨ìˆ˜)

    Args:
        idx: DataFrame ì¸ë±ìŠ¤
        article: {"title": ..., "description": ..., "query": ...}
        prompts_config: prompts.yaml ì„¤ì •
        openai_key: OpenAI API í‚¤

    Returns:
        {"idx": idx, "success": True/False, "result": {...}, "error": ..., "error_type": ...}
    """
    try:
        llm_result = analyze_article_llm(article, prompts_config, openai_key)
        return {
            "idx": idx,
            "success": True,
            "result": llm_result,
            "error": None,
            "error_type": None
        }
    except Exception as e:
        import traceback
        error_type = type(e).__name__
        error_msg = str(e)
        error_trace = traceback.format_exc()
        return {
            "idx": idx,
            "success": False,
            "result": None,
            "error": error_msg,
            "error_type": error_type,
            "error_trace": error_trace
        }


def classify_llm(
    df: pd.DataFrame,
    openai_key: str,
    chunk_size: int = 50,
    dry_run: bool = False,
    max_competitor_classify: int = 50,
    max_workers: int = 3,
    result_csv_path: Optional[str] = None,
    spreadsheet = None,
    raw_df: pd.DataFrame = None
) -> tuple[pd.DataFrame, Dict]:
    """
    LLM-only ë¶„ë¥˜ ë©”ì¸ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)

    í”„ë¡œì„¸ìŠ¤:
    1. ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ (ìš°ë¦¬ ë¸Œëœë“œ ì „ì²´ + ê²½ìŸì‚¬ ìƒìœ„ Nê°œ ë˜ëŠ” ì „ì²´)
    2. LLM ë¶„ì„ (ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬)
    3. ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ì¦‰ì‹œ result.csvì— append
    4. ê° ì²­í¬ ì™„ë£Œ ì‹œ Google Sheets ë™ê¸°í™”

    Args:
        df: ì…ë ¥ DataFrame (title, description, query í•„ìˆ˜)
        openai_key: OpenAI API í‚¤
        chunk_size: ë°°ì¹˜ í¬ê¸° (ì²­í¬ë‹¹ ê¸°ì‚¬ ìˆ˜)
        dry_run: Trueë©´ LLM í˜¸ì¶œ ìƒëµ
        max_competitor_classify: ê²½ìŸì‚¬ë³„ ìµœëŒ€ ë¶„ë¥˜ ê°œìˆ˜ (0ì´ë©´ ë¬´ì œí•œ)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        result_csv_path: ê²°ê³¼ ì €ì¥ CSV ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        spreadsheet: Google Sheets ê°ì²´ (ì²­í¬ë³„ ë™ê¸°í™”ìš©, ì„ íƒì‚¬í•­)
        raw_df: ì›ë³¸ DataFrame (Sheets ë™ê¸°í™”ìš©, ì„ íƒì‚¬í•­)

    Returns:
        (ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrame, ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬)
    """
    df = df.copy()

    # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    metrics = {
        "articles_classified_llm": 0,
        "llm_api_calls": 0,
        "classification_errors": 0,
        "press_releases_skipped": 0,
    }

    # ì´ˆê¸°í™”: ëª¨ë“  ê²°ê³¼ ì»¬ëŸ¼
    result_columns = [
        "brand_relevance",
        "brand_relevance_query_keywords",
        "sentiment_stage",
        "danger_level",
        "issue_category",
        "news_category",
        "news_keyword_summary"
    ]

    # ì»¬ëŸ¼ ì´ˆê¸°í™” (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°’ì€ ë³´ì¡´)
    for col in result_columns:
        if col not in df.columns:
            df[col] = None

    if "classified_at" not in df.columns:
        df["classified_at"] = ""

    # Load prompts config
    print("\nğŸ”§ LLM ë¶„ë¥˜ ì‹œì‘...")
    prompts_config = load_prompts()

    if dry_run:
        print("ğŸ”¬ DRY RUN ëª¨ë“œ: LLM ë¶„ì„ ìƒëµ")
        return df, metrics

    # ========================================
    # STEP 1: ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ
    # ========================================
    print("\n[1/2] ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ ì¤‘...")

    def already_classified(row) -> bool:
        value = clean_bom(row.get("classified_at", ""))
        return isinstance(value, str) and len(value.strip()) > 1

    def has_article_id(row) -> bool:
        value = clean_bom(row.get("article_id", ""))
        return isinstance(value, str) and len(value.strip()) >= 6

    indices_to_classify = []
    for idx, row in df.iterrows():
        if already_classified(row):
            continue
        if not has_article_id(row):
            continue
        indices_to_classify.append(idx)

    # ìŠ¤í‚µëœ ê¸°ì‚¬ í†µê³„ (ë³´ë„ìë£Œ ì „ì²˜ë¦¬ ë“±)
    skipped_count = sum(1 for _, row in df.iterrows() if already_classified(row))

    metrics["press_releases_skipped"] = skipped_count

    if len(indices_to_classify) == 0:
        if skipped_count > 0:
            print(f"â„¹ï¸  ì „ì²´ {len(df)}ê°œ ê¸°ì‚¬ ì¤‘ {skipped_count}ê°œëŠ” ì´ë¯¸ ë¶„ë¥˜ë¨ (ë³´ë„ìë£Œ ë“±)")
            print("âš ï¸  LLM ë¶„ë¥˜ ëŒ€ìƒ ì—†ìŒ")
        else:
            print("âš ï¸  ë¶„ë¥˜ ëŒ€ìƒ ì—†ìŒ")
        return df, metrics

    print(f"  ì„ íƒëœ ê¸°ì‚¬: {len(indices_to_classify)}ê°œ")
    if skipped_count > 0:
        print(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ (ì´ë¯¸ ë¶„ë¥˜ë¨)")

    # ========================================
    # STEP 2: LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
    # ========================================
    print(f"\n[2/2] LLM ë¶„ì„ ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì›Œì»¤: {max_workers})...")

    timestamp = datetime.now().isoformat()
    # ì‹¤í–‰ íƒœìŠ¤í¬ êµ¬ì„±
    tasks = []
    for idx in indices_to_classify:
        article = {
            "query": df.at[idx, "query"],
            "group": df.at[idx, "group"] if "group" in df.columns else "",
            "title": df.at[idx, "title"],
            "description": df.at[idx, "description"]
        }
        tasks.append({"task_id": idx, "idx": idx, "article": article})

    def worker(task: Dict) -> Dict:
        return _process_single_article(
            task["idx"],
            task["article"],
            prompts_config,
            openai_key
        )

    def on_success(result: Dict):
        idx = result["idx"]
        llm_result = result["result"] or {}

        # LLM ê²°ê³¼ë¥¼ DataFrameì— ë³‘í•©
        for col in result_columns:
            if col in llm_result:
                value = llm_result[col]
                # JSON serializable íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                if isinstance(value, (dict, list)):
                    value = json.dumps(value, ensure_ascii=False)
                # ëª¨ë“  BOM ë° invisible ë¬¸ì ì œê±°
                if isinstance(value, str):
                    value = clean_bom(value)
                df.at[idx, col] = value

        df.at[idx, "classified_at"] = timestamp

        # ì„±ê³µí•œ ê¸°ì‚¬ ì¦‰ì‹œ CSVì— ì €ì¥
        save_result_to_csv_incremental(df, idx, result_csv_path)

    def on_failure(result: Dict, _chunk_fail_count: int, total_fail_count: int):
        idx = result.get("idx")
        if idx is None:
            idx = result.get("task_id")
        if idx is None:
            return

        # Fallback: ë¹ˆ ê°’ ì‚¬ìš©
        for col in result_columns:
            df.at[idx, col] = ""
        df.at[idx, "classified_at"] = timestamp

        # ì‹¤íŒ¨ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        title = df.at[idx, 'title'] if pd.notna(df.at[idx, 'title']) else ""
        print(f"\nâŒ ë¶„ë¥˜ ì‹¤íŒ¨ [idx={idx}]:")
        print(f"   ì œëª©: {title[:80]}...")
        print(f"   ì—ëŸ¬ íƒ€ì…: {result.get('error_type', 'Unknown')}")
        print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {result.get('error', 'Unknown error')}")
        # Tracebackì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì²« ì‹¤íŒ¨ë§Œ ì¶œë ¥
        if total_fail_count == 1 and 'error_trace' in result:
            print(f"   ìƒì„¸ Traceback (ì²« ì‹¤íŒ¨ë§Œ):\n{result['error_trace']}")

    def sync_callback(_chunk_num: int, _total_chunks: int, _succ: int, _fail: int):
        if spreadsheet and result_csv_path and raw_df is not None:
            sync_result_to_sheets(result_csv_path, raw_df, spreadsheet, verbose=True)

    run_stats = run_chunked_parallel(
        tasks=tasks,
        worker_fn=worker,
        on_success=on_success,
        on_failure=on_failure,
        chunk_size=chunk_size,
        max_workers=max_workers,
        progress_desc="LLM ë¶„ì„",
        unit="ê¸°ì‚¬",
        inter_chunk_sleep=0.5,
        sync_callback=sync_callback,
    )

    # ì „ì²´ í†µê³„ ì¶œë ¥
    total_processed = run_stats["processed"]
    total_success = run_stats["success"]
    total_failed = run_stats["failed"]
    overall_success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0

    print(f"\nâœ… LLM ë¶„ë¥˜ ì™„ë£Œ:")
    print(f"   ì´ ì²˜ë¦¬: {total_processed}ê°œ ê¸°ì‚¬")
    print(f"   ì„±ê³µ: {total_success}ê°œ ({overall_success_rate:.1f}%)")
    print(f"   ì‹¤íŒ¨: {total_failed}ê°œ ({100-overall_success_rate:.1f}%)")

    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    metrics["articles_classified_llm"] = total_success
    metrics["llm_api_calls"] = total_success  # 1 article = 1 API call
    metrics["classification_errors"] = total_failed

    # ë¹„ìš© ì¶”ì • (gpt-5-nano: $0.00015/1K input tokens, $0.0006/1K output tokens)
    # í‰ê· : ~1500 input tokens/article, ~300 output tokens/article
    avg_input_tokens_per_article = 1500
    avg_output_tokens_per_article = 300
    input_cost = (total_success * avg_input_tokens_per_article / 1000) * 0.00015
    output_cost = (total_success * avg_output_tokens_per_article / 1000) * 0.0006
    metrics["llm_cost_estimated"] = round(input_cost + output_cost, 4)

    return df, metrics
