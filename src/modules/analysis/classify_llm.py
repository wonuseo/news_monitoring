"""
classify_llm.py - LLM-Only Classification Orchestrator
ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›, ì‹¤ì‹œê°„ CSV ì €ì¥
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import pandas as pd
from tqdm import tqdm

from .llm_engine import load_prompts, analyze_article_llm
from src.modules.export.sheets import clean_bom


# CSV ì“°ê¸° Lock (ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ íŒŒì¼ ì“°ê¸° ê²½í•© ë°©ì§€)
csv_write_lock = Lock()


def _process_single_article(
    idx: int,
    article: Dict,
    prompts_config: dict,
    openai_key: str
) -> Dict:
    """
    ë‹¨ì¼ ê¸°ì‚¬ LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼ í•¨ìˆ˜)

    Args:
        idx: DataFrame ì¸ë±ìŠ¤
        article: {"title": ..., "description": ..., "query": ...}
        prompts_config: prompts.yaml ì„¤ì •
        openai_key: OpenAI API í‚¤

    Returns:
        {"idx": idx, "success": True/False, "result": {...}, "error": ..., "error_type": ...}
    """
    try:
        llm_result = analyze_article_llm(article, prompts_config, openai_key)
        return {
            "idx": idx,
            "success": True,
            "result": llm_result,
            "error": None,
            "error_type": None
        }
    except Exception as e:
        import traceback
        error_type = type(e).__name__
        error_msg = str(e)
        error_trace = traceback.format_exc()
        return {
            "idx": idx,
            "success": False,
            "result": None,
            "error": error_msg,
            "error_type": error_type,
            "error_trace": error_trace
        }


def classify_llm(
    df: pd.DataFrame,
    openai_key: str,
    chunk_size: int = 50,
    dry_run: bool = False,
    max_competitor_classify: int = 50,
    max_workers: int = 10,
    result_csv_path: Optional[str] = None,
    spreadsheet = None,
    raw_df: pd.DataFrame = None
) -> tuple[pd.DataFrame, Dict]:
    """
    LLM-only ë¶„ë¥˜ ë©”ì¸ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)

    í”„ë¡œì„¸ìŠ¤:
    1. ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ (ìš°ë¦¬ ë¸Œëœë“œ ì „ì²´ + ê²½ìŸì‚¬ ìƒìœ„ Nê°œ ë˜ëŠ” ì „ì²´)
    2. LLM ë¶„ì„ (ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬)
    3. ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ì¦‰ì‹œ result.csvì— append
    4. ê° ì²­í¬ ì™„ë£Œ ì‹œ Google Sheets ë™ê¸°í™”

    Args:
        df: ì…ë ¥ DataFrame (title, description, query í•„ìˆ˜)
        openai_key: OpenAI API í‚¤
        chunk_size: ë°°ì¹˜ í¬ê¸° (ì²­í¬ë‹¹ ê¸°ì‚¬ ìˆ˜)
        dry_run: Trueë©´ LLM í˜¸ì¶œ ìƒëµ
        max_competitor_classify: ê²½ìŸì‚¬ë³„ ìµœëŒ€ ë¶„ë¥˜ ê°œìˆ˜ (0ì´ë©´ ë¬´ì œí•œ)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        result_csv_path: ê²°ê³¼ ì €ì¥ CSV ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        spreadsheet: Google Sheets ê°ì²´ (ì²­í¬ë³„ ë™ê¸°í™”ìš©, ì„ íƒì‚¬í•­)
        raw_df: ì›ë³¸ DataFrame (Sheets ë™ê¸°í™”ìš©, ì„ íƒì‚¬í•­)

    Returns:
        (ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrame, ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬)
    """
    df = df.copy()

    # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    metrics = {
        "articles_classified_llm": 0,
        "llm_api_calls": 0,
        "classification_errors": 0,
        "press_releases_skipped": 0,
    }

    # ì´ˆê¸°í™”: ëª¨ë“  ê²°ê³¼ ì»¬ëŸ¼
    result_columns = [
        "brand_relevance",
        "brand_relevance_query_keywords",
        "sentiment_stage",
        "danger_level",
        "issue_category",
        "news_category",
        "news_keyword_summary"
    ]

    # ì»¬ëŸ¼ ì´ˆê¸°í™” (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°’ì€ ë³´ì¡´)
    for col in result_columns:
        if col not in df.columns:
            df[col] = None

    if "classified_at" not in df.columns:
        df["classified_at"] = ""

    # Load prompts config
    print("\nğŸ”§ LLM ë¶„ë¥˜ ì‹œì‘...")
    prompts_config = load_prompts()

    if dry_run:
        print("ğŸ”¬ DRY RUN ëª¨ë“œ: LLM ë¶„ì„ ìƒëµ")
        return df, metrics

    # ========================================
    # STEP 1: ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ
    # ========================================
    print("\n[1/2] ë¶„ë¥˜ ëŒ€ìƒ ì„ íƒ ì¤‘...")

    def already_classified(row) -> bool:
        value = clean_bom(row.get("classified_at", ""))
        return isinstance(value, str) and len(value.strip()) > 1

    def has_article_id(row) -> bool:
        value = clean_bom(row.get("article_id", ""))
        return isinstance(value, str) and len(value.strip()) >= 6

    indices_to_classify = []
    for idx, row in df.iterrows():
        if already_classified(row):
            continue
        if not has_article_id(row):
            continue
        indices_to_classify.append(idx)

    # ìŠ¤í‚µëœ ê¸°ì‚¬ í†µê³„ (ë³´ë„ìë£Œ ì „ì²˜ë¦¬ ë“±)
    skipped_count = sum(1 for _, row in df.iterrows() if already_classified(row))

    metrics["press_releases_skipped"] = skipped_count

    if len(indices_to_classify) == 0:
        if skipped_count > 0:
            print(f"â„¹ï¸  ì „ì²´ {len(df)}ê°œ ê¸°ì‚¬ ì¤‘ {skipped_count}ê°œëŠ” ì´ë¯¸ ë¶„ë¥˜ë¨ (ë³´ë„ìë£Œ ë“±)")
            print("âš ï¸  LLM ë¶„ë¥˜ ëŒ€ìƒ ì—†ìŒ")
        else:
            print("âš ï¸  ë¶„ë¥˜ ëŒ€ìƒ ì—†ìŒ")
        return df, metrics

    print(f"  ì„ íƒëœ ê¸°ì‚¬: {len(indices_to_classify)}ê°œ")
    if skipped_count > 0:
        print(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ (ì´ë¯¸ ë¶„ë¥˜ë¨)")

    # ========================================
    # STEP 2: LLM ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
    # ========================================
    print(f"\n[2/2] LLM ë¶„ì„ ì¤‘ (ì²­í¬ í¬ê¸°: {chunk_size}, ì›Œì»¤: {max_workers})...")

    timestamp = datetime.now().isoformat()
    total = len(indices_to_classify)
    total_chunks = (total + chunk_size - 1) // chunk_size

    # ì „ì²´ ì§„í–‰ë¥  í‘œì‹œìš© tqdm
    pbar = tqdm(total=total, desc="LLM ë¶„ì„", unit="ê¸°ì‚¬")

    for chunk_start in range(0, total, chunk_size):
        chunk_end = min(chunk_start + chunk_size, total)
        chunk_indices = indices_to_classify[chunk_start:chunk_end]
        chunk_num = (chunk_start // chunk_size) + 1

        # ì²­í¬ ë‚´ ê¸°ì‚¬ ì •ë³´ ì¤€ë¹„
        articles_to_process = []
        for idx in chunk_indices:
            article = {
                "query": df.at[idx, "query"],
                "group": df.at[idx, "group"] if "group" in df.columns else "",
                "title": df.at[idx, "title"],
                "description": df.at[idx, "description"]
            }

            articles_to_process.append({
                "idx": idx,
                "article": article
            })

        # ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor)
        success_count = 0
        fail_count = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_idx = {
                executor.submit(
                    _process_single_article,
                    item["idx"],
                    item["article"],
                    prompts_config,
                    openai_key
                ): item["idx"]
                for item in articles_to_process
            }

            # Process completed tasks
            for future in as_completed(future_to_idx):
                result = future.result()
                idx = result["idx"]

                if result["success"]:
                    # LLM ê²°ê³¼ë¥¼ DataFrameì— ë³‘í•©
                    llm_result = result["result"]
                    for col in result_columns:
                        if col in llm_result:
                            value = llm_result[col]
                            # JSON serializable íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                            if isinstance(value, (dict, list)):
                                value = json.dumps(value, ensure_ascii=False)
                            # ëª¨ë“  BOM ë° invisible ë¬¸ì ì œê±°
                            if isinstance(value, str):
                                value = clean_bom(value)
                            df.at[idx, col] = value

                    df.at[idx, "classified_at"] = timestamp
                    success_count += 1

                    # ì„±ê³µí•œ ê¸°ì‚¬ ì¦‰ì‹œ CSVì— ì €ì¥ (Lock ì‚¬ìš©)
                    if result_csv_path:
                        try:
                            with csv_write_lock:
                                row_df = df.loc[[idx]].copy()
                                # íŒŒì¼ì´ ì—†ìœ¼ë©´ header í¬í•¨, ìˆìœ¼ë©´ appendë§Œ
                                file_exists = os.path.exists(result_csv_path)
                                row_df.to_csv(
                                    result_csv_path,
                                    mode='a' if file_exists else 'w',
                                    header=not file_exists,
                                    index=False,
                                    encoding='utf-8-sig' if not file_exists else 'utf-8'
                                )
                        except Exception as csv_err:
                            print(f"âš ï¸  CSV ì €ì¥ ì‹¤íŒ¨ [idx={idx}]: {csv_err}")
                else:
                    # Fallback: ë¹ˆ ê°’ ì‚¬ìš©
                    for col in result_columns:
                        df.at[idx, col] = ""
                    df.at[idx, "classified_at"] = timestamp
                    fail_count += 1

                    # ì‹¤íŒ¨ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
                    title = df.at[idx, 'title'] if pd.notna(df.at[idx, 'title']) else ""
                    print(f"\nâŒ ë¶„ë¥˜ ì‹¤íŒ¨ [idx={idx}]:")
                    print(f"   ì œëª©: {title[:80]}...")
                    print(f"   ì—ëŸ¬ íƒ€ì…: {result.get('error_type', 'Unknown')}")
                    print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {result['error']}")
                    # Tracebackì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì²« ì‹¤íŒ¨ë§Œ ì¶œë ¥
                    if fail_count == 1 and 'error_trace' in result:
                        print(f"   ìƒì„¸ Traceback (ì²« ì‹¤íŒ¨ë§Œ):\n{result['error_trace']}")

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                pbar.update(1)

        pbar.set_postfix({"ì²­í¬": f"{chunk_num}/{total_chunks}", "ì„±ê³µ": success_count, "ì‹¤íŒ¨": fail_count})

        # ì²­í¬ ì™„ë£Œ í†µê³„ ì¶œë ¥
        chunk_total = success_count + fail_count
        success_rate = (success_count / chunk_total * 100) if chunk_total > 0 else 0
        print(f"\n  ì²­í¬ {chunk_num}/{total_chunks} ì™„ë£Œ: ì„±ê³µ {success_count}/{chunk_total} ({success_rate:.1f}%)")

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ì²­í¬ ì™„ë£Œ ì‹œë§ˆë‹¤)
        if spreadsheet and result_csv_path and raw_df is not None:
            try:
                # sync_raw_and_processedë¥¼ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
                from src.modules.export.sheets import sync_raw_and_processed

                # result.csv ì „ì²´ë¥¼ ë‹¤ì‹œ ì½ì–´ì„œ ë™ê¸°í™” (ì¤‘ë³µ ì²´í¬ ìë™, upsert ì§€ì›)
                if os.path.exists(result_csv_path):
                    df_result_current = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                    sync_results = sync_raw_and_processed(raw_df, df_result_current, spreadsheet)
                    added_count = sum(r.get('added', 0) for r in sync_results.values())
                    updated_count = sum(r.get('updated', 0) for r in sync_results.values())
                    if added_count > 0 or updated_count > 0:
                        msg_parts = []
                        if added_count > 0:
                            msg_parts.append(f"{added_count}ê°œ ì¶”ê°€")
                        if updated_count > 0:
                            msg_parts.append(f"{updated_count}ê°œ ì—…ë°ì´íŠ¸")
                        print(f"    â˜ï¸  Sheets ë™ê¸°í™”: {', '.join(msg_parts)}")
            except Exception as e:
                print(f"    âš ï¸  Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}")

        # ì²­í¬ ê°„ ì§§ì€ ëŒ€ê¸° (rate limiting)
        if chunk_num < total_chunks:
            time.sleep(0.5)

    pbar.close()

    # ì „ì²´ í†µê³„ ì¶œë ¥
    total_processed = sum(1 for idx in indices_to_classify if pd.notna(df.at[idx, "classified_at"]))
    total_success = sum(1 for idx in indices_to_classify
                       if pd.notna(df.at[idx, "classified_at"])
                       and df.at[idx, "sentiment_stage"] != "")
    total_failed = total_processed - total_success
    overall_success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0

    print(f"\nâœ… LLM ë¶„ë¥˜ ì™„ë£Œ:")
    print(f"   ì´ ì²˜ë¦¬: {total_processed}ê°œ ê¸°ì‚¬")
    print(f"   ì„±ê³µ: {total_success}ê°œ ({overall_success_rate:.1f}%)")
    print(f"   ì‹¤íŒ¨: {total_failed}ê°œ ({100-overall_success_rate:.1f}%)")

    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    metrics["articles_classified_llm"] = total_success
    metrics["llm_api_calls"] = total_success  # 1 article = 1 API call
    metrics["classification_errors"] = total_failed

    # ë¹„ìš© ì¶”ì • (gpt-5-nano: $0.00015/1K input tokens, $0.0006/1K output tokens)
    # í‰ê· : ~1500 input tokens/article, ~300 output tokens/article
    avg_input_tokens_per_article = 1500
    avg_output_tokens_per_article = 300
    input_cost = (total_success * avg_input_tokens_per_article / 1000) * 0.00015
    output_cost = (total_success * avg_output_tokens_per_article / 1000) * 0.0006
    metrics["llm_cost_estimated"] = round(input_cost + output_cost, 4)

    return df, metrics


def get_classification_stats(df: pd.DataFrame) -> Dict:
    """
    ë¶„ë¥˜ í†µê³„ ìƒì„±

    Args:
        df: ë¶„ì„ ê²°ê³¼ DataFrame

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    stats = {}

    # Sentiment distribution
    if "sentiment_stage" in df.columns:
        sentiment_counts = df["sentiment_stage"].value_counts().to_dict()
        stats["sentiment_stage"] = sentiment_counts

    # Danger distribution
    if "danger_level" in df.columns:
        danger_counts = df["danger_level"].value_counts().to_dict()
        stats["danger_level"] = danger_counts

    # Brand relevance distribution
    if "brand_relevance" in df.columns:
        relevance_counts = df["brand_relevance"].value_counts().to_dict()
        stats["brand_relevance"] = relevance_counts

    # Issue category distribution
    if "issue_category" in df.columns:
        category_counts = df["issue_category"].value_counts().to_dict()
        stats["issue_category"] = category_counts

    # News category distribution
    if "news_category" in df.columns:
        news_cat_counts = df["news_category"].value_counts().to_dict()
        stats["news_category"] = news_cat_counts

    return stats


def print_classification_stats(stats: Dict):
    """
    ë¶„ë¥˜ í†µê³„ ì¶œë ¥

    Args:
        stats: get_classification_stats() ê²°ê³¼
    """
    print("\nğŸ“Š ë¶„ë¥˜ í†µê³„:")

    if "sentiment_stage" in stats:
        print("\n  ê°ì • ë‹¨ê³„:")
        for sentiment, count in stats["sentiment_stage"].items():
            print(f"    - {sentiment}: {count}ê°œ")

    if "danger_level" in stats:
        print("\n  ìœ„í—˜ë„:")
        for danger, count in stats["danger_level"].items():
            print(f"    - {danger}: {count}ê°œ")

    if "brand_relevance" in stats:
        print("\n  ë¸Œëœë“œ ê´€ë ¨ì„±:")
        for relevance, count in stats["brand_relevance"].items():
            print(f"    - {relevance}: {count}ê°œ")

    if "issue_category" in stats:
        print("\n  ì´ìŠˆ ì¹´í…Œê³ ë¦¬ (ìƒìœ„ 5ê°œ):")
        for category, count in sorted(stats["issue_category"].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    - {category}: {count}ê°œ")

    if "news_category" in stats:
        print("\n  ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (ìƒìœ„ 5ê°œ):")
        for category, count in sorted(stats["news_category"].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"    - {category}: {count}ê°œ")
