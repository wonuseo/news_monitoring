"""
Run Logger - Tracks pipeline execution metrics and saves to CSV + Google Sheets
"""

import time
import uuid
import json
import os
import csv
from datetime import datetime
from typing import Dict, Any, Optional
import pandas as pd


class RunLogger:
    """
    Tracks metrics across pipeline stages and saves to CSV + Google Sheets.

    Usage:
        logger = RunLogger()
        logger.log("articles_collected", 100)
        logger.log_stage("collection", {"api_calls": 9, "articles": 900})
        logger.save_csv("data/logs/run_history.csv")
    """

    def __init__(self):
        self.run_id = str(uuid.uuid4())[:8]  # Short UUID (e.g., "a3f2c1d4")
        self.start_time = time.time()
        self.metrics = {
            "run_id": self.run_id,
            "timestamp": datetime.now().isoformat(),
        }
        self.stage_start_times = {}

    def log(self, key: str, value: Any):
        """Log a single metric"""
        self.metrics[key] = value

    def log_dict(self, metrics_dict: Dict[str, Any]):
        """Log multiple metrics at once"""
        self.metrics.update(metrics_dict)

    def start_stage(self, stage_name: str):
        """Mark the start of a pipeline stage (for duration tracking)"""
        self.stage_start_times[stage_name] = time.time()

    def end_stage(self, stage_name: str):
        """Mark the end of a pipeline stage and calculate duration"""
        if stage_name in self.stage_start_times:
            duration = time.time() - self.stage_start_times[stage_name]
            self.metrics[f"duration_{stage_name}"] = round(duration, 2)

    def finalize(self):
        """Finalize metrics (calculate total duration)"""
        self.metrics["duration_total"] = round(time.time() - self.start_time, 2)

    def to_dict(self) -> Dict[str, Any]:
        """Return all metrics as dictionary"""
        return self.metrics.copy()

    def to_dataframe(self) -> pd.DataFrame:
        """Convert metrics to single-row DataFrame"""
        # Ensure all values are serializable
        clean_metrics = {}
        for key, value in self.metrics.items():
            if isinstance(value, (dict, list)):
                clean_metrics[key] = json.dumps(value, ensure_ascii=False)
            else:
                clean_metrics[key] = value

        return pd.DataFrame([clean_metrics])

    def save_csv(self, csv_path: str):
        """
        Save metrics to CSV file (append mode).
        Creates file if it doesn't exist.
        """
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)

            df = self.to_dataframe()

            # Append to existing file or create new one
            if os.path.exists(csv_path):
                df.to_csv(csv_path, mode='a', header=False, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)
            else:
                df.to_csv(csv_path, mode='w', header=True, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)

            print(f"\nğŸ“Š ë¡œê·¸ ì €ì¥: {csv_path}")
            return True

        except Exception as e:
            print(f"\nâš ï¸ ë¡œê·¸ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def print_summary(self):
        """Print a formatted summary of key metrics"""
        print("\n" + "="*60)
        print("ğŸ“Š ì‹¤í–‰ ìš”ì•½ (Run Summary)")
        print("="*60)

        # Basic info
        print(f"Run ID: {self.metrics.get('run_id', 'N/A')}")
        print(f"Timestamp: {self.metrics.get('timestamp', 'N/A')}")
        print(f"Total Duration: {self.metrics.get('duration_total', 0):.2f}ì´ˆ")
        print()

        # Collection
        print("ğŸ“¥ ìˆ˜ì§‘ (Collection)")
        print(f"  - ì „ì²´ ìˆ˜ì§‘: {self.metrics.get('articles_collected_total', 0)}ê°œ")
        if 'articles_collected_per_query' in self.metrics:
            per_query = self.metrics['articles_collected_per_query']
            if isinstance(per_query, str):
                per_query = json.loads(per_query)
            for brand, count in per_query.items():
                print(f"    â€¢ {brand}: {count}ê°œ")
        print(f"  - ê¸°ì¡´ ë§í¬ ìŠ¤í‚µ: {self.metrics.get('existing_links_skipped', 0)}ê°œ")
        print(f"  - ì†Œìš” ì‹œê°„: {self.metrics.get('duration_collection', 0):.2f}ì´ˆ")
        print()

        # Processing
        print("âš™ï¸ ì „ì²˜ë¦¬ (Processing)")
        print(f"  - ì¤‘ë³µ ì œê±°: {self.metrics.get('duplicates_removed', 0)}ê°œ")
        print(f"  - ì „ì²˜ë¦¬ ì™„ë£Œ: {self.metrics.get('articles_processed', 0)}ê°œ")
        print(f"  - ë³´ë„ìë£Œ íƒì§€: {self.metrics.get('press_releases_detected', 0)}ê°œ")
        print(f"  - ë³´ë„ìë£Œ ê·¸ë£¹: {self.metrics.get('press_release_groups', 0)}ê°œ")
        print(f"  - ì†Œìš” ì‹œê°„: {self.metrics.get('duration_processing', 0):.2f}ì´ˆ")
        print()

        # Media classification
        if self.metrics.get('media_domains_total', 0) > 0:
            print("ğŸ¢ ë¯¸ë””ì–´ ë¶„ë¥˜ (Media Classification)")
            print(f"  - ì „ì²´ ë„ë©”ì¸: {self.metrics.get('media_domains_total', 0)}ê°œ")
            print(f"  - ì‹ ê·œ ë¶„ë¥˜: {self.metrics.get('media_domains_new', 0)}ê°œ")
            print(f"  - ìºì‹œ ì‚¬ìš©: {self.metrics.get('media_domains_cached', 0)}ê°œ")
            print()

        # Classification
        print("ğŸ¤– LLM ë¶„ì„ (Classification)")
        print(f"  - ë¶„ì„ ì™„ë£Œ: {self.metrics.get('articles_classified_llm', 0)}ê°œ")
        print(f"  - API í˜¸ì¶œ: {self.metrics.get('llm_api_calls', 0)}íšŒ")
        print(f"  - ì¶”ì • ë¹„ìš©: ${self.metrics.get('llm_cost_estimated', 0):.4f}")
        print(f"  - ë³´ë„ìë£Œ ìŠ¤í‚µ: {self.metrics.get('press_releases_skipped', 0)}ê°œ")
        print(f"  - ë¶„ë¥˜ ì‹¤íŒ¨: {self.metrics.get('classification_errors', 0)}ê°œ")
        print(f"  - ì†Œìš” ì‹œê°„: {self.metrics.get('duration_classification', 0):.2f}ì´ˆ")
        print()

        # Results
        print("ğŸ“ˆ ë¶„ë¥˜ ê²°ê³¼ (Results)")
        print(f"  - ìš°ë¦¬ ë¸Œëœë“œ ê´€ë ¨: {self.metrics.get('our_brands_relevant', 0)}ê°œ")
        print(f"  - ìš°ë¦¬ ë¸Œëœë“œ ë¶€ì •: {self.metrics.get('our_brands_negative', 0)}ê°œ")
        print(f"  - ìœ„í—˜ë„ ìƒ: {self.metrics.get('danger_high', 0)}ê°œ")
        print(f"  - ìœ„í—˜ë„ ì¤‘: {self.metrics.get('danger_medium', 0)}ê°œ")
        print(f"  - ê²½ìŸì‚¬ ê¸°ì‚¬: {self.metrics.get('competitor_articles', 0)}ê°œ")
        print()

        # Sheets sync
        if self.metrics.get('sheets_sync_enabled'):
            print("â˜ï¸ Google Sheets ë™ê¸°í™”")
            print(f"  - raw_data ì—…ë¡œë“œ: {self.metrics.get('sheets_rows_uploaded_raw', 0)}í–‰")
            print(f"  - result ì—…ë¡œë“œ: {self.metrics.get('sheets_rows_uploaded_result', 0)}í–‰")
            print(f"  - logs ì—…ë¡œë“œ: {self.metrics.get('sheets_logs_uploaded', 0)}í–‰")
            print(f"  - ì†Œìš” ì‹œê°„: {self.metrics.get('duration_sheets_sync', 0):.2f}ì´ˆ")
            print()

        # Errors
        if self.metrics.get('errors_total', 0) > 0:
            print("âš ï¸ ì—ëŸ¬")
            print(f"  - ì „ì²´ ì—ëŸ¬: {self.metrics.get('errors_total', 0)}ê°œ")
            print(f"  - ê²½ê³ : {self.metrics.get('warnings_total', 0)}ê°œ")
            print()

        print("="*60)


def sync_logs_to_sheets(csv_path: str, spreadsheet, sheet_name: str = "logs"):
    """
    Sync run history CSV to Google Sheets.

    Args:
        csv_path: Path to run_history.csv
        spreadsheet: gspread spreadsheet object
        sheet_name: Sheet name (default: "logs")
    """
    try:
        if not os.path.exists(csv_path):
            print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì—†ìŒ: {csv_path}")
            return False

        # Load CSV (with proper quoting for JSON fields)
        df = pd.read_csv(csv_path, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)

        if df.empty:
            print("âš ï¸ ë¡œê·¸ ë°ì´í„° ì—†ìŒ")
            return False

        # Get or create worksheet
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
        except:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=50)

        # Clear and upload (full replace strategy for logs)
        worksheet.clear()

        # Convert DataFrame to list of lists (with header)
        data = [df.columns.tolist()] + df.values.tolist()

        # Upload
        worksheet.update('A1', data, value_input_option='RAW')

        print(f"âœ… ë¡œê·¸ ë™ê¸°í™” ì™„ë£Œ: {len(df)}ê°œ ì‹¤í–‰ ê¸°ë¡")
        return True

    except Exception as e:
        print(f"âš ï¸ ë¡œê·¸ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        return False
