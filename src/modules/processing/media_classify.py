"""
media_classify.py - Media Outlet Classification Module
ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ë° ë§¤ì²´ ì •ë³´ ì¶”ê°€
"""

import json
import time
import random
import uuid
import yaml
import requests
from typing import Dict, List, Optional
from urllib.parse import urlparse
from pathlib import Path
import pandas as pd

OPENAI_API_URL = "https://api.openai.com/v1/responses"

_error_callback = None


def set_error_callback(callback):
    """Register error logger callback: fn(message: str, data: dict)."""
    global _error_callback
    _error_callback = callback


def load_api_models() -> dict:
    """api_models.yamlì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
    yaml_path = Path(__file__).parent.parent.parent / "api_models.yaml"
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            return config.get("models", {})
    except FileNotFoundError:
        return {"media_classification": "gpt-5-nano"}


def extract_domain_safe(url: str) -> str:
    """
    URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ (www. ì œê±°, ì—ëŸ¬ ì•ˆì „)

    Args:
        url: ê¸°ì‚¬ URL

    Returns:
        ë„ë©”ì¸ (ì˜ˆ: "chosun.com") ë˜ëŠ” ë¹ˆ ë¬¸ìì—´

    Examples:
        https://www.chosun.com/article/123 â†’ chosun.com
        https://woman.chosun.com/article/456 â†’ woman.chosun.com
        invalid-url â†’ ""
    """
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        return domain if domain else ""
    except Exception:
        return ""


def load_media_directory(spreadsheet=None, csv_path: Path = None) -> Dict[str, Dict]:
    """
    media_directory ë¡œë“œ (Google Sheets ë˜ëŠ” CSV). ìš°ì„ ìˆœìœ„: Sheets > CSV > ë¹ˆ dict

    Args:
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­)
        csv_path: CSV íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)

    Returns:
        {domain: {"media_name": ..., "media_group": ..., "media_type": ...}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    # 1. Google Sheets ì‹œë„
    if spreadsheet:
        try:
            worksheet = spreadsheet.worksheet("media_directory")
            existing_data = worksheet.get_all_records()

            if existing_data:
                media_dir = {}
                for row in existing_data:
                    domain = row.get("domain", "").strip()
                    if domain:
                        media_dir[domain] = {
                            "media_name": row.get("media_name", ""),
                            "media_group": row.get("media_group", ""),
                            "media_type": row.get("media_type", "")
                        }
                print(f"ğŸ“‚ media_directory (Google Sheets): {len(media_dir)}ê°œ ë„ë©”ì¸ ë¡œë“œ")
                return media_dir
        except Exception as e:
            print(f"  âš ï¸  Google Sheets ë¡œë“œ ì‹¤íŒ¨: {e}, CSV ì‹œë„")

    # 2. CSV ì‹œë„
    if csv_path and csv_path.exists():
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')

            media_dir = {}
            for _, row in df.iterrows():
                domain = str(row.get("domain", "")).strip()
                if domain:
                    media_dir[domain] = {
                        "media_name": str(row.get("media_name", "")),
                        "media_group": str(row.get("media_group", "")),
                        "media_type": str(row.get("media_type", ""))
                    }
            print(f"ğŸ“‚ media_directory (CSV): {len(media_dir)}ê°œ ë„ë©”ì¸ ë¡œë“œ")
            return media_dir
        except Exception as e:
            print(f"  âš ï¸  CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

    # 3. ë¹ˆ dict
    print("  â„¹ï¸  media_directoryê°€ ì—†ìŠµë‹ˆë‹¤. ì‹ ê·œ ìƒì„±í•©ë‹ˆë‹¤.")
    return {}


def classify_media_outlets_batch(
    domains: List[str],
    openai_key: str,
    retry: bool = True,
    retry_count: int = 0,
    max_retries: int = 5,
    base_wait: int = 15,
    request_id: str = None
) -> Dict[str, Dict]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ë¡ ì‚¬ ì •ë³´ ë¶„ë¥˜ (ë°°ì¹˜ ì²˜ë¦¬)

    Args:
        domains: ë¶„ë¥˜í•  ë„ë©”ì¸ ëª©ë¡
        openai_key: OpenAI API í‚¤
        retry: ì¬ì‹œë„ ì—¬ë¶€

    Returns:
        {domain: {"media_name": ..., "media_group": ..., "media_type": ...}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    if not domains:
        return {}

    # ë„ë©”ì¸ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    domain_list = "\n".join(domains)

    prompt = f"""í•œêµ­ ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ë„ë©”ì¸:
{domain_list}

JSONë§Œ ë°˜í™˜:
{{"classifications":[{{"domain":"example.com","media_name":"ì˜ˆì‹œì–¸ë¡ ","media_group":"ì˜ˆì‹œê·¸ë£¹","media_type":"ì¢…í•©ì§€"}}]}}

ë¶„ë¥˜:
- ì¢…í•©ì§€/ê²½ì œì§€/ITì „ë¬¸ì§€/ë°©ì†¡ì‚¬/í†µì‹ ì‚¬/ì¸í„°ë„·ì‹ ë¬¸/ê¸°íƒ€
- groupì€ ëª¨ë¥´ë©´ media_name ë™ì¼
- JSONë§Œ"""

    headers = {
        "Authorization": f"Bearer {openai_key}",
        "Content-Type": "application/json"
    }

    # api_models.yamlì—ì„œ ëª¨ë¸ ë¡œë“œ
    api_models = load_api_models()
    model = api_models.get("media_classification", "gpt-5-nano")

    data = {
        "model": model,
        "input": [
            {"role": "user", "content": [{"type": "input_text", "text": prompt}]}
        ],
        "text": {
            "format": {
                "type": "json_schema",
                "name": "media_classification_batch",
                "strict": True,
                "schema": {
                    "type": "object",
                    "additionalProperties": False,
                    "required": ["classifications"],
                    "properties": {
                        "classifications": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "additionalProperties": False,
                                "required": ["domain", "media_name", "media_group", "media_type"],
                                "properties": {
                                    "domain": {"type": "string"},
                                    "media_name": {"type": "string"},
                                    "media_group": {"type": "string"},
                                    "media_type": {
                                        "type": "string",
                                        "enum": ["ì¢…í•©ì§€", "ê²½ì œì§€", "ITì „ë¬¸ì§€", "ë°©ì†¡ì‚¬", "í†µì‹ ì‚¬", "ì¸í„°ë„·ì‹ ë¬¸", "ê¸°íƒ€"]
                                    }
                                }
                            }
                        }
                    }
                }
            }
        },
        "max_output_tokens": min(len(domains) * 100, 8000)  # ìµœì†Œ 100í† í°/ë„ë©”ì¸, ìµœëŒ€ 8000
    }

    request_id = request_id or uuid.uuid4().hex[:8]
    current_retry = retry_count

    while True:
        try:
            print(f"  ğŸ¤– OpenAI ë¶„ë¥˜: {len(domains)}ê°œ ì‹ ê·œ ë„ë©”ì¸", end="", flush=True)
            response = requests.post(
                OPENAI_API_URL,
                headers=headers,
                json=data,
                timeout=60
            )

            if response.status_code == 429:  # Rate limit
                if retry and current_retry < max_retries - 1:
                    wait_time = base_wait * (2 ** current_retry)
                    retry_after = response.headers.get("Retry-After")
                    retry_after_seconds = None
                    if retry_after:
                        try:
                            retry_after_seconds = float(retry_after)
                        except ValueError:
                            retry_after_seconds = None
                    if retry_after_seconds is not None:
                        wait_time = retry_after_seconds
                    wait_time = max(wait_time, 10)
                    jitter = random.uniform(0, 6)
                    wait_time += jitter
                    if _error_callback:
                        _error_callback(
                            "OpenAI ìš”ì²­ í•œë„ ì´ˆê³¼ (429)",
                            {
                                "request_id": request_id,
                                "status": 429,
                                "retry_after": retry_after_seconds,
                                "wait_time": round(wait_time, 1),
                                "attempt": current_retry + 1,
                            }
                        )
                    if retry_after_seconds is not None:
                        print(f" (Rate limit [req:{request_id}] Retry-After={retry_after_seconds:.1f}s, jitter={jitter:.1f}s â†’ {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„, retry {current_retry + 1}/{max_retries})")
                    else:
                        print(f" (Rate limit [req:{request_id}] Retry-After ì—†ìŒ, jitter={jitter:.1f}s â†’ {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„, retry {current_retry + 1}/{max_retries})")
                    time.sleep(wait_time)
                    current_retry += 1
                    continue
                print(" (Rate limit ì´ˆê³¼, ê¸°ë³¸ê°’ ì‚¬ìš©)")
                if _error_callback:
                    _error_callback(
                        "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ 429 - ì¬ì‹œë„ ì‹¤íŒ¨",
                        {"request_id": request_id, "status": 429, "attempts": max_retries}
                    )
                return _fallback_classification(domains)

            if response.status_code != 200:
                print(f" (API ì˜¤ë¥˜ {response.status_code}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
                if _error_callback:
                    _error_callback(
                        "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ API ì˜¤ë¥˜",
                        {"request_id": request_id, "status": response.status_code}
                    )
                return _fallback_classification(domains)

            result = response.json()
            content = result.get("output_text", "").strip()
            if not content:
                output = result.get("output", [])
                if output:
                    contents = output[0].get("content", [])
                    for item in contents:
                        if item.get("type") == "output_text" and item.get("text"):
                            content = item["text"].strip()
                            break

            try:
                parsed = json.loads(content)
                classifications = parsed.get("classifications", [])
            except json.JSONDecodeError as e:
                if _error_callback:
                    _error_callback(
                        "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ íŒŒì‹± ì‹¤íŒ¨",
                        {"request_id": request_id, "error": f"{type(e).__name__}: {e}", "attempt": current_retry + 1}
                    )
                if retry and current_retry < max_retries - 1:
                    print(f" (JSON íŒŒì‹± ì‹¤íŒ¨ [req:{request_id}]: {str(e)[:50]}, 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ {current_retry + 1}/{max_retries})")
                    time.sleep(2)
                    current_retry += 1
                    continue
                else:
                    print(f" (JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)[:50]}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
                    if _error_callback:
                        _error_callback(
                            "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ íŒŒì‹± ì‹¤íŒ¨",
                            {"request_id": request_id, "error": f"{type(e).__name__}: {e}", "attempt": current_retry + 1}
                        )
                    return _fallback_classification(domains)

            # ë„ë©”ì¸ ê¸°ì¤€ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            media_info = {}
            for item in classifications:
                domain = item.get("domain", "")
                if domain:
                    media_info[domain] = {
                        "media_name": item.get("media_name", domain),
                        "media_group": item.get("media_group", domain),
                        "media_type": item.get("media_type", "ê¸°íƒ€")
                    }

            print(f" âœ…")
            return media_info

        except requests.exceptions.Timeout:
            print(" (Timeout, ê¸°ë³¸ê°’ ì‚¬ìš©)")
            if _error_callback:
                _error_callback(
                    "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ Timeout",
                    {"request_id": request_id}
                )
            return _fallback_classification(domains)
        except Exception as e:
            print(f" (ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
            if _error_callback:
                _error_callback(
                    "OpenAI ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ì‹¤íŒ¨(ì˜ˆì™¸)",
                    {"request_id": request_id, "error": str(e)}
                )
            return _fallback_classification(domains)


def _fallback_classification(domains: List[str]) -> Dict[str, Dict]:
    """
    OpenAI í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©

    Args:
        domains: ë„ë©”ì¸ ëª©ë¡

    Returns:
        {domain: {media_name: domain, media_group: domain, media_type: "ê¸°íƒ€"}}
    """
    return {
        domain: {
            "media_name": domain,
            "media_group": domain,
            "media_type": "ê¸°íƒ€"
        }
        for domain in domains
    }


def update_media_directory(spreadsheet=None, new_entries: Dict[str, Dict] = None, csv_path: Path = None) -> None:
    """
    media_directory ì—…ë°ì´íŠ¸ (Google Sheets ë°/ë˜ëŠ” CSV)

    Args:
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­)
        new_entries: {domain: {"media_name": ..., "media_group": ..., "media_type": ...}}
        csv_path: CSV íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    """
    if not new_entries:
        return

    # 1. Google Sheets ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ rate limit ë°©ì§€)
    if spreadsheet:
        try:
            try:
                worksheet = spreadsheet.worksheet("media_directory")
            except:
                worksheet = spreadsheet.add_worksheet(title="media_directory", rows=1, cols=4)
                worksheet.append_row(["domain", "media_name", "media_group", "media_type"])

            # ë°°ì¹˜ ì—…ë°ì´íŠ¸: ëª¨ë“  í–‰ì„ í•œ ë²ˆì— ì¶”ê°€
            rows_to_add = [
                [domain, info.get("media_name", ""), info.get("media_group", ""), info.get("media_type", "")]
                for domain, info in new_entries.items()
            ]

            # append_rowsë¡œ í•œ ë²ˆì— ì¶”ê°€ (1 write request)
            if rows_to_add:
                worksheet.append_rows(rows_to_add)

            print(f"âœ… media_directory (Google Sheets): {len(new_entries)}ê°œ ì‹ ê·œ ë„ë©”ì¸ ì¶”ê°€")
        except Exception as e:
            print(f"âš ï¸  Google Sheets ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    # 2. CSV ì—…ë°ì´íŠ¸ (í•­ìƒ ì‹¤í–‰)
    if csv_path:
        try:
            if csv_path.exists():
                existing_df = pd.read_csv(csv_path, encoding='utf-8-sig')
            else:
                existing_df = pd.DataFrame(columns=["domain", "media_name", "media_group", "media_type"])

            new_rows = [
                {"domain": domain, "media_name": info.get("media_name", ""),
                 "media_group": info.get("media_group", ""), "media_type": info.get("media_type", "")}
                for domain, info in new_entries.items()
            ]

            new_df = pd.DataFrame(new_rows)
            updated_df = pd.concat([existing_df, new_df], ignore_index=True)
            updated_df = updated_df.drop_duplicates(subset=["domain"], keep="last")

            csv_path.parent.mkdir(parents=True, exist_ok=True)
            updated_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

            print(f"âœ… media_directory (CSV): {len(new_entries)}ê°œ ì‹ ê·œ ë„ë©”ì¸ ì¶”ê°€")
        except Exception as e:
            print(f"âš ï¸  CSV ì €ì¥ ì‹¤íŒ¨: {e}")


def add_media_columns(
    df: pd.DataFrame,
    spreadsheet=None,
    openai_key: str = None,
    csv_path: Path = None
) -> pd.DataFrame:
    """
    DataFrameì— ì–¸ë¡ ì‚¬ ì •ë³´ ì»¬ëŸ¼ ì¶”ê°€

    Args:
        df: ì²˜ë¦¬ëœ DataFrame (originallink ì»¬ëŸ¼ í•„ìš”)
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­)
        openai_key: OpenAI API í‚¤ (ì„ íƒì‚¬í•­)
        csv_path: media_directory CSV ê²½ë¡œ (ì„ íƒì‚¬í•­)

    Returns:
        4ê°œì˜ ìƒˆë¡œìš´ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame:
        - media_domain: ì¶”ì¶œëœ ë„ë©”ì¸
        - media_name: ì–¸ë¡ ì‚¬ëª…
        - media_group: ì–¸ë¡ ì‚¬ ê·¸ë£¹
        - media_type: ë§¤ì²´ ë¶„ë¥˜
    """
    print("ğŸ¢ ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ ì¤‘...")
    df = df.copy()

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["media_domain"] = ""
    df["media_name"] = ""
    df["media_group"] = ""
    df["media_type"] = ""

    # originallink ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ë°˜í™˜
    if "originallink" not in df.columns:
        print("  âš ï¸  originallink ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df

    try:
        # ë„ë©”ì¸ ì¶”ì¶œ
        df["media_domain"] = df["originallink"].apply(extract_domain_safe)

        # ê³ ìœ í•œ ë„ë©”ì¸ ëª©ë¡
        unique_domains = df[df["media_domain"] != ""]["media_domain"].unique()
        unique_domains = [d for d in unique_domains if d]

        if not unique_domains:
            print("  âš ï¸  ì¶”ì¶œëœ ë„ë©”ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return df

        # media_directory ë¡œë“œ (Google Sheets ë˜ëŠ” CSV)
        existing_media = load_media_directory(spreadsheet=spreadsheet, csv_path=csv_path)
        new_domains = [d for d in unique_domains if d not in existing_media]

        # ì‹ ê·œ ë„ë©”ì¸ ë¶„ë¥˜ (OpenAI) - ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
        if new_domains and openai_key:
            new_media = {}
            for i in range(0, len(new_domains), 100):
                batch = new_domains[i:i+100]
                batch_result = classify_media_outlets_batch(batch, openai_key)
                new_media.update(batch_result)

            existing_media.update(new_media)

            # media_directory ì—…ë°ì´íŠ¸ (Sheets + CSV)
            update_media_directory(spreadsheet=spreadsheet, new_entries=new_media, csv_path=csv_path)

        # DataFrameì— ì •ë³´ ì¶”ê°€
        for idx, row in df.iterrows():
            domain = row["media_domain"]
            if domain in existing_media:
                info = existing_media[domain]
                df.at[idx, "media_name"] = info.get("media_name", "")
                df.at[idx, "media_group"] = info.get("media_group", "")
                df.at[idx, "media_type"] = info.get("media_type", "")

        # í†µê³„
        has_info = (df["media_name"] != "").sum()
        print(f"âœ… ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬ì— ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€")
        print(f"  - ê¸°ì¡´ ë””ë ‰í† ë¦¬: {len(existing_media) - len(new_domains)}ê°œ")
        if new_domains:
            print(f"  - ì‹ ê·œ ë¶„ë¥˜: {len(new_domains)}ê°œ")

        return df

    except Exception as e:
        print(f"âš ï¸  ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return df
