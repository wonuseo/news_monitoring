"""
fulltext.py - Full-Text Article Extraction Module
ìœ„í—˜ë„ê°€ ë†’ì€ ê¸°ì‚¬ì— ëŒ€í•´ ì „ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ëª¨ë“ˆ
"""

import time
import requests
import pandas as pd
from typing import List, Dict, Optional
from datetime import datetime
from bs4 import BeautifulSoup


def fetch_full_text(url: str, timeout: int = 10) -> Dict[str, str]:
    """
    ë‹¨ì¼ ê¸°ì‚¬ì˜ ì „ë¬¸ì„ ìŠ¤í¬ë˜í•‘

    Args:
        url: ê¸°ì‚¬ URL (Naver ë˜ëŠ” ì›ë³¸ ë§í¬)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        {
            "full_text": ê¸°ì‚¬ ì „ë¬¸ (ë˜ëŠ” ë¹ˆ ë¬¸ìì—´),
            "status": "success" | "paywall" | "404" | "timeout" | "error"
        }
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }

        response = requests.get(url, headers=headers, timeout=timeout)

        # 404/403 ì²˜ë¦¬
        if response.status_code == 404:
            return {"full_text": "", "status": "404"}
        elif response.status_code == 403:
            return {"full_text": "", "status": "paywall"}
        elif response.status_code >= 400:
            return {"full_text": "", "status": f"http_{response.status_code}"}

        response.raise_for_status()

        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(response.content, "lxml")

        # Naver ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        article_body = None

        # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ (id="articleBodyContents")
        article_body = soup.find("div", id="articleBodyContents")

        # 2. ë˜ëŠ” ì¼ë°˜ article íƒœê·¸
        if not article_body:
            article_body = soup.find("article")

        # 3. ë˜ëŠ” div classê°€ contentì¸ ê²½ìš°
        if not article_body:
            article_body = soup.find("div", class_=lambda x: x and "content" in x.lower())

        if not article_body:
            return {"full_text": "", "status": "no_content"}

        # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = article_body.get_text(separator="\n", strip=True)

        # ê³µë°± ì •ë¦¬
        full_text = "\n".join(line.strip() for line in full_text.split("\n") if line.strip())

        if not full_text or len(full_text) < 50:
            return {"full_text": "", "status": "too_short"}

        return {"full_text": full_text[:5000], "status": "success"}  # ìµœëŒ€ 5000ì ì œí•œ

    except requests.exceptions.Timeout:
        return {"full_text": "", "status": "timeout"}
    except requests.exceptions.RequestException as e:
        return {"full_text": "", "status": "request_error"}
    except Exception as e:
        return {"full_text": "", "status": "error"}


def batch_fetch_full_text(df: pd.DataFrame, risk_levels: List[str] = None,
                         max_articles: Optional[int] = None) -> pd.DataFrame:
    """
    DataFrameì˜ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ ì¼ê´„ ì „ë¬¸ ìŠ¤í¬ë˜í•‘

    Args:
        df: ë¶„ë¥˜ëœ DataFrame (sentiment, risk_level í¬í•¨)
        risk_levels: ìŠ¤í¬ë˜í•‘í•  ìœ„í—˜ë„ ëª©ë¡ (ê¸°ë³¸: ['ìƒ', 'ì¤‘'])
        max_articles: ìµœëŒ€ ìŠ¤í¬ë˜í•‘ ê¸°ì‚¬ ìˆ˜ (None = ë¬´ì œí•œ)

    Returns:
        ë‹¤ìŒ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame:
        - full_text: ê¸°ì‚¬ ì „ë¬¸
        - full_text_status: ìŠ¤í¬ë˜í•‘ ìƒíƒœ
        - full_text_scraped_at: ìŠ¤í¬ë˜í•‘ ì‹œê°„
    """
    df = df.copy()

    # ê¸°ë³¸ê°’
    if risk_levels is None:
        risk_levels = ["ìƒ", "ì¤‘"]

    # ì´ˆê¸°í™”
    df["full_text"] = ""
    df["full_text_status"] = "not_scraped"
    df["full_text_scraped_at"] = ""

    # í•„í„°ë§: ì§€ì •í•œ ìœ„í—˜ë„ë§Œ
    if "risk_level" in df.columns:
        mask = df["risk_level"].isin(risk_levels)
        indices_to_scrape = df[mask].index.tolist()
    else:
        # risk_level ì»¬ëŸ¼ ì—†ìœ¼ë©´ ëª¨ë‘ ìŠ¤í¬ë˜í•‘
        indices_to_scrape = df.index.tolist()

    if max_articles:
        indices_to_scrape = indices_to_scrape[:max_articles]

    print(f"ğŸ“„ ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {len(indices_to_scrape)}ê°œ ê¸°ì‚¬")

    for i, idx in enumerate(indices_to_scrape):
        try:
            # ë§í¬ ì„ íƒ: originallink ìš°ì„ , ì—†ìœ¼ë©´ link
            url = df.at[idx, "link"]
            if "originallink" in df.columns and df.at[idx, "originallink"]:
                url = df.at[idx, "originallink"]

            if not url or not isinstance(url, str):
                df.at[idx, "full_text_status"] = "no_url"
                continue

            # ê¸°ì‚¬ ì œëª© (ë¡œê¹…ìš©)
            title = df.at[idx, "title"] if "title" in df.columns else "Unknown"

            # ì „ë¬¸ ìŠ¤í¬ë˜í•‘
            result = fetch_full_text(url)

            df.at[idx, "full_text"] = result["full_text"]
            df.at[idx, "full_text_status"] = result["status"]
            df.at[idx, "full_text_scraped_at"] = datetime.now().isoformat()

            # ì§„í–‰ìƒí™© ë¡œê¹…
            status_icon = "âœ“" if result["status"] == "success" else "âš "
            print(f"  [{i+1}/{len(indices_to_scrape)}] {status_icon} {title[:50]}... ({result['status']})")

            # Rate limiting
            time.sleep(1)

        except Exception as e:
            print(f"  [{i+1}/{len(indices_to_scrape)}] âŒ ì˜¤ë¥˜: {str(e)[:50]}")
            df.at[idx, "full_text_status"] = "error"

    # í†µê³„
    success_count = (df["full_text_status"] == "success").sum()
    paywall_count = (df["full_text_status"] == "paywall").sum()
    error_count = (df["full_text_status"].isin(["error", "404", "timeout", "no_content"])).sum()

    print(f"\nâœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ:")
    print(f"  - ì„±ê³µ: {success_count}ê°œ")
    print(f"  - í˜ì´ì›”: {paywall_count}ê°œ")
    print(f"  - ì‹¤íŒ¨: {error_count}ê°œ")

    return df
