"""
press_release_detector.py - Press Release Detection & Summarization
ë³´ë„ìë£Œ ê²€ì¶œ, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½ (TF-IDF + Cosine Similarity + OpenAI)
"""

import re
import json
import time
import yaml
import requests
import pandas as pd
from pathlib import Path
from typing import Dict, List


def _extract_response_text(result: Dict) -> str:
    text = result.get("output_text", "")
    if isinstance(text, str) and text.strip():
        return text.strip()

    for section in result.get("output", []):
        contents = section.get("content", [])
        for item in contents:
            if not isinstance(item, dict):
                continue
            if item.get("type") == "output_text" and isinstance(item.get("text"), str):
                return item["text"].strip()
            # Some payloads embed raw text without explicit type
            raw_text = item.get("text")
            if isinstance(raw_text, str) and raw_text.strip():
                return raw_text.strip()

    # Fallback to legacy choices (if any)
    for choice in result.get("choices", []):
        message = choice.get("message", {})
        content = message.get("content", "")
        if isinstance(content, str) and content.strip():
            return content.strip()

    return ""


def _trim_json_object(payload: str) -> str:
    # Remove wrapping markdown fences, keep inner braces/brackets
    cleaned = re.sub(r"```(?:json)?\s*", "", payload)
    cleaned = cleaned.replace("`", "")
    start = cleaned.find("{")
    end = cleaned.rfind("}")
    if start != -1 and end != -1 and start < end:
        json_snippet = cleaned[start:end + 1]
        trailing = cleaned[end + 1:]
        trailing_match = re.search(r"\]", trailing)
        if trailing_match:
            json_snippet += trailing[: trailing_match.end()]
        return json_snippet
    return cleaned


def _repair_json_text(payload: str) -> str:
    cleaned = payload.strip()
    cleaned = re.sub(r",\s*([\]\}])", r"\1", cleaned)
    return cleaned


def _parse_summaries_from_result(result: Dict) -> List[Dict]:
    text = _extract_response_text(result)
    if not text:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨: API ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        print(f"   ì „ì²´ ì‘ë‹µ êµ¬ì¡°: {json.dumps(result, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses API ë°˜í™˜ bodyì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    trimmed = _trim_json_object(text)

    last_exc: Exception | None = None
    parsed = None
    candidates = [trimmed, _repair_json_text(trimmed)]

    for i, candidate in enumerate(candidates, 1):
        try:
            parsed = json.loads(candidate)
            break
        except json.JSONDecodeError as exc:
            last_exc = exc
            print(f"âŒ JSON íŒŒì‹± ì‹œë„ {i} ì‹¤íŒ¨: {exc}")
            print(f"   ì‹œë„í•œ í…ìŠ¤íŠ¸ (ì• 300ì): {candidate[:300]}")

    if parsed is None:
        print("âŒ ëª¨ë“  JSON íŒŒì‹± ì‹œë„ ì‹¤íŒ¨")
        print(f"   ì›ë³¸ í…ìŠ¤íŠ¸ (ì• 500ì): {text[:500]}")
        print(f"   ì •ì œëœ í…ìŠ¤íŠ¸ (ì• 500ì): {trimmed[:500]}")
        raise last_exc or ValueError("Responses API JSONì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if isinstance(parsed, dict):
        summaries = parsed.get("summaries")
    elif isinstance(parsed, list):
        summaries = parsed
    else:
        print(f"âŒ ì˜ˆìƒí•˜ì§€ ëª»í•œ JSON êµ¬ì¡°: {type(parsed)}")
        print(f"   íŒŒì‹±ëœ ê°ì²´: {json.dumps(parsed, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses APIê°€ ìš”ì•½ ëª©ë¡ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not isinstance(summaries, list):
        print(f"âŒ summariesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(summaries)}")
        print(f"   summaries ê°’: {summaries}")
        raise ValueError("Responses APIê°€ summaries ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return summaries

OPENAI_API_URL = "https://api.openai.com/v1/responses"


def load_api_models() -> dict:
    """api_models.yamlì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
    yaml_path = Path(__file__).parent.parent.parent / "api_models.yaml"
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            return config.get("models", {})
    except FileNotFoundError:
        return {"press_release_summary": "gpt-5-nano"}


def detect_similar_articles(
    df: pd.DataFrame,
    day_window: int = 3,
    strong_start_day: int = 4,
    thr_title_cos: float = 0.70,
    thr_title_jac: float = 0.18,
    thr_desc_cos: float = 0.60,
    thr_desc_jac: float = 0.10,
    strong_desc_cos: float = 0.85,
    strong_desc_jac: float = 0.35,
    min_token_len: int = 2
) -> pd.DataFrame:
    """
    ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ë¡œ ë³´ë„ìë£Œ ì‹ë³„
    - ë‚ ì§œ ê¸°ë°˜ ì°¨ë“± ì„ê³„ê°’ ì ìš© (Î”days â‰¤ 3 vs Î”days â‰¥ 4)
    - TF-IDF Cosine + Jaccard ì´ì¤‘ ìœ ì‚¬ë„ ì¸¡ì •
    - Title/Description ë³„ë„ ì²˜ë¦¬
    - Queryë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§
    - BFSë¡œ Connected Components ê·¸ë£¹í™”
    - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€ (ë¹„íŒŒê´´ì  ë¼ë²¨ë§ë§Œ ìˆ˜í–‰)

    Args:
        df: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°ëœ DataFrame (query, pub_datetime ì»¬ëŸ¼ í•„ìˆ˜)
        day_window: ê¸°ë³¸ ê·œì¹™ ì ìš© ë‚ ì§œ ë²”ìœ„ (ê¸°ë³¸ê°’: 3ì¼)
        strong_start_day: ì´ˆê°•ìœ ì‚¬ ê·œì¹™ ì‹œì‘ ë‚ ì§œ (ê¸°ë³¸ê°’: 4ì¼)
        thr_title_cos: Title Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.70)
        thr_title_jac: Title Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.18)
        thr_desc_cos: Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.60)
        thr_desc_jac: Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.10)
        strong_desc_cos: ì´ˆê°•ìœ ì‚¬ Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.85)
        strong_desc_jac: ì´ˆê°•ìœ ì‚¬ Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.35)
        min_token_len: í† í° ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 2)

    Returns:
        'source', 'cluster_id' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
    print(f"  - ê¸°ë³¸ ê·œì¹™(Î”daysâ‰¤{day_window}): title {thr_title_cos}/{thr_title_jac}, desc {thr_desc_cos}/{thr_desc_jac}")
    print(f"  - ì´ˆê°•ìœ ì‚¬(Î”daysâ‰¥{strong_start_day}): desc {strong_desc_cos}/{strong_desc_jac}")

    df = df.copy()

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["source"] = "ì¼ë°˜ê¸°ì‚¬"
    df["cluster_id"] = ""

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        from collections import deque
    except ImportError as e:
        print(f"âš ï¸  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ({e}). ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if "query" not in df.columns:
        print("âš ï¸  'query' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ìœ í‹¸ í•¨ìˆ˜ë“¤
    def clean_html(s: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r"<[^>]+>", " ", s)
        s = s.replace("&quot;", " ").replace("&amp;", "&")
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def tokenize_simple(s: str):
        """ê°„ë‹¨ í† í°í™” (í•œê¸€/ì˜ë¬¸/ìˆ«ì ë©ì–´ë¦¬)"""
        if not isinstance(s, str) or not s.strip():
            return []
        s = s.lower()
        toks = re.findall(r"[ê°€-í£a-z0-9]+", s)
        return [t for t in toks if len(t) >= min_token_len]

    def jaccard(a: set, b: set) -> float:
        """Jaccard ìœ ì‚¬ë„"""
        if not a and not b:
            return 0.0
        inter = len(a & b)
        union = len(a | b)
        return inter / union if union else 0.0

    def to_ordinal_safe(ts):
        """datetime â†’ ordinal, NaTë©´ np.nan"""
        if pd.isna(ts):
            return np.nan
        try:
            return pd.to_datetime(ts).to_pydatetime().date().toordinal()
        except:
            return np.nan

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    df["title_clean"] = df["title"].fillna("").map(clean_html)
    df["desc_clean"] = df["description"].fillna("").map(clean_html)

    # ë‚ ì§œ ë³€í™˜ (pub_datetime â†’ ordinal)
    df["pub_dt_ordinal"] = df["pub_datetime"].map(to_ordinal_safe)

    # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
    valid_mask = (df["title_clean"].str.len() > 0) | (df["desc_clean"].str.len() > 0)
    if valid_mask.sum() < 2:
        print(f"âœ… ìœ íš¨í•œ ê¸°ì‚¬ê°€ {valid_mask.sum()}ê°œë¡œ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ ìŠ¤í‚µ")
        df = df.drop(columns=["title_clean", "desc_clean", "pub_dt_ordinal"])
        return df

    # Jaccardìš© í† í° ì…‹
    df["title_tokset"] = df["title_clean"].map(lambda x: set(tokenize_simple(x)))
    df["desc_tokset"] = df["desc_clean"].map(lambda x: set(tokenize_simple(x)))

    # Queryë³„ í´ëŸ¬ìŠ¤í„°ë§
    total_press_release = 0
    total_clusters = 0

    for query, group_df in df.groupby("query"):
        cluster_id_counter = 1  # Queryë³„ë¡œ 1ë¶€í„° ì‹œì‘
        idxs = group_df.index.tolist()
        m = len(idxs)

        # ë‹¨ë… ê¸°ì‚¬
        if m == 1:
            continue

        try:
            # TF-IDF ë²¡í„°í™” (Title/Description ë³„ë„)
            vec_title = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)
            vec_desc = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)

            X_title = vec_title.fit_transform(df.loc[idxs, "title_clean"])
            X_desc = vec_desc.fit_transform(df.loc[idxs, "desc_clean"])

            # Cosine similarity matrices
            sim_t = cosine_similarity(X_title)
            sim_d = cosine_similarity(X_desc)

            # ë‚ ì§œ ì°¨ì´(ì¼) í–‰ë ¬
            ordinals = df.loc[idxs, "pub_dt_ordinal"].to_numpy()
            dd = np.abs(ordinals[:, None] - ordinals[None, :])  # NaN í¬í•¨ ê°€ëŠ¥

            close_mask = (dd <= day_window)  # Î”days â‰¤ 3
            far_mask = (dd >= strong_start_day)  # Î”days â‰¥ 4

            # Adjacency list
            adj = [set() for _ in range(m)]

            # í† í° ì…‹ ìºì‹œ
            title_sets = df.loc[idxs, "title_tokset"].tolist()
            desc_sets = df.loc[idxs, "desc_tokset"].tolist()

            # (1) Î”days â‰¤ 3: Title ê·œì¹™
            cand_title = np.triu((sim_t >= thr_title_cos) & close_mask, k=1)
            ti, tj = np.where(cand_title)
            for i, j in zip(ti, tj):
                if jaccard(title_sets[i], title_sets[j]) >= thr_title_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (2) Î”days â‰¤ 3: Description ê·œì¹™
            cand_desc = np.triu((sim_d >= thr_desc_cos) & close_mask, k=1)
            di, dj = np.where(cand_desc)
            for i, j in zip(di, dj):
                if jaccard(desc_sets[i], desc_sets[j]) >= thr_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (3) Î”days â‰¥ 4: ì´ˆê°•ìœ ì‚¬ ê·œì¹™
            cand_strong = np.triu((sim_d >= strong_desc_cos) & far_mask, k=1)
            si, sj = np.where(cand_strong)
            for i, j in zip(si, sj):
                if jaccard(desc_sets[i], desc_sets[j]) >= strong_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # Connected components (BFS)
            visited = [False] * m
            for i in range(m):
                if visited[i]:
                    continue

                queue = deque([i])
                visited[i] = True
                comp = [i]

                while queue:
                    u = queue.popleft()
                    for v in adj[u]:
                        if not visited[v]:
                            visited[v] = True
                            queue.append(v)
                            comp.append(v)

                # 2ê°œ ì´ìƒì¸ í´ëŸ¬ìŠ¤í„°ë§Œ ì²˜ë¦¬
                if len(comp) < 2:
                    continue

                members_global = [idxs[k] for k in comp]
                cid = f"{query}_c{cluster_id_counter:05d}"
                cluster_id_counter += 1
                total_clusters += 1

                for gidx in members_global:
                    df.at[gidx, "source"] = "ë³´ë„ìë£Œ"
                    df.at[gidx, "cluster_id"] = cid
                    total_press_release += 1

        except Exception as e:
            print(f"âš ï¸  '{query}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["title_clean", "desc_clean", "pub_dt_ordinal", "title_tokset", "desc_tokset"])

    unique_count = len(df) - total_press_release
    print(f"âœ… {total_press_release}ê°œ ê¸°ì‚¬ë¥¼ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ ({total_clusters}ê°œ í´ëŸ¬ìŠ¤í„°), {unique_count}ê°œ ê¸°ì‚¬ëŠ” ë…ë¦½ ê¸°ì‚¬")

    return df


def _call_openai_summarize_batch(
    articles: List[Dict],
    openai_key: str,
    retry: bool = True
) -> Dict[str, str]:
    """OpenAI APIë¡œ ê¸°ì‚¬ ë°°ì¹˜ ìš”ì•½"""
    articles_text = "\n".join([
        f"[{a['cluster_id']}]\nì œëª©: {a['title']}\nì„¤ëª…: {a['description'][:200]}"
        for a in articles
    ])

    prompt = f"""ê° ê¸°ì‚¬ë¥¼ 5ë‹¨ì–´ ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ê¸°ì‚¬:
{articles_text}

IMPORTANT: JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì‘ë‹µ:
{{"summaries":[{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00001","summary":"ì‹ ë¼í˜¸í…” ê°œì¥"}},{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00002","summary":"í˜¸í…” ë¦¬ëª¨ë¸ë§"}}]}}

ê·œì¹™:
- ë°˜ë“œì‹œ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
- ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì²´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ
- 5ë‹¨ì–´ ì´ë‚´ í•œêµ­ì–´ ìš”ì•½
- ëª…ì‚¬í˜• ìœ„ì£¼
- ì½”ë“œ ë¸”ë¡(```)ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ ê¸ˆì§€"""

    # api_models.yamlì—ì„œ ëª¨ë¸ ë¡œë“œ
    api_models = load_api_models()
    model = api_models.get("press_release_summary", "gpt-5-nano")

    try:
        response = requests.post(
            OPENAI_API_URL,
            headers={"Authorization": f"Bearer {openai_key}", "Content-Type": "application/json"},
            json={
                "model": model,
                "input": [{"role": "user", "content": [{"type": "input_text", "text": prompt}]}],
                "text": {
                    "format": {
                        "type": "json_schema",
                        "name": "press_release_summaries",
                        "strict": False,
                        "schema": {
                            "type": "object",
                            "required": ["summaries"],
                            "additionalProperties": True,
                            "properties": {
                                "summaries": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "required": ["cluster_id", "summary"],
                                        "additionalProperties": True,
                                        "properties": {
                                            "cluster_id": {"type": "string"},
                                            "summary": {"type": "string"}
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
            },
            timeout=60
        )

        if response.status_code == 429 and retry:
            print("  (Rate limit, 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„)")
            time.sleep(5)
            return _call_openai_summarize_batch(articles, openai_key, retry=False)

        if response.status_code != 200:
            print(f"  (API ì˜¤ë¥˜ {response.status_code}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
            return {a["cluster_id"]: a["title"][:15] for a in articles}

        result = response.json()
        try:
            summaries = _parse_summaries_from_result(result)
            return {
                item["cluster_id"]: item["summary"]
                for item in summaries
                if isinstance(item, dict) and "cluster_id" in item and "summary" in item
            }
        except (ValueError, json.JSONDecodeError, KeyError) as e:
            if retry:
                print(f"  (JSON íŒŒì‹± ì‹¤íŒ¨: {type(e).__name__}, ì¬ì‹œë„)")
                time.sleep(2)
                return _call_openai_summarize_batch(articles, openai_key, retry=False)
            print(f"  (ìµœì¢… íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©)")
            return {a["cluster_id"]: a["title"][:15] for a in articles}

    except Exception as e:
        print(f"  (ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
        return {a["cluster_id"]: a["title"][:15] for a in articles}


def summarize_press_release_groups(
    df: pd.DataFrame,
    openai_key: str
) -> pd.DataFrame:
    """
    ë³´ë„ìë£Œ ê·¸ë£¹ë³„ë¡œ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ë¥¼ OpenAIë¡œ ìš”ì•½

    Args:
        df: cluster_id, pub_datetime, title, description ì»¬ëŸ¼ í¬í•¨
        openai_key: OpenAI API í‚¤

    Returns:
        press_release_group ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ“ ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ìƒì„± ì¤‘...")
    df = df.copy()
    df["press_release_group"] = ""

    press_release_mask = (df["source"] == "ë³´ë„ìë£Œ") & (df["cluster_id"] != "")
    if press_release_mask.sum() == 0:
        print("  â„¹ï¸  ë³´ë„ìë£Œ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df

    try:
        # ê·¸ë£¹ë³„ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ ì„ íƒ
        pr_df = df[press_release_mask].copy()
        pr_df["pub_datetime_parsed"] = pd.to_datetime(pr_df["pub_datetime"], errors="coerce")
        earliest_articles = pr_df.sort_values("pub_datetime_parsed").groupby("cluster_id").first().reset_index()

        # OpenAI ë°°ì¹˜ ìš”ì•½ (20ê°œì”©)
        articles_to_summarize = [
            {"cluster_id": row["cluster_id"], "title": row["title"], "description": row["description"]}
            for _, row in earliest_articles.iterrows()
        ]

        group_summaries = {}
        for i in range(0, len(articles_to_summarize), 20):
            batch = articles_to_summarize[i:i+20]
            batch_summaries = _call_openai_summarize_batch(batch, openai_key)
            group_summaries.update(batch_summaries)

        # press_release_group ì—…ë°ì´íŠ¸
        for idx, row in df[press_release_mask].iterrows():
            cluster_id = row["cluster_id"]
            if cluster_id in group_summaries:
                df.at[idx, "press_release_group"] = group_summaries[cluster_id]

        print(f"âœ… {len(group_summaries)}ê°œ ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì™„ë£Œ")
        return df

    except Exception as e:
        print(f"âš ï¸  ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        return df
