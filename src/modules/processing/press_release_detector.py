"""
press_release_detector.py - Press Release Detection & Summarization
ë³´ë„ìë£Œ ê²€ì¶œ, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½ (TF-IDF + Cosine Similarity + OpenAI)
"""

import re
import json
import uuid
import pandas as pd
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm

from src.utils.openai_client import (
    OPENAI_API_URL,
    set_error_callback,
    load_api_models,
    call_openai_with_retry,
    extract_response_text,
    notify_error,
)

# â”€â”€ ì„ê³„ê°’: config/thresholds.yamlì—ì„œ ë¡œë“œ, ì—†ìœ¼ë©´ í•˜ë“œì½”ë”© fallback â”€â”€
def _load_pr_thresholds():
    try:
        from src.utils.config import load_config
        return load_config("thresholds").get("press_release", {})
    except Exception:
        return {}

_pr_thr = _load_pr_thresholds()
_pr_bl  = _pr_thr.get("borderline", {})


def _trim_json_object(payload: str) -> str:
    # Remove wrapping markdown fences, keep inner braces/brackets
    cleaned = re.sub(r"```(?:json)?\s*", "", payload)
    cleaned = cleaned.replace("`", "")
    start = cleaned.find("{")
    end = cleaned.rfind("}")
    if start != -1 and end != -1 and start < end:
        json_snippet = cleaned[start:end + 1]
        trailing = cleaned[end + 1:]
        trailing_match = re.search(r"\]", trailing)
        if trailing_match:
            json_snippet += trailing[: trailing_match.end()]
        return json_snippet
    return cleaned


def _repair_json_text(payload: str) -> str:
    cleaned = payload.strip()
    cleaned = re.sub(r",\s*([\]\}])", r"\1", cleaned)
    return cleaned


def _parse_summaries_from_result(result: Dict) -> List[Dict]:
    text = extract_response_text(result)
    if not text:
        print("  JSON íŒŒì‹± ì‹¤íŒ¨: API ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        print(f"   ì „ì²´ ì‘ë‹µ êµ¬ì¡°: {json.dumps(result, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses API ë°˜í™˜ bodyì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    trimmed = _trim_json_object(text)

    last_exc: Exception | None = None
    parsed = None
    candidates = [trimmed, _repair_json_text(trimmed)]

    for i, candidate in enumerate(candidates, 1):
        try:
            parsed = json.loads(candidate)
            break
        except json.JSONDecodeError as exc:
            last_exc = exc
            print(f"  JSON íŒŒì‹± ì‹œë„ {i} ì‹¤íŒ¨: {exc}")
            print(f"   ì‹œë„í•œ í…ìŠ¤íŠ¸ (ì• 300ì): {candidate[:300]}")

    if parsed is None:
        print("  ëª¨ë“  JSON íŒŒì‹± ì‹œë„ ì‹¤íŒ¨")
        print(f"   ì›ë³¸ í…ìŠ¤íŠ¸ (ì• 500ì): {text[:500]}")
        print(f"   ì •ì œëœ í…ìŠ¤íŠ¸ (ì• 500ì): {trimmed[:500]}")
        raise last_exc or ValueError("Responses API JSONì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if isinstance(parsed, dict):
        summaries = parsed.get("summaries")
    elif isinstance(parsed, list):
        summaries = parsed
    else:
        print(f"  ì˜ˆìƒí•˜ì§€ ëª»í•œ JSON êµ¬ì¡°: {type(parsed)}")
        print(f"   íŒŒì‹±ëœ ê°ì²´: {json.dumps(parsed, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses APIê°€ ìš”ì•½ ëª©ë¡ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not isinstance(summaries, list):
        print(f"  summariesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(summaries)}")
        print(f"   summaries ê°’: {summaries}")
        raise ValueError("Responses APIê°€ summaries ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return summaries


def detect_similar_articles(
    df: pd.DataFrame,
    spreadsheet=None,
    day_window: int          = None,
    strong_start_day: int    = None,
    thr_title_cos: float     = None,
    thr_title_jac: float     = None,
    thr_desc_cos: float      = None,
    thr_desc_jac: float      = None,
    strong_desc_cos: float   = None,
    strong_desc_jac: float   = None,
    min_token_len: int       = None,
    enable_llm_borderline: bool = False,
    openai_key: str = None,
    group_our_brands: bool = False,
) -> pd.DataFrame:
    """
    ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ë¡œ ë³´ë„ìë£Œ ì‹ë³„
    - ë‚ ì§œ ê¸°ë°˜ ì°¨ë“± ì„ê³„ê°’ ì ìš© (Î”days â‰¤ 3 vs Î”days â‰¥ 4)
    - TF-IDF Cosine + Jaccard ì´ì¤‘ ìœ ì‚¬ë„ ì¸¡ì •
    - Title/Description ë³„ë„ ì²˜ë¦¬
    - Queryë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§
    - BFSë¡œ Connected Components ê·¸ë£¹í™”
    - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€ (ë¹„íŒŒê´´ì  ë¼ë²¨ë§ë§Œ ìˆ˜í–‰)
    - Cumulative cluster numbering (Google Sheets ê¸°ë°˜)

    Args:
        df: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°ëœ DataFrame (query, pub_datetime ì»¬ëŸ¼ í•„ìˆ˜)
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­, cumulative numberingìš©)
        day_window: ê¸°ë³¸ ê·œì¹™ ì ìš© ë‚ ì§œ ë²”ìœ„ (ê¸°ë³¸ê°’: 3ì¼)
        strong_start_day: ì´ˆê°•ìœ ì‚¬ ê·œì¹™ ì‹œì‘ ë‚ ì§œ (ê¸°ë³¸ê°’: 4ì¼)
        thr_title_cos: Title Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.70)
        thr_title_jac: Title Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.18)
        thr_desc_cos: Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.60)
        thr_desc_jac: Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.10)
        strong_desc_cos: ì´ˆê°•ìœ ì‚¬ Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.85)
        strong_desc_jac: ì´ˆê°•ìœ ì‚¬ Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.35)
        min_token_len: í† í° ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 2)

    Returns:
        'source', 'cluster_id' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    # config ê¸°ë³¸ê°’ ì ìš© (ëª…ì‹œì  ì¸ìê°€ ì—†ì„ ë•Œë§Œ)
    if day_window       is None: day_window       = _pr_thr.get("day_window",           3)
    if strong_start_day is None: strong_start_day = _pr_thr.get("strong_start_day",     4)
    if thr_title_cos    is None: thr_title_cos    = _pr_thr.get("title_cosine",         0.70)
    if thr_title_jac    is None: thr_title_jac    = _pr_thr.get("title_jaccard",        0.18)
    if thr_desc_cos     is None: thr_desc_cos     = _pr_thr.get("desc_cosine",          0.60)
    if thr_desc_jac     is None: thr_desc_jac     = _pr_thr.get("desc_jaccard",         0.10)
    if strong_desc_cos  is None: strong_desc_cos  = _pr_thr.get("strong_desc_cosine",   0.85)
    if strong_desc_jac  is None: strong_desc_jac  = _pr_thr.get("strong_desc_jaccard",  0.35)
    if min_token_len    is None: min_token_len    = _pr_thr.get("min_token_len",         2)

    print("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
    print(f"  - ê¸°ë³¸ ê·œì¹™(Î”daysâ‰¤{day_window}): title {thr_title_cos}/{thr_title_jac}, desc {thr_desc_cos}/{thr_desc_jac}")
    print(f"  - ì´ˆê°•ìœ ì‚¬(Î”daysâ‰¥{strong_start_day}): desc {strong_desc_cos}/{strong_desc_jac}")

    df = df.copy()

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["source"] = "ì¼ë°˜ê¸°ì‚¬"
    df["cluster_id"] = ""

    # Cumulative cluster numbering (Google Sheetsì—ì„œ ê¸°ì¡´ ìµœëŒ€ê°’ ê°€ì ¸ì˜¤ê¸°)
    max_cluster_num = 0
    if spreadsheet:
        from src.utils.sheets_helpers import get_max_values_from_sheets
        max_values = get_max_values_from_sheets(spreadsheet)
        max_cluster_num = max_values["max_cluster_num"]
        if max_cluster_num > 0:
            print(f"  ğŸ“Š ê¸°ì¡´ ìµœëŒ€ cluster_id ë²ˆí˜¸: {max_cluster_num} (cumulative numbering)")

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        from collections import deque
    except ImportError as e:
        print(f"  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ({e}). ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if "query" not in df.columns:
        print("  'query' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ìœ í‹¸ í•¨ìˆ˜ë“¤
    def clean_html(s: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r"<[^>]+>", " ", s)
        s = s.replace("&quot;", " ").replace("&amp;", "&")
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def tokenize_simple(s: str):
        """ê°„ë‹¨ í† í°í™” (í•œê¸€/ì˜ë¬¸/ìˆ«ì ë©ì–´ë¦¬)"""
        if not isinstance(s, str) or not s.strip():
            return []
        s = s.lower()
        toks = re.findall(r"[ê°€-í£a-z0-9]+", s)
        return [t for t in toks if len(t) >= min_token_len]

    def jaccard(a: set, b: set) -> float:
        """Jaccard ìœ ì‚¬ë„"""
        if not a and not b:
            return 0.0
        inter = len(a & b)
        union = len(a | b)
        return inter / union if union else 0.0

    def to_ordinal_safe(ts):
        """datetime â†’ ordinal, NaTë©´ np.nan"""
        if pd.isna(ts):
            return np.nan
        try:
            return pd.to_datetime(ts).to_pydatetime().date().toordinal()
        except:
            return np.nan

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    df["title_clean"] = df["title"].fillna("").map(clean_html)
    df["desc_clean"] = df["description"].fillna("").map(clean_html)

    # ë‚ ì§œ ë³€í™˜ (pub_datetime â†’ ordinal)
    df["pub_dt_ordinal"] = df["pub_datetime"].map(to_ordinal_safe)

    # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
    valid_mask = (df["title_clean"].str.len() > 0) | (df["desc_clean"].str.len() > 0)
    if valid_mask.sum() < 2:
        print(f"  ìœ íš¨í•œ ê¸°ì‚¬ê°€ {valid_mask.sum()}ê°œë¡œ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ ìŠ¤í‚µ")
        df = df.drop(columns=["title_clean", "desc_clean", "pub_dt_ordinal"])
        return df

    # Jaccardìš© í† í° ì…‹
    df["title_tokset"] = df["title_clean"].map(lambda x: set(tokenize_simple(x)))
    df["desc_tokset"] = df["desc_clean"].map(lambda x: set(tokenize_simple(x)))

    # í´ëŸ¬ìŠ¤í„°ë§ ê·¸ë£¹ í‚¤ ìƒì„±
    # group_our_brands=True: OUR ë¸Œëœë“œ ê¸°ì‚¬ëŠ” query ë¬´ê´€í•˜ê²Œ í•˜ë‚˜ì˜ í’€ë¡œ ë¬¶ìŒ
    # COMPETITOR ê¸°ì‚¬ëŠ” í•­ìƒ queryë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§
    if group_our_brands and "group" in df.columns:
        df["_cluster_group_key"] = df.apply(
            lambda r: "OUR" if r.get("group") == "OUR" else r.get("query", ""),
            axis=1,
        )
        print(f"  - OUR ë¸Œëœë“œ í†µí•© í´ëŸ¬ìŠ¤í„°ë§ í™œì„±í™” (group_our_brands=True)")
    else:
        df["_cluster_group_key"] = df["query"]

    # ê·¸ë£¹ë³„ í´ëŸ¬ìŠ¤í„°ë§ (cumulative global counter)
    total_press_release = 0
    total_clusters = 0
    cluster_id_counter = max_cluster_num + 1  # ì „ì²´ global counter (ê¸°ì¡´ ìµœëŒ€ê°’ + 1ë¶€í„° ì‹œì‘)

    group_items = list(df.groupby("_cluster_group_key"))
    total_groups = len(group_items)

    for group_idx, (group_key, group_df) in enumerate(group_items, 1):
        print(f"  [{group_idx}/{total_groups}] ê·¸ë£¹ '{group_key}' ({len(group_df)}ê°œ ê¸°ì‚¬) ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...", end="")
        # cluster_id_counterëŠ” ê·¸ë£¹ ê°„ì—ë„ ê³„ì† ì¦ê°€ (cumulative)
        idxs = group_df.index.tolist()
        m = len(idxs)

        # ë‹¨ë… ê¸°ì‚¬
        if m == 1:
            print(f" ë‹¨ë… ê¸°ì‚¬, ìŠ¤í‚µ")
            continue

        try:
            # TF-IDF ë²¡í„°í™” (Title/Description ë³„ë„)
            vec_title = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)
            vec_desc = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)

            X_title = vec_title.fit_transform(df.loc[idxs, "title_clean"])
            X_desc = vec_desc.fit_transform(df.loc[idxs, "desc_clean"])

            # Cosine similarity matrices
            sim_t = cosine_similarity(X_title)
            sim_d = cosine_similarity(X_desc)

            # ë‚ ì§œ ì°¨ì´(ì¼) í–‰ë ¬
            ordinals = df.loc[idxs, "pub_dt_ordinal"].to_numpy()
            dd = np.abs(ordinals[:, None] - ordinals[None, :])  # NaN í¬í•¨ ê°€ëŠ¥

            close_mask = (dd <= day_window)  # Î”days â‰¤ 3
            far_mask = (dd >= strong_start_day)  # Î”days â‰¥ 4

            # Adjacency list
            adj = [set() for _ in range(m)]

            # í† í° ì…‹ ìºì‹œ
            title_sets = df.loc[idxs, "title_tokset"].tolist()
            desc_sets = df.loc[idxs, "desc_tokset"].tolist()

            # (1) Î”days â‰¤ 3: Title ê·œì¹™
            cand_title = np.triu((sim_t >= thr_title_cos) & close_mask, k=1)
            ti, tj = np.where(cand_title)
            title_pairs = len(ti)
            for i, j in zip(ti, tj):
                if jaccard(title_sets[i], title_sets[j]) >= thr_title_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (2) Î”days â‰¤ 3: Description ê·œì¹™
            cand_desc = np.triu((sim_d >= thr_desc_cos) & close_mask, k=1)
            di, dj = np.where(cand_desc)
            desc_pairs = len(di)
            for i, j in zip(di, dj):
                if jaccard(desc_sets[i], desc_sets[j]) >= thr_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (3) Î”days â‰¥ 4: ì´ˆê°•ìœ ì‚¬ ê·œì¹™
            cand_strong = np.triu((sim_d >= strong_desc_cos) & far_mask, k=1)
            si, sj = np.where(cand_strong)
            strong_pairs = len(si)
            for i, j in zip(si, sj):
                if jaccard(desc_sets[i], desc_sets[j]) >= strong_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            print(f" í›„ë³´ìŒ: title={title_pairs}, desc={desc_pairs}, strong={strong_pairs}", end="")

            # (4) LLM ê²½ê³„ì„  ê·œì¹™ (enable_llm_borderline=Trueì¼ ë•Œë§Œ)
            if enable_llm_borderline and openai_key:
                from src.modules.analysis.source_verifier import llm_judge_component_representative

                # borderline ì„¤ì • (config/thresholds.yaml â†’ press_release.borderline)
                borderline_title_cos_low = _pr_bl.get("title_cos_low", 0.62)
                borderline_title_jac_min = _pr_bl.get("title_jac_min", 0.25)
                borderline_desc_cos_low  = _pr_bl.get("desc_cos_low",  0.52)
                borderline_desc_jac_min  = _pr_bl.get("desc_jac_min",  0.18)
                borderline_top_k = 3
                borderline_min_component_size = 3

                # 1) ê²½ê³„ì„  í›„ë³´ ìŒ ìˆ˜ì§‘ (pair-level LLM ì œê±°)
                borderline_pair_scores = {}
                title_borderline_hits = 0
                desc_borderline_hits = 0

                borderline_title = np.triu(
                    (sim_t >= borderline_title_cos_low) & (sim_t < thr_title_cos) & close_mask, k=1
                )
                bti, btj = np.where(borderline_title)
                for i, j in zip(bti, btj):
                    if j in adj[i]:
                        continue
                    jac = jaccard(title_sets[i], title_sets[j])
                    if jac < borderline_title_jac_min:
                        continue
                    score = 0.7 * float(sim_t[i, j]) + 0.3 * float(jac)
                    key = (i, j) if i < j else (j, i)
                    prev = borderline_pair_scores.get(key)
                    if prev is None or score > prev:
                        borderline_pair_scores[key] = score
                    title_borderline_hits += 1

                borderline_desc = np.triu(
                    (sim_d >= borderline_desc_cos_low) & (sim_d < thr_desc_cos) & close_mask, k=1
                )
                bdi, bdj = np.where(borderline_desc)
                for i, j in zip(bdi, bdj):
                    if j in adj[i]:
                        continue
                    jac = jaccard(desc_sets[i], desc_sets[j])
                    if jac < borderline_desc_jac_min:
                        continue
                    score = 0.7 * float(sim_d[i, j]) + 0.3 * float(jac)
                    key = (i, j) if i < j else (j, i)
                    prev = borderline_pair_scores.get(key)
                    if prev is None or score > prev:
                        borderline_pair_scores[key] = score
                    desc_borderline_hits += 1

                print(f", LLMê²½ê³„ì„  í›„ë³´: title={title_borderline_hits}, desc={desc_borderline_hits}", end="")

                # 2) top-k (per article) ê²½ê³„ì„  ì—£ì§€ ìœ ì§€
                selected_edges = set()
                if borderline_pair_scores:
                    per_node = {k: [] for k in range(m)}
                    for (i, j), score in borderline_pair_scores.items():
                        per_node[i].append((score, j))
                        per_node[j].append((score, i))

                    for node, score_list in per_node.items():
                        if not score_list:
                            continue
                        score_list.sort(key=lambda x: x[0], reverse=True)
                        for score, other in score_list[:borderline_top_k]:
                            a, b = (node, other) if node < other else (other, node)
                            selected_edges.add((a, b))

                # 3) ê²½ê³„ì„  ê·¸ë˜í”„ ì»´í¬ë„ŒíŠ¸ êµ¬ì„±
                llm_call_count = 0
                llm_approved_components = 0
                llm_connected = 0

                if selected_edges:
                    borderline_adj = {k: [] for k in range(m)}
                    for i, j in selected_edges:
                        borderline_adj[i].append(j)
                        borderline_adj[j].append(i)

                    border_components = []
                    border_visited = set()
                    for start in range(m):
                        if start in border_visited or len(borderline_adj[start]) == 0:
                            continue
                        comp = []
                        queue = deque([start])
                        while queue:
                            node = queue.popleft()
                            if node in border_visited:
                                continue
                            border_visited.add(node)
                            comp.append(node)
                            for neighbor in borderline_adj[node]:
                                if neighbor not in border_visited:
                                    queue.append(neighbor)
                        if len(comp) >= 2:
                            border_components.append(comp)

                    target_components = [c for c in border_components if len(c) >= borderline_min_component_size]
                    comp_pbar = tqdm(
                        total=len(target_components),
                        desc=f"    [{group_idx}/{total_groups}] LLM ê²½ê³„ì„  ì»´í¬ë„ŒíŠ¸",
                        unit="ì»´í¬ë„ŒíŠ¸",
                        leave=False,
                    )

                    def rep_sort_key(local_idx: int):
                        ord_val = ordinals[local_idx]
                        has_date = 0 if not np.isnan(ord_val) else 1
                        ord_key = ord_val if not np.isnan(ord_val) else float("inf")
                        gidx = idxs[local_idx]
                        info_len = len(str(df.at[gidx, "title"])) + len(str(df.at[gidx, "description"]))
                        return (has_date, ord_key, -info_len)

                    for comp in target_components:
                        rep_local_idx = min(comp, key=rep_sort_key)
                        rep_global_idx = idxs[rep_local_idx]
                        llm_call_count += 1

                        should_connect = llm_judge_component_representative(
                            {
                                "query": str(df.at[rep_global_idx, "query"]) if "query" in df.columns else "",
                                "group": str(df.at[rep_global_idx, "group"]) if "group" in df.columns else "",
                                "title": str(df.at[rep_global_idx, "title"]) if "title" in df.columns else "",
                                "description": str(df.at[rep_global_idx, "description"]) if "description" in df.columns else "",
                            },
                            openai_key=openai_key,
                            mode="press_release_borderline",
                        )

                        if should_connect:
                            llm_approved_components += 1
                            comp_size = len(comp)
                            for a_idx in range(comp_size):
                                for b_idx in range(a_idx + 1, comp_size):
                                    i, j = comp[a_idx], comp[b_idx]
                                    if j not in adj[i]:
                                        adj[i].add(j)
                                        adj[j].add(i)
                                        llm_connected += 1

                        comp_pbar.update(1)

                    comp_pbar.close()
                    print(
                        f", ì»´í¬ë„ŒíŠ¸={len(border_components)}ê°œ(ëŒ€ìƒ {len(target_components)}ê°œ), "
                        f"LLM {llm_call_count}íšŒ, ìŠ¹ì¸ {llm_approved_components}ê°œ, "
                        f"ì—°ê²° {llm_connected}ìŒ"
                    )
                else:
                    print(", LLMê²½ê³„ì„  í›„ë³´ ì—†ìŒ")

            # Connected components (BFS)
            visited = [False] * m
            for i in range(m):
                if visited[i]:
                    continue

                queue = deque([i])
                visited[i] = True
                comp = [i]

                while queue:
                    u = queue.popleft()
                    for v in adj[u]:
                        if not visited[v]:
                            visited[v] = True
                            queue.append(v)
                            comp.append(v)

                # 2ê°œ ì´ìƒì¸ í´ëŸ¬ìŠ¤í„°ë§Œ ì²˜ë¦¬
                if len(comp) < 2:
                    continue

                members_global = [idxs[k] for k in comp]
                # cluster_id prefix: OUR í†µí•© ì‹œ ì²« ë©¤ë²„ì˜ query ì‚¬ìš©, ì•„ë‹ˆë©´ group_key
                if group_our_brands and group_key == "OUR":
                    first_query = df.at[members_global[0], "query"]
                    cid_prefix = str(first_query).split("|")[0]
                else:
                    cid_prefix = group_key
                cid = f"{cid_prefix}_c{cluster_id_counter:05d}"
                cluster_id_counter += 1
                total_clusters += 1

                for gidx in members_global:
                    df.at[gidx, "source"] = "ë³´ë„ìë£Œ"
                    df.at[gidx, "cluster_id"] = cid
                    total_press_release += 1

            # ì¤„ë°”ê¿ˆ (LLM ê²½ê³„ì„ ì´ ì´ë¯¸ ì¤„ë°”ê¿ˆì„ í–ˆìœ¼ë©´ ìŠ¤í‚µ)
            if not (enable_llm_borderline and openai_key):
                print("")  # ì¤„ë°”ê¿ˆ

        except Exception as e:
            print(f"\n  '{group_key}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    drop_cols = ["title_clean", "desc_clean", "pub_dt_ordinal", "title_tokset", "desc_tokset", "_cluster_group_key"]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns])

    unique_count = len(df) - total_press_release
    print(f"âœ… {total_press_release}ê°œ ê¸°ì‚¬ë¥¼ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ ({total_clusters}ê°œ í´ëŸ¬ìŠ¤í„°), {unique_count}ê°œ ê¸°ì‚¬ëŠ” ë…ë¦½ ê¸°ì‚¬")

    return df


def _call_openai_summarize_batch(
    articles: List[Dict],
    openai_key: str,
    max_retries: int = 5,
) -> Dict[str, str]:
    """OpenAI APIë¡œ ê¸°ì‚¬ ë°°ì¹˜ ìš”ì•½"""
    articles_text = "\n".join([
        f"[{a['cluster_id']}]\nì œëª©: {a['title']}\nì„¤ëª…: {a['description'][:200]}"
        for a in articles
    ])

    prompt = f"""ê° ê¸°ì‚¬ë¥¼ 5ë‹¨ì–´ ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ê¸°ì‚¬:
{articles_text}

IMPORTANT: JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì‘ë‹µ:
{{"summaries":[{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00001","summary":"ì‹ ë¼í˜¸í…” ê°œì¥"}},{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00002","summary":"í˜¸í…” ë¦¬ëª¨ë¸ë§"}}]}}

ê·œì¹™:
- ë°˜ë“œì‹œ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
- ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì²´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ
- 5ë‹¨ì–´ ì´ë‚´ í•œêµ­ì–´ ìš”ì•½
- ëª…ì‚¬í˜• ìœ„ì£¼
- ì½”ë“œ ë¸”ë¡(```)ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ ê¸ˆì§€"""

    # api_models.yamlì—ì„œ ëª¨ë¸ ë¡œë“œ
    api_models = load_api_models()
    model = api_models.get("press_release_summary", "gpt-5-nano")

    request_id = uuid.uuid4().hex[:8]

    headers = {"Authorization": f"Bearer {openai_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "input": [{"role": "user", "content": [{"type": "input_text", "text": prompt}]}],
        "text": {
            "format": {
                "type": "json_schema",
                "name": "press_release_summaries",
                "strict": False,
                "schema": {
                    "type": "object",
                    "required": ["summaries"],
                    "additionalProperties": True,
                    "properties": {
                        "summaries": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["cluster_id", "summary"],
                                "additionalProperties": True,
                                "properties": {
                                    "cluster_id": {"type": "string"},
                                    "summary": {"type": "string"}
                                }
                            }
                        }
                    }
                }
            }
        },
    }

    response = call_openai_with_retry(
        OPENAI_API_URL, headers, payload,
        max_retries=max_retries, request_id=request_id, label="ë³´ë„ìë£Œìš”ì•½"
    )

    if response is None:
        return {a["cluster_id"]: a["title"][:15] for a in articles}

    result = response.json()
    try:
        summaries = _parse_summaries_from_result(result)
        return {
            item["cluster_id"]: item["summary"]
            for item in summaries
            if isinstance(item, dict) and "cluster_id" in item and "summary" in item
        }
    except (ValueError, json.JSONDecodeError, KeyError) as e:
        notify_error(
            "OpenAI ë³´ë„ìë£Œ ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨",
            {"request_id": request_id, "error": f"{type(e).__name__}: {e}"}
        )
        return {a["cluster_id"]: a["title"][:15] for a in articles}


def summarize_clusters(
    df: pd.DataFrame,
    openai_key: str
) -> pd.DataFrame:
    """
    cluster_idê°€ ìˆëŠ” ëª¨ë“  í´ëŸ¬ìŠ¤í„°(ë³´ë„ìë£Œ+ìœ ì‚¬ì£¼ì œ)ë¥¼ OpenAIë¡œ ìš”ì•½.
    ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ìŠ¤í‚µ.

    Args:
        df: cluster_id, pub_datetime, title, description ì»¬ëŸ¼ í¬í•¨
        openai_key: OpenAI API í‚¤

    Returns:
        cluster_summary ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ“ í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìƒì„± ì¤‘...")
    df = df.copy()

    # cluster_summary ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (ê¸°ì¡´ ê°’ ìœ ì§€)
    if "cluster_summary" not in df.columns:
        df["cluster_summary"] = ""

    # cluster_idê°€ ìˆëŠ” ëª¨ë“  ê¸°ì‚¬ ëŒ€ìƒ (source ì¡°ê±´ ì—†ìŒ)
    cluster_mask = df["cluster_id"].astype(str).str.strip() != ""
    if cluster_mask.sum() == 0:
        print("  â„¹ï¸  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df

    try:
        # ê·¸ë£¹ë³„ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ ì„ íƒ
        cl_df = df[cluster_mask].copy()
        cl_df["pub_datetime_parsed"] = pd.to_datetime(cl_df["pub_datetime"], errors="coerce")
        earliest_articles = cl_df.sort_values("pub_datetime_parsed").groupby("cluster_id").first().reset_index()

        # ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ê¸°ì¡´ ê°’ì„ ì „ì²´ ë©¤ë²„ì— ì „íŒŒí•˜ê³ , OpenAI í˜¸ì¶œ ëŒ€ìƒì—ì„œ ì œì™¸
        existing_cluster_ids = set()
        for cluster_id, cluster_rows in cl_df.groupby("cluster_id"):
            existing_values = cluster_rows["cluster_summary"].dropna().astype(str)
            existing_values = existing_values[existing_values.str.strip() != ""]
            if len(existing_values) == 0:
                continue

            existing_cluster_ids.add(cluster_id)
            existing_summary = existing_values.iloc[0]
            c_mask = cluster_mask & (df["cluster_id"] == cluster_id)
            df.loc[c_mask, "cluster_summary"] = existing_summary

        # ìš”ì•½ì´ í•„ìš”í•œ cluster_idë§Œ ì„ íƒ (í´ëŸ¬ìŠ¤í„° ë‚´ ìš”ì•½ì´ ì „í˜€ ì—†ëŠ” ê²½ìš°)
        articles_to_summarize = [
            {"cluster_id": row["cluster_id"], "title": row["title"], "description": row["description"]}
            for _, row in earliest_articles.iterrows()
            if row["cluster_id"] not in existing_cluster_ids
        ]

        if len(articles_to_summarize) == 0:
            print(f"  â„¹ï¸  ëª¨ë“  í´ëŸ¬ìŠ¤í„°({len(existing_cluster_ids)}ê°œ)ê°€ ì´ë¯¸ ìš”ì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return df

        group_summaries = {}
        total_to_summarize = len(articles_to_summarize)
        pbar = tqdm(total=total_to_summarize, desc="í´ëŸ¬ìŠ¤í„° ìš”ì•½", unit="ê·¸ë£¹")
        for i in range(0, total_to_summarize, 20):
            batch = articles_to_summarize[i:i+20]
            batch_summaries = _call_openai_summarize_batch(batch, openai_key)
            group_summaries.update(batch_summaries)
            pbar.update(len(batch))
        pbar.close()

        # cluster_summary ì—…ë°ì´íŠ¸
        for idx, row in df[cluster_mask].iterrows():
            cluster_id = row["cluster_id"]
            if cluster_id in group_summaries:
                df.at[idx, "cluster_summary"] = group_summaries[cluster_id]

        total_groups = len(earliest_articles)
        new_summaries = len(group_summaries)
        existing_count = len(existing_cluster_ids)

        print(f"âœ… í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì™„ë£Œ:")
        print(f"   - ì „ì²´ í´ëŸ¬ìŠ¤í„°: {total_groups}ê°œ")
        print(f"   - ìƒˆë¡œ ìš”ì•½: {new_summaries}ê°œ (LLM í˜¸ì¶œ)")
        print(f"   - ê¸°ì¡´ ìœ ì§€: {existing_count}ê°œ (ìŠ¤í‚µ)")
        return df

    except Exception as e:
        print(f"  í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        return df
