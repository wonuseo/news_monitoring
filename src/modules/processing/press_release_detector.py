"""
press_release_detector.py - Press Release Detection & Summarization
ë³´ë„ìë£Œ ê²€ì¶œ, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½ (TF-IDF + Cosine Similarity + OpenAI)
"""

import re
import json
import uuid
import pandas as pd
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm

from src.utils.openai_client import (
    OPENAI_API_URL,
    set_error_callback,
    load_api_models,
    call_openai_with_retry,
    extract_response_text,
    notify_error,
)


def _trim_json_object(payload: str) -> str:
    # Remove wrapping markdown fences, keep inner braces/brackets
    cleaned = re.sub(r"```(?:json)?\s*", "", payload)
    cleaned = cleaned.replace("`", "")
    start = cleaned.find("{")
    end = cleaned.rfind("}")
    if start != -1 and end != -1 and start < end:
        json_snippet = cleaned[start:end + 1]
        trailing = cleaned[end + 1:]
        trailing_match = re.search(r"\]", trailing)
        if trailing_match:
            json_snippet += trailing[: trailing_match.end()]
        return json_snippet
    return cleaned


def _repair_json_text(payload: str) -> str:
    cleaned = payload.strip()
    cleaned = re.sub(r",\s*([\]\}])", r"\1", cleaned)
    return cleaned


def _parse_summaries_from_result(result: Dict) -> List[Dict]:
    text = extract_response_text(result)
    if not text:
        print("  JSON íŒŒì‹± ì‹¤íŒ¨: API ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        print(f"   ì „ì²´ ì‘ë‹µ êµ¬ì¡°: {json.dumps(result, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses API ë°˜í™˜ bodyì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    trimmed = _trim_json_object(text)

    last_exc: Exception | None = None
    parsed = None
    candidates = [trimmed, _repair_json_text(trimmed)]

    for i, candidate in enumerate(candidates, 1):
        try:
            parsed = json.loads(candidate)
            break
        except json.JSONDecodeError as exc:
            last_exc = exc
            print(f"  JSON íŒŒì‹± ì‹œë„ {i} ì‹¤íŒ¨: {exc}")
            print(f"   ì‹œë„í•œ í…ìŠ¤íŠ¸ (ì• 300ì): {candidate[:300]}")

    if parsed is None:
        print("  ëª¨ë“  JSON íŒŒì‹± ì‹œë„ ì‹¤íŒ¨")
        print(f"   ì›ë³¸ í…ìŠ¤íŠ¸ (ì• 500ì): {text[:500]}")
        print(f"   ì •ì œëœ í…ìŠ¤íŠ¸ (ì• 500ì): {trimmed[:500]}")
        raise last_exc or ValueError("Responses API JSONì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if isinstance(parsed, dict):
        summaries = parsed.get("summaries")
    elif isinstance(parsed, list):
        summaries = parsed
    else:
        print(f"  ì˜ˆìƒí•˜ì§€ ëª»í•œ JSON êµ¬ì¡°: {type(parsed)}")
        print(f"   íŒŒì‹±ëœ ê°ì²´: {json.dumps(parsed, indent=2, ensure_ascii=False)[:500]}")
        raise ValueError("Responses APIê°€ ìš”ì•½ ëª©ë¡ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not isinstance(summaries, list):
        print(f"  summariesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(summaries)}")
        print(f"   summaries ê°’: {summaries}")
        raise ValueError("Responses APIê°€ summaries ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return summaries


def detect_similar_articles(
    df: pd.DataFrame,
    spreadsheet=None,
    day_window: int = 3,
    strong_start_day: int = 4,
    thr_title_cos: float = 0.70,
    thr_title_jac: float = 0.18,
    thr_desc_cos: float = 0.60,
    thr_desc_jac: float = 0.10,
    strong_desc_cos: float = 0.85,
    strong_desc_jac: float = 0.35,
    min_token_len: int = 2
) -> pd.DataFrame:
    """
    ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ë¡œ ë³´ë„ìë£Œ ì‹ë³„
    - ë‚ ì§œ ê¸°ë°˜ ì°¨ë“± ì„ê³„ê°’ ì ìš© (Î”days â‰¤ 3 vs Î”days â‰¥ 4)
    - TF-IDF Cosine + Jaccard ì´ì¤‘ ìœ ì‚¬ë„ ì¸¡ì •
    - Title/Description ë³„ë„ ì²˜ë¦¬
    - Queryë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§
    - BFSë¡œ Connected Components ê·¸ë£¹í™”
    - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€ (ë¹„íŒŒê´´ì  ë¼ë²¨ë§ë§Œ ìˆ˜í–‰)
    - Cumulative cluster numbering (Google Sheets ê¸°ë°˜)

    Args:
        df: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°ëœ DataFrame (query, pub_datetime ì»¬ëŸ¼ í•„ìˆ˜)
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­, cumulative numberingìš©)
        day_window: ê¸°ë³¸ ê·œì¹™ ì ìš© ë‚ ì§œ ë²”ìœ„ (ê¸°ë³¸ê°’: 3ì¼)
        strong_start_day: ì´ˆê°•ìœ ì‚¬ ê·œì¹™ ì‹œì‘ ë‚ ì§œ (ê¸°ë³¸ê°’: 4ì¼)
        thr_title_cos: Title Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.70)
        thr_title_jac: Title Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.18)
        thr_desc_cos: Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.60)
        thr_desc_jac: Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.10)
        strong_desc_cos: ì´ˆê°•ìœ ì‚¬ Description Cosine ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.85)
        strong_desc_jac: ì´ˆê°•ìœ ì‚¬ Description Jaccard ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.35)
        min_token_len: í† í° ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ê°’: 2)

    Returns:
        'source', 'cluster_id' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
    print(f"  - ê¸°ë³¸ ê·œì¹™(Î”daysâ‰¤{day_window}): title {thr_title_cos}/{thr_title_jac}, desc {thr_desc_cos}/{thr_desc_jac}")
    print(f"  - ì´ˆê°•ìœ ì‚¬(Î”daysâ‰¥{strong_start_day}): desc {strong_desc_cos}/{strong_desc_jac}")

    df = df.copy()

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["source"] = "ì¼ë°˜ê¸°ì‚¬"
    df["cluster_id"] = ""

    # Cumulative cluster numbering (Google Sheetsì—ì„œ ê¸°ì¡´ ìµœëŒ€ê°’ ê°€ì ¸ì˜¤ê¸°)
    max_cluster_num = 0
    if spreadsheet:
        from src.utils.sheets_helpers import get_max_values_from_sheets
        max_values = get_max_values_from_sheets(spreadsheet)
        max_cluster_num = max_values["max_cluster_num"]
        if max_cluster_num > 0:
            print(f"  ğŸ“Š ê¸°ì¡´ ìµœëŒ€ cluster_id ë²ˆí˜¸: {max_cluster_num} (cumulative numbering)")

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        from collections import deque
    except ImportError as e:
        print(f"  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ({e}). ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if "query" not in df.columns:
        print("  'query' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ìœ í‹¸ í•¨ìˆ˜ë“¤
    def clean_html(s: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r"<[^>]+>", " ", s)
        s = s.replace("&quot;", " ").replace("&amp;", "&")
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def tokenize_simple(s: str):
        """ê°„ë‹¨ í† í°í™” (í•œê¸€/ì˜ë¬¸/ìˆ«ì ë©ì–´ë¦¬)"""
        if not isinstance(s, str) or not s.strip():
            return []
        s = s.lower()
        toks = re.findall(r"[ê°€-í£a-z0-9]+", s)
        return [t for t in toks if len(t) >= min_token_len]

    def jaccard(a: set, b: set) -> float:
        """Jaccard ìœ ì‚¬ë„"""
        if not a and not b:
            return 0.0
        inter = len(a & b)
        union = len(a | b)
        return inter / union if union else 0.0

    def to_ordinal_safe(ts):
        """datetime â†’ ordinal, NaTë©´ np.nan"""
        if pd.isna(ts):
            return np.nan
        try:
            return pd.to_datetime(ts).to_pydatetime().date().toordinal()
        except:
            return np.nan

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    df["title_clean"] = df["title"].fillna("").map(clean_html)
    df["desc_clean"] = df["description"].fillna("").map(clean_html)

    # ë‚ ì§œ ë³€í™˜ (pub_datetime â†’ ordinal)
    df["pub_dt_ordinal"] = df["pub_datetime"].map(to_ordinal_safe)

    # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
    valid_mask = (df["title_clean"].str.len() > 0) | (df["desc_clean"].str.len() > 0)
    if valid_mask.sum() < 2:
        print(f"  ìœ íš¨í•œ ê¸°ì‚¬ê°€ {valid_mask.sum()}ê°œë¡œ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ ìŠ¤í‚µ")
        df = df.drop(columns=["title_clean", "desc_clean", "pub_dt_ordinal"])
        return df

    # Jaccardìš© í† í° ì…‹
    df["title_tokset"] = df["title_clean"].map(lambda x: set(tokenize_simple(x)))
    df["desc_tokset"] = df["desc_clean"].map(lambda x: set(tokenize_simple(x)))

    # Queryë³„ í´ëŸ¬ìŠ¤í„°ë§ (cumulative global counter)
    total_press_release = 0
    total_clusters = 0
    cluster_id_counter = max_cluster_num + 1  # ì „ì²´ global counter (ê¸°ì¡´ ìµœëŒ€ê°’ + 1ë¶€í„° ì‹œì‘)

    for query, group_df in df.groupby("query"):
        # cluster_id_counterëŠ” query ê°„ì—ë„ ê³„ì† ì¦ê°€ (cumulative)
        idxs = group_df.index.tolist()
        m = len(idxs)

        # ë‹¨ë… ê¸°ì‚¬
        if m == 1:
            continue

        try:
            # TF-IDF ë²¡í„°í™” (Title/Description ë³„ë„)
            vec_title = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)
            vec_desc = TfidfVectorizer(analyzer="char", ngram_range=(3, 5), min_df=2)

            X_title = vec_title.fit_transform(df.loc[idxs, "title_clean"])
            X_desc = vec_desc.fit_transform(df.loc[idxs, "desc_clean"])

            # Cosine similarity matrices
            sim_t = cosine_similarity(X_title)
            sim_d = cosine_similarity(X_desc)

            # ë‚ ì§œ ì°¨ì´(ì¼) í–‰ë ¬
            ordinals = df.loc[idxs, "pub_dt_ordinal"].to_numpy()
            dd = np.abs(ordinals[:, None] - ordinals[None, :])  # NaN í¬í•¨ ê°€ëŠ¥

            close_mask = (dd <= day_window)  # Î”days â‰¤ 3
            far_mask = (dd >= strong_start_day)  # Î”days â‰¥ 4

            # Adjacency list
            adj = [set() for _ in range(m)]

            # í† í° ì…‹ ìºì‹œ
            title_sets = df.loc[idxs, "title_tokset"].tolist()
            desc_sets = df.loc[idxs, "desc_tokset"].tolist()

            # (1) Î”days â‰¤ 3: Title ê·œì¹™
            cand_title = np.triu((sim_t >= thr_title_cos) & close_mask, k=1)
            ti, tj = np.where(cand_title)
            for i, j in zip(ti, tj):
                if jaccard(title_sets[i], title_sets[j]) >= thr_title_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (2) Î”days â‰¤ 3: Description ê·œì¹™
            cand_desc = np.triu((sim_d >= thr_desc_cos) & close_mask, k=1)
            di, dj = np.where(cand_desc)
            for i, j in zip(di, dj):
                if jaccard(desc_sets[i], desc_sets[j]) >= thr_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # (3) Î”days â‰¥ 4: ì´ˆê°•ìœ ì‚¬ ê·œì¹™
            cand_strong = np.triu((sim_d >= strong_desc_cos) & far_mask, k=1)
            si, sj = np.where(cand_strong)
            for i, j in zip(si, sj):
                if jaccard(desc_sets[i], desc_sets[j]) >= strong_desc_jac:
                    adj[i].add(j)
                    adj[j].add(i)

            # Connected components (BFS)
            visited = [False] * m
            for i in range(m):
                if visited[i]:
                    continue

                queue = deque([i])
                visited[i] = True
                comp = [i]

                while queue:
                    u = queue.popleft()
                    for v in adj[u]:
                        if not visited[v]:
                            visited[v] = True
                            queue.append(v)
                            comp.append(v)

                # 2ê°œ ì´ìƒì¸ í´ëŸ¬ìŠ¤í„°ë§Œ ì²˜ë¦¬
                if len(comp) < 2:
                    continue

                members_global = [idxs[k] for k in comp]
                cid = f"{query}_c{cluster_id_counter:05d}"
                cluster_id_counter += 1
                total_clusters += 1

                for gidx in members_global:
                    df.at[gidx, "source"] = "ë³´ë„ìë£Œ"
                    df.at[gidx, "cluster_id"] = cid
                    total_press_release += 1

        except Exception as e:
            print(f"  '{query}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["title_clean", "desc_clean", "pub_dt_ordinal", "title_tokset", "desc_tokset"])

    unique_count = len(df) - total_press_release
    print(f"âœ… {total_press_release}ê°œ ê¸°ì‚¬ë¥¼ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ ({total_clusters}ê°œ í´ëŸ¬ìŠ¤í„°), {unique_count}ê°œ ê¸°ì‚¬ëŠ” ë…ë¦½ ê¸°ì‚¬")

    return df


def _call_openai_summarize_batch(
    articles: List[Dict],
    openai_key: str,
    max_retries: int = 5,
) -> Dict[str, str]:
    """OpenAI APIë¡œ ê¸°ì‚¬ ë°°ì¹˜ ìš”ì•½"""
    articles_text = "\n".join([
        f"[{a['cluster_id']}]\nì œëª©: {a['title']}\nì„¤ëª…: {a['description'][:200]}"
        for a in articles
    ])

    prompt = f"""ê° ê¸°ì‚¬ë¥¼ 5ë‹¨ì–´ ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ê¸°ì‚¬:
{articles_text}

IMPORTANT: JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSONë§Œ ì‘ë‹µ:
{{"summaries":[{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00001","summary":"ì‹ ë¼í˜¸í…” ê°œì¥"}},{{"cluster_id":"ë¡¯ë°í˜¸í…”_c00002","summary":"í˜¸í…” ë¦¬ëª¨ë¸ë§"}}]}}

ê·œì¹™:
- ë°˜ë“œì‹œ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
- ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì²´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ
- 5ë‹¨ì–´ ì´ë‚´ í•œêµ­ì–´ ìš”ì•½
- ëª…ì‚¬í˜• ìœ„ì£¼
- ì½”ë“œ ë¸”ë¡(```)ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ ê¸ˆì§€"""

    # api_models.yamlì—ì„œ ëª¨ë¸ ë¡œë“œ
    api_models = load_api_models()
    model = api_models.get("press_release_summary", "gpt-5-nano")

    request_id = uuid.uuid4().hex[:8]

    headers = {"Authorization": f"Bearer {openai_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "input": [{"role": "user", "content": [{"type": "input_text", "text": prompt}]}],
        "text": {
            "format": {
                "type": "json_schema",
                "name": "press_release_summaries",
                "strict": False,
                "schema": {
                    "type": "object",
                    "required": ["summaries"],
                    "additionalProperties": True,
                    "properties": {
                        "summaries": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "required": ["cluster_id", "summary"],
                                "additionalProperties": True,
                                "properties": {
                                    "cluster_id": {"type": "string"},
                                    "summary": {"type": "string"}
                                }
                            }
                        }
                    }
                }
            }
        },
    }

    response = call_openai_with_retry(
        OPENAI_API_URL, headers, payload,
        max_retries=max_retries, request_id=request_id, label="ë³´ë„ìë£Œìš”ì•½"
    )

    if response is None:
        return {a["cluster_id"]: a["title"][:15] for a in articles}

    result = response.json()
    try:
        summaries = _parse_summaries_from_result(result)
        return {
            item["cluster_id"]: item["summary"]
            for item in summaries
            if isinstance(item, dict) and "cluster_id" in item and "summary" in item
        }
    except (ValueError, json.JSONDecodeError, KeyError) as e:
        notify_error(
            "OpenAI ë³´ë„ìë£Œ ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨",
            {"request_id": request_id, "error": f"{type(e).__name__}: {e}"}
        )
        return {a["cluster_id"]: a["title"][:15] for a in articles}


def summarize_press_release_groups(
    df: pd.DataFrame,
    openai_key: str
) -> pd.DataFrame:
    """
    ë³´ë„ìë£Œ ê·¸ë£¹ë³„ë¡œ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ë¥¼ OpenAIë¡œ ìš”ì•½
    ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ” ê·¸ë£¹ì€ ìŠ¤í‚µ

    Args:
        df: cluster_id, pub_datetime, title, description ì»¬ëŸ¼ í¬í•¨
        openai_key: OpenAI API í‚¤

    Returns:
        press_release_group ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ“ ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ìƒì„± ì¤‘...")
    df = df.copy()

    # press_release_group ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (ê¸°ì¡´ ê°’ ìœ ì§€)
    if "press_release_group" not in df.columns:
        df["press_release_group"] = ""

    press_release_mask = (df["source"] == "ë³´ë„ìë£Œ") & (df["cluster_id"] != "")
    if press_release_mask.sum() == 0:
        print("  â„¹ï¸  ë³´ë„ìë£Œ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df

    try:
        # ê·¸ë£¹ë³„ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ ì„ íƒ
        pr_df = df[press_release_mask].copy()
        pr_df["pub_datetime_parsed"] = pd.to_datetime(pr_df["pub_datetime"], errors="coerce")
        earliest_articles = pr_df.sort_values("pub_datetime_parsed").groupby("cluster_id").first().reset_index()

        # ì´ë¯¸ ìš”ì•½ì´ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ê¸°ì¡´ ê°’ì„ ì „ì²´ ë©¤ë²„ì— ì „íŒŒí•˜ê³ , OpenAI í˜¸ì¶œ ëŒ€ìƒì—ì„œ ì œì™¸
        existing_cluster_ids = set()
        for cluster_id, cluster_rows in pr_df.groupby("cluster_id"):
            existing_values = cluster_rows["press_release_group"].dropna().astype(str)
            existing_values = existing_values[existing_values.str.strip() != ""]
            if len(existing_values) == 0:
                continue

            existing_cluster_ids.add(cluster_id)
            existing_summary = existing_values.iloc[0]
            cluster_mask = press_release_mask & (df["cluster_id"] == cluster_id)
            df.loc[cluster_mask, "press_release_group"] = existing_summary

        # ìš”ì•½ì´ í•„ìš”í•œ cluster_idë§Œ ì„ íƒ (í´ëŸ¬ìŠ¤í„° ë‚´ ìš”ì•½ì´ ì „í˜€ ì—†ëŠ” ê²½ìš°)
        articles_to_summarize = [
            {"cluster_id": row["cluster_id"], "title": row["title"], "description": row["description"]}
            for _, row in earliest_articles.iterrows()
            if row["cluster_id"] not in existing_cluster_ids
        ]

        if len(articles_to_summarize) == 0:
            print(f"  â„¹ï¸  ëª¨ë“  ë³´ë„ìë£Œ ê·¸ë£¹({len(existing_cluster_ids)}ê°œ)ì´ ì´ë¯¸ ìš”ì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return df

        group_summaries = {}
        total_to_summarize = len(articles_to_summarize)
        pbar = tqdm(total=total_to_summarize, desc="ë³´ë„ìë£Œ ìš”ì•½", unit="ê·¸ë£¹")
        for i in range(0, total_to_summarize, 20):
            batch = articles_to_summarize[i:i+20]
            batch_summaries = _call_openai_summarize_batch(batch, openai_key)
            group_summaries.update(batch_summaries)
            pbar.update(len(batch))
        pbar.close()

        # press_release_group ì—…ë°ì´íŠ¸
        for idx, row in df[press_release_mask].iterrows():
            cluster_id = row["cluster_id"]
            if cluster_id in group_summaries:
                df.at[idx, "press_release_group"] = group_summaries[cluster_id]

        total_groups = len(earliest_articles)
        new_summaries = len(group_summaries)
        existing_count = len(existing_cluster_ids)

        print(f"âœ… ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì™„ë£Œ:")
        print(f"   - ì „ì²´ ê·¸ë£¹: {total_groups}ê°œ")
        print(f"   - ìƒˆë¡œ ìš”ì•½: {new_summaries}ê°œ (LLM í˜¸ì¶œ)")
        print(f"   - ê¸°ì¡´ ìœ ì§€: {existing_count}ê°œ (ìŠ¤í‚µ)")
        return df

    except Exception as e:
        print(f"  ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        return df
