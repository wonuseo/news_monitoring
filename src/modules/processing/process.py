"""
process.py - Data Processing Module
ë°ì´í„° ì •ê·œí™”, ì¤‘ë³µ ì œê±°, Excel ì €ì¥
"""

import re
import json
import time
import requests
from html import unescape
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict
import pandas as pd


def strip_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° ì—”í‹°í‹° ë””ì½”ë”©"""
    if not text:
        return ""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = unescape(text)
    return text.strip()


def parse_pubdate(pubdate_str: str) -> Optional[str]:
    """
    ë„¤ì´ë²„ pubDateë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    ì˜ˆ: 'Mon, 03 Feb 2026 10:30:00 +0900' â†’ '2026-02-03T10:30:00+09:00'
    """
    if not pubdate_str:
        return None
    
    try:
        dt = datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
        return dt.isoformat()
    except Exception as e:
        print(f"âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ '{pubdate_str}': {e}")
        return None


def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë°ì´í„° ì •ê·œí™”
    - HTML íƒœê·¸ ì œê±°
    - ë‚ ì§œ íŒŒì‹±
    """
    print("ğŸ”§ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    df = df.copy()

    # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ strip_html ì ìš©
    df["title"] = df["title"].fillna("").apply(strip_html)
    df["description"] = df["description"].fillna("").apply(strip_html)
    df["pub_datetime"] = df["pubDate"].fillna("").apply(parse_pubdate)
    
    print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ì •ê·œí™” ì™„ë£Œ")
    return df


def dedupe_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì¤‘ë³µ ì œê±°
    - originallink ë˜ëŠ” linkë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    - ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€
    """
    print("ğŸ”§ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")
    df = df.copy()

    # Primary key ìƒì„±
    df["pk"] = df["originallink"].where(df["originallink"].str.strip() != "", df["link"])

    # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹  â†’ ì˜¤ë˜ëœ)
    df["pub_datetime_sort"] = pd.to_datetime(df["pub_datetime"], errors="coerce")
    df = df.sort_values("pub_datetime_sort", ascending=False, na_position="last")

    # ì¤‘ë³µ ì œê±° (ìµœì‹  ê²ƒë§Œ ìœ ì§€)
    original_count = len(df)
    df_deduped = df.drop_duplicates(subset=["pk"], keep="first")

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df_deduped = df_deduped.drop(columns=["pk", "pub_datetime_sort"])

    removed = original_count - len(df_deduped)
    print(f"âœ… ì¤‘ë³µ {removed}ê°œ ì œê±°, {len(df_deduped)}ê°œ ê¸°ì‚¬ ìœ ì§€")
    return df_deduped.reset_index(drop=True)


def enrich_with_media_info(
    df: pd.DataFrame,
    spreadsheet=None,
    openai_key: str = None,
    csv_path: Path = None
) -> pd.DataFrame:
    """
    DataFrameì— ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (wrapper for media_classify.add_media_columns)

    Args:
        df: ì²˜ë¦¬ëœ DataFrame
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­)
        openai_key: OpenAI API í‚¤ (ì„ íƒì‚¬í•­)
        csv_path: media_directory CSV ê²½ë¡œ (ì„ íƒì‚¬í•­)

    Returns:
        ì–¸ë¡ ì‚¬ ì •ë³´ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    try:
        from src.modules.processing.media_classify import add_media_columns
        return add_media_columns(df, spreadsheet, openai_key, csv_path)
    except ImportError:
        print("âš ï¸  media_classify ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return df
    except Exception as e:
        print(f"âš ï¸  ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return df


def detect_similar_articles(
    df: pd.DataFrame,
    similarity_threshold: float = 0.8,
    min_text_length: int = 10
) -> pd.DataFrame:
    """
    ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ë¡œ ë³´ë„ìë£Œ ì‹ë³„
    - TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ê¸°ì‚¬ ê°ì§€
    - ìœ ì‚¬ë„ >= thresholdì¸ ê¸°ì‚¬ë“¤ì„ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ
    - Union-Findë¡œ ê·¸ë£¹í™”
    - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€ (ë¹„íŒŒê´´ì  ë¼ë²¨ë§ë§Œ ìˆ˜í–‰)

    Args:
        df: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°ëœ DataFrame
        similarity_threshold: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.8)
        min_text_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 10ì)

    Returns:
        'source', 'group_id' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
    df = df.copy()

    # ì»¬ëŸ¼ ì´ˆê¸°í™”
    df["source"] = "ì¼ë°˜ê¸°ì‚¬"
    df["group_id"] = ""

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError:
        print("âš ï¸  scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ì œëª©ê³¼ ì„¤ëª…ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ í…ìŠ¤íŠ¸ ìƒì„±
    df["search_text"] = (
        df["title"].fillna("") + " " + df["description"].fillna("")
    ).str.strip()

    # ìµœì†Œ ê¸¸ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê¸°ì‚¬ í•„í„°ë§
    valid_mask = df["search_text"].str.len() >= min_text_length
    valid_indices = df[valid_mask].index.tolist()

    if len(valid_indices) < 2:
        print(f"âœ… ê²€ìƒ‰ ëŒ€ìƒ ê¸°ì‚¬ê°€ {len(valid_indices)}ê°œë¡œ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ ìŠ¤í‚µ")
        df = df.drop(columns=["search_text"])
        return df

    try:
        # TF-IDF ë²¡í„°í™” (í•œê¸€ ìµœì í™”: ë¬¸ì ë‹¨ìœ„ n-gram ì‚¬ìš©)
        print(f"  - ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
        print(f"  - ê²€ìƒ‰ ëŒ€ìƒ: {len(valid_indices)}ê°œ ê¸°ì‚¬")

        vectorizer = TfidfVectorizer(
            analyzer='char',
            ngram_range=(2, 4),
            min_df=2,
            max_df=0.9,
            max_features=5000
        )

        # ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ë²¡í„°í™”
        valid_texts = df.loc[valid_indices, "search_text"].tolist()
        tfidf_matrix = vectorizer.fit_transform(valid_texts)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (í¬ì†Œ í–‰ë ¬ ì‚¬ìš©)
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # Union-Findë¡œ ìœ ì‚¬ ê¸°ì‚¬ ê·¸ë£¹í™”
        parent = {i: i for i in range(len(valid_indices))}

        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[px] = py

        # ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ì„ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°
        for i in range(len(valid_indices)):
            for j in range(i + 1, len(valid_indices)):
                if similarity_matrix[i, j] >= similarity_threshold:
                    union(i, j)

        # ê·¸ë£¹ë³„ ê¸°ì‚¬ ìˆ˜ì§‘
        groups = {}
        for i in range(len(valid_indices)):
            root = find(i)
            if root not in groups:
                groups[root] = []
            groups[root].append(i)

        # source, group_id ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
        # 2ê°œ ì´ìƒì˜ ê¸°ì‚¬ê°€ ìˆëŠ” ê·¸ë£¹ë§Œ í•„í„°ë§í•˜ê³  ì •ë ¬
        sorted_groups = sorted(
            [(gid, indices) for gid, indices in groups.items() if len(indices) > 1],
            key=lambda x: min(x[1])  # ì²« ë²ˆì§¸ ê¸°ì‚¬ì˜ ì¸ë±ìŠ¤ ê¸°ì¤€ ì •ë ¬
        )

        # 1ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ê·¸ë£¹ ID í• ë‹¹
        for new_id, (old_id, indices) in enumerate(sorted_groups, start=1):
            for i in indices:
                idx = valid_indices[i]
                df.at[idx, "source"] = "ë³´ë„ìë£Œ"
                df.at[idx, "group_id"] = f"group_{new_id}"

        similar_count = sum(len(indices) for indices in groups.values() if len(indices) > 1)
        unique_count = len(valid_indices) - similar_count

        print(f"âœ… {similar_count}ê°œ ê¸°ì‚¬ë¥¼ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ ({len(groups)}ê°œ ê·¸ë£¹), {unique_count}ê°œ ê¸°ì‚¬ëŠ” ë…ë¦½ ê¸°ì‚¬")

    except MemoryError:
        print("âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸  ìœ ì‚¬ë„ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["search_text"])
    return df


def save_csv(df: pd.DataFrame, filepath: Path) -> None:
    """CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 BOM ì¸ì½”ë”©)"""
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥: {filepath}")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨ ({filepath}): {e}")
        print("  â†’ ë””ìŠ¤í¬ ê³µê°„ ë˜ëŠ” ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")


def _call_openai_summarize_batch(
    articles: List[Dict],
    openai_key: str,
    retry: bool = True
) -> Dict[str, str]:
    """OpenAI APIë¡œ ê¸°ì‚¬ ë°°ì¹˜ ìš”ì•½"""
    articles_text = "\n".join([
        f"[{a['group_id']}]\nì œëª©: {a['title']}\nì„¤ëª…: {a['description'][:200]}"
        for a in articles
    ])

    prompt = f"""ê° ê¸°ì‚¬ë¥¼ 3ë‹¨ì–´ ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ê¸°ì‚¬:
{articles_text}

JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
[{{"group_id":"group_0","summary":"ìš”ì•½"}},...]

ê·œì¹™:
- 3ë‹¨ì–´ ì´ë‚´
- ëª…ì‚¬í˜• ìœ„ì£¼
- JSONë§Œ ì¶œë ¥"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {openai_key}", "Content-Type": "application/json"},
            json={
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": min(len(articles) * 30, 4096)  # ìµœì†Œ 30í† í°/í•­ëª©, ìµœëŒ€ 4096
            },
            timeout=60
        )

        if response.status_code == 429 and retry:
            print("  (Rate limit, 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„)")
            time.sleep(5)
            return _call_openai_summarize_batch(articles, openai_key, retry=False)

        if response.status_code != 200:
            print(f"  (API ì˜¤ë¥˜ {response.status_code}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
            return {a["group_id"]: a["title"][:15] for a in articles}

        result = response.json()
        content = result.get("choices", [{}])[0].get("message", {}).get("content", "")

        try:
            summaries = json.loads(content)
            return {item["group_id"]: item["summary"] for item in summaries}
        except json.JSONDecodeError:
            if retry:
                print("  (JSON íŒŒì‹± ì‹¤íŒ¨, ì¬ì‹œë„)")
                time.sleep(2)
                return _call_openai_summarize_batch(articles, openai_key, retry=False)
            return {a["group_id"]: a["title"][:15] for a in articles}

    except Exception as e:
        print(f"  (ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©)")
        return {a["group_id"]: a["title"][:15] for a in articles}


def summarize_press_release_groups(
    df: pd.DataFrame,
    openai_key: str
) -> pd.DataFrame:
    """
    ë³´ë„ìë£Œ ê·¸ë£¹ë³„ë¡œ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ë¥¼ OpenAIë¡œ ìš”ì•½

    Args:
        df: group_id, pub_datetime, title, description ì»¬ëŸ¼ í¬í•¨
        openai_key: OpenAI API í‚¤

    Returns:
        press_release_group ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ“ ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ìƒì„± ì¤‘...")
    df = df.copy()
    df["press_release_group"] = ""

    press_release_mask = (df["source"] == "ë³´ë„ìë£Œ") & (df["group_id"] != "")
    if press_release_mask.sum() == 0:
        print("  â„¹ï¸  ë³´ë„ìë£Œ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df

    try:
        # ê·¸ë£¹ë³„ ê°€ì¥ ì´ë¥¸ ê¸°ì‚¬ ì„ íƒ
        pr_df = df[press_release_mask].copy()
        pr_df["pub_datetime_parsed"] = pd.to_datetime(pr_df["pub_datetime"], errors="coerce")
        earliest_articles = pr_df.sort_values("pub_datetime_parsed").groupby("group_id").first().reset_index()

        # OpenAI ë°°ì¹˜ ìš”ì•½ (100ê°œì”©)
        articles_to_summarize = [
            {"group_id": row["group_id"], "title": row["title"], "description": row["description"]}
            for _, row in earliest_articles.iterrows()
        ]

        group_summaries = {}
        for i in range(0, len(articles_to_summarize), 100):
            batch = articles_to_summarize[i:i+100]
            batch_summaries = _call_openai_summarize_batch(batch, openai_key)
            group_summaries.update(batch_summaries)

        # press_release_group ì—…ë°ì´íŠ¸
        for idx, row in df[press_release_mask].iterrows():
            group_id = row["group_id"]
            if group_id in group_summaries:
                df.at[idx, "press_release_group"] = group_summaries[group_id]

        print(f"âœ… {len(group_summaries)}ê°œ ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì™„ë£Œ")
        return df

    except Exception as e:
        print(f"âš ï¸  ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}")
        return df


def save_excel(df: pd.DataFrame, filepath: Path, sheet_name: str = "data") -> None:
    """Excel íŒŒì¼ë¡œ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    df.to_excel(filepath, index=False, sheet_name=sheet_name, engine='openpyxl')
    print(f"ğŸ’¾ ì €ì¥: {filepath}")
