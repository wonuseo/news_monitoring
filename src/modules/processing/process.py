"""
process.py - Data Processing Module
ë°ì´í„° ì •ê·œí™”, ì¤‘ë³µ ì œê±°, Excel ì €ì¥
"""

import re
from html import unescape
from datetime import datetime
from pathlib import Path
from typing import Optional
import pandas as pd


def strip_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° ì—”í‹°í‹° ë””ì½”ë”©"""
    if not text:
        return ""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = unescape(text)
    return text.strip()


def parse_pubdate(pubdate_str: str) -> Optional[str]:
    """
    ë„¤ì´ë²„ pubDateë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    ì˜ˆ: 'Mon, 03 Feb 2026 10:30:00 +0900' â†’ '2026-02-03T10:30:00+09:00'
    """
    if not pubdate_str:
        return None
    
    try:
        dt = datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
        return dt.isoformat()
    except Exception as e:
        print(f"âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ '{pubdate_str}': {e}")
        return None


def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë°ì´í„° ì •ê·œí™”
    - HTML íƒœê·¸ ì œê±°
    - ë‚ ì§œ íŒŒì‹±
    """
    print("ğŸ”§ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    df = df.copy()
    
    df["title"] = df["title"].apply(strip_html)
    df["description"] = df["description"].apply(strip_html)
    df["pub_datetime"] = df["pubDate"].apply(parse_pubdate)
    
    print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ì •ê·œí™” ì™„ë£Œ")
    return df


def dedupe_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì¤‘ë³µ ì œê±°
    - originallink ë˜ëŠ” linkë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    - ìµœì‹  ê¸°ì‚¬ë§Œ ìœ ì§€
    """
    print("ğŸ”§ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")
    df = df.copy()

    # Primary key ìƒì„±
    df["pk"] = df["originallink"].where(df["originallink"].str.strip() != "", df["link"])

    # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹  â†’ ì˜¤ë˜ëœ)
    df["pub_datetime_sort"] = pd.to_datetime(df["pub_datetime"], errors="coerce")
    df = df.sort_values("pub_datetime_sort", ascending=False, na_position="last")

    # ì¤‘ë³µ ì œê±° (ìµœì‹  ê²ƒë§Œ ìœ ì§€)
    original_count = len(df)
    df_deduped = df.drop_duplicates(subset=["pk"], keep="first")

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df_deduped = df_deduped.drop(columns=["pk", "pub_datetime_sort"])

    removed = original_count - len(df_deduped)
    print(f"âœ… ì¤‘ë³µ {removed}ê°œ ì œê±°, {len(df_deduped)}ê°œ ê¸°ì‚¬ ìœ ì§€")
    return df_deduped.reset_index(drop=True)


def enrich_with_media_info(
    df: pd.DataFrame,
    spreadsheet=None,
    openai_key: str = None
) -> pd.DataFrame:
    """
    DataFrameì— ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (wrapper for media_classify.add_media_columns)

    Args:
        df: ì²˜ë¦¬ëœ DataFrame
        spreadsheet: gspread Spreadsheet ê°ì²´ (ì„ íƒì‚¬í•­)
        openai_key: OpenAI API í‚¤ (ì„ íƒì‚¬í•­)

    Returns:
        ì–¸ë¡ ì‚¬ ì •ë³´ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    try:
        from src.modules.enhancement.media_classify import add_media_columns
        return add_media_columns(df, spreadsheet, openai_key)
    except ImportError:
        print("âš ï¸  media_classify ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return df
    except Exception as e:
        print(f"âš ï¸  ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return df


def detect_similar_articles(
    df: pd.DataFrame,
    similarity_threshold: float = 0.8,
    min_text_length: int = 10
) -> pd.DataFrame:
    """
    ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ë¡œ ë³´ë„ìë£Œ ì‹ë³„
    - TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ê¸°ì‚¬ ê°ì§€
    - ìœ ì‚¬ë„ >= thresholdì¸ ê¸°ì‚¬ë“¤ì„ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ
    - ëª¨ë“  ê¸°ì‚¬ ìœ ì§€ (ë¹„íŒŒê´´ì  ë¼ë²¨ë§ë§Œ ìˆ˜í–‰)

    Args:
        df: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°ëœ DataFrame
        similarity_threshold: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.8)
        min_text_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ê°’: 10ì)

    Returns:
        'source' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
    df = df.copy()

    # source ì»¬ëŸ¼ ì´ˆê¸°í™” (ëª¨ë‘ ë¹ˆ ë¬¸ìì—´)
    df["source"] = ""

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError:
        print("âš ï¸  scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ì œëª©ê³¼ ì„¤ëª…ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ í…ìŠ¤íŠ¸ ìƒì„±
    df["search_text"] = (
        df["title"].fillna("") + " " + df["description"].fillna("")
    ).str.strip()

    # ìµœì†Œ ê¸¸ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê¸°ì‚¬ í•„í„°ë§
    valid_mask = df["search_text"].str.len() >= min_text_length
    valid_indices = df[valid_mask].index.tolist()

    if len(valid_indices) < 2:
        print(f"âœ… ê²€ìƒ‰ ëŒ€ìƒ ê¸°ì‚¬ê°€ {len(valid_indices)}ê°œë¡œ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ ìŠ¤í‚µ")
        df = df.drop(columns=["search_text"])
        return df

    try:
        # TF-IDF ë²¡í„°í™” (í•œê¸€ ìµœì í™”: ë¬¸ì ë‹¨ìœ„ n-gram ì‚¬ìš©)
        print(f"  - ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
        print(f"  - ê²€ìƒ‰ ëŒ€ìƒ: {len(valid_indices)}ê°œ ê¸°ì‚¬")

        vectorizer = TfidfVectorizer(
            analyzer='char',
            ngram_range=(2, 4),
            min_df=2,
            max_df=0.9,
            max_features=5000
        )

        # ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ë²¡í„°í™”
        valid_texts = df.loc[valid_indices, "search_text"].tolist()
        tfidf_matrix = vectorizer.fit_transform(valid_texts)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (í¬ì†Œ í–‰ë ¬ ì‚¬ìš©)
        similarity_matrix = cosine_similarity(tfidf_matrix)

        # ìœ ì‚¬ ê¸°ì‚¬ ê·¸ë£¹ ì‹ë³„
        similar_groups = set()
        for i in range(len(valid_indices)):
            for j in range(i + 1, len(valid_indices)):
                if similarity_matrix[i, j] >= similarity_threshold:
                    # ìœ ì‚¬í•œ ìŒ ë°œê²¬ - ë‘ ê¸°ì‚¬ ëª¨ë‘ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ
                    idx_i = valid_indices[i]
                    idx_j = valid_indices[j]
                    similar_groups.add(idx_i)
                    similar_groups.add(idx_j)

        # source ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
        for idx in similar_groups:
            df.at[idx, "source"] = "ë³´ë„ìë£Œ"

        similar_count = len(similar_groups)
        unique_count = len(valid_indices) - similar_count

        print(f"âœ… {similar_count}ê°œ ê¸°ì‚¬ë¥¼ 'ë³´ë„ìë£Œ'ë¡œ í‘œì‹œ, {unique_count}ê°œ ê¸°ì‚¬ëŠ” ë…ë¦½ ê¸°ì‚¬")

    except MemoryError:
        print("âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸  ìœ ì‚¬ë„ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=["search_text"])
    return df


def save_excel(df: pd.DataFrame, filepath: Path, sheet_name: str = "data") -> None:
    """Excel íŒŒì¼ë¡œ ì €ì¥"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    df.to_excel(filepath, index=False, sheet_name=sheet_name, engine='openpyxl')
    print(f"ğŸ’¾ ì €ì¥: {filepath}")
