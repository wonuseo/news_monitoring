"""
sheets.py - Google Sheets Integration Module
Google Sheetsë¡œ ë°ì´í„°ë¥¼ ì¦ë¶„ ì—…ë¡œë“œí•˜ê³  Looker Studio ì—°ê³„
"""

import pandas as pd
from typing import Dict, Optional, List, Tuple
from datetime import datetime
import os
import time

from src.utils.text_cleaning import clean_bom
from src.utils.sheets_helpers import get_or_create_worksheet
from src.utils.group_labels import is_competitor_group, is_our_group


def clean_all_bom_in_sheets(spreadsheet, sheet_names: list = None) -> Dict[str, int]:
    """
    Google Sheetsì˜ ëª¨ë“  ì…€ì—ì„œ BOM ë° invisible ë¬¸ìë¥¼ ì¼ê´„ ì œê±°

    ì „ì²´ ì‹œíŠ¸ ì¬ì‘ì„± ë°©ì‹: APIì˜ FORMATTED_VALUEê°€ BOMì„ ìˆ¨ê²¨ì„œ
    ì…€ ë‹¨ìœ„ ë¹„êµë¡œëŠ” ê°ì§€ ë¶ˆê°€ëŠ¥í•œ BOMë„ ì œê±°.

    ë™ì‘ ë°©ì‹:
    1. ì‹œíŠ¸ ì „ì²´ ê°’ì„ ì½ê¸°
    2. ëª¨ë“  ì…€ ê°’ì— clean_bom() ì ìš©
    3. ì „ì²´ ì‹œíŠ¸ë¥¼ ì •ë¦¬ëœ ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸° (ìˆ¨ê²¨ì§„ BOMë„ ì œê±°)

    Args:
        spreadsheet: gspread Spreadsheet ê°ì²´
        sheet_names: ì •ë¦¬í•  ì‹œíŠ¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ raw_data, total_result)

    Returns:
        {sheet_name: cleaned_cell_count}
    """
    if sheet_names is None:
        sheet_names = ["raw_data", "total_result"]

    results = {}

    for sheet_name in sheet_names:
        try:
            try:
                worksheet = spreadsheet.worksheet(sheet_name)
            except Exception:
                print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
                results[sheet_name] = 0
                continue

            # ì „ì²´ ë°ì´í„° ì½ê¸°
            all_values = worksheet.get_all_values()
            if not all_values:
                print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                results[sheet_name] = 0
                continue

            # ëª¨ë“  ì…€ ê°’ ì •ë¦¬ (APIê°€ BOMì„ ìˆ¨ê²¨ë„ ê°ì§€ ê°€ëŠ¥í•œ ê²ƒì€ ì¹´ìš´íŠ¸)
            cleaned_rows = []
            detected_count = 0

            for row in all_values:
                cleaned_row = []
                for cell_value in row:
                    if isinstance(cell_value, str) and cell_value:
                        cleaned = clean_bom(cell_value)
                        if cleaned != cell_value:
                            detected_count += 1
                        cleaned_row.append(cleaned)
                    else:
                        cleaned_row.append(cell_value if cell_value else "")
                cleaned_rows.append(cleaned_row)

            # ì „ì²´ ì‹œíŠ¸ ì¬ì‘ì„± (ìˆ¨ê²¨ì§„ BOMë„ ë®ì–´ì“°ê¸°ë¡œ ì œê±°)
            num_rows = len(cleaned_rows)
            num_cols = max(len(row) for row in cleaned_rows) if cleaned_rows else 0

            if num_rows > 0 and num_cols > 0:
                # ëª¨ë“  í–‰ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° (íŒ¨ë”©)
                for row in cleaned_rows:
                    while len(row) < num_cols:
                        row.append("")

                last_col = col_num_to_letter(num_cols)
                range_str = f"A1:{last_col}{num_rows}"

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë°ì´íŠ¸ (ëŒ€ìš©ëŸ‰ ì‹œíŠ¸ ëŒ€ì‘)
                batch_row_size = 2000
                for i in range(0, num_rows, batch_row_size):
                    batch_rows = cleaned_rows[i:i + batch_row_size]
                    start_row = i + 1
                    end_row = i + len(batch_rows)
                    batch_range = f"A{start_row}:{last_col}{end_row}"
                    worksheet.update(batch_range, batch_rows, value_input_option='RAW')
                    if i + batch_row_size < num_rows:
                        time.sleep(1.0)

                if detected_count > 0:
                    print(f"  âœ… '{sheet_name}': {detected_count}ê°œ ì…€ BOM ê°ì§€ + ì „ì²´ ì‹œíŠ¸ ì¬ì‘ì„± ì™„ë£Œ ({num_rows}í–‰)")
                else:
                    print(f"  âœ… '{sheet_name}': ì „ì²´ ì‹œíŠ¸ ì¬ì‘ì„± ì™„ë£Œ ({num_rows}í–‰, ìˆ¨ê²¨ì§„ BOM í¬í•¨ ì œê±°)")

            results[sheet_name] = detected_count

        except Exception as e:
            print(f"  âŒ '{sheet_name}' BOM ì •ë¦¬ ì‹¤íŒ¨: {e}")
            results[sheet_name] = 0

    return results


def col_num_to_letter(col_num: int) -> str:
    """
    ì»¬ëŸ¼ ë²ˆí˜¸ë¥¼ Excel/Sheets ìŠ¤íƒ€ì¼ ë¬¸ìë¡œ ë³€í™˜

    Args:
        col_num: ì»¬ëŸ¼ ë²ˆí˜¸ (1-based, 1=A, 27=AA)

    Returns:
        ì»¬ëŸ¼ ë¬¸ì (A, B, ..., Z, AA, AB, ...)

    Examples:
        1 -> A
        26 -> Z
        27 -> AA
        52 -> AZ
        53 -> BA
    """
    result = ""
    while col_num > 0:
        col_num -= 1  # 0-basedë¡œ ë³€í™˜
        result = chr(65 + (col_num % 26)) + result
        col_num //= 26
    return result


def connect_sheets(credentials_path: str, sheet_id: str):
    """
    Google Sheets ì—°ê²°

    Args:
        credentials_path: ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤ ê²½ë¡œ
        sheet_id: êµ¬ê¸€ ì‹œíŠ¸ ID

    Returns:
        gspread Spreadsheet ê°ì²´
    """
    try:
        import gspread
        from google.oauth2.service_account import Credentials

        if not os.path.exists(credentials_path):
            raise FileNotFoundError(f"ìê²©ì¦ëª… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {credentials_path}")

        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        scope = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]

        creds = Credentials.from_service_account_file(credentials_path, scopes=scope)
        client = gspread.authorize(creds)

        # ì‹œíŠ¸ ì—´ê¸°
        spreadsheet = client.open_by_key(sheet_id)

        print(f"âœ… Google Sheets ì—°ê²° ì„±ê³µ: {spreadsheet.title}")
        return spreadsheet

    except ImportError:
        print("âŒ gspread ë˜ëŠ” google-auth ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  pip install gspread google-auth google-auth-oauthlib")
        return None
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return None
    except Exception as e:
        print(f"âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
        return None


def _normalize_header(value) -> str:
    """Normalize headers/cells for robust column matching."""
    return clean_bom(value)


def _read_sheet_values(worksheet) -> Tuple[List[str], List[List[str]]]:
    """
    Read worksheet values and return normalized headers + data rows.

    Returns:
        (headers, rows)
    """
    all_values = worksheet.get_all_values()
    if not all_values:
        return [], []
    headers = [_normalize_header(h) for h in all_values[0]]
    return headers, all_values[1:]


def _find_header_index(headers: List[str], column_name: str) -> Optional[int]:
    """Find header index (case-insensitive)."""
    target = _normalize_header(column_name).lower()
    for idx, header in enumerate(headers):
        if _normalize_header(header).lower() == target:
            return idx
    return None


def _ensure_sheet_headers(
    worksheet,
    existing_headers: List[str],
    df_headers: List[str],
) -> List[str]:
    """
    Ensure worksheet has clean headers and includes all DataFrame columns.
    Returns the final header order used for read/write.
    """
    final_headers = [_normalize_header(h) for h in existing_headers]

    # ë¹ˆ ì‹œíŠ¸(ë˜ëŠ” í—¤ë”ë§Œ ì†ìƒ)ë©´ DataFrame í—¤ë”ë¡œ ì´ˆê¸°í™”
    if not final_headers or all(h == "" for h in final_headers):
        final_headers = [_normalize_header(col) for col in df_headers]
    else:
        # ì‹œíŠ¸ì— ì—†ëŠ” ì‹ ê·œ ì»¬ëŸ¼ì€ ë’¤ì— ì¶”ê°€
        for col in df_headers:
            normalized = _normalize_header(col)
            if normalized and _find_header_index(final_headers, normalized) is None:
                final_headers.append(normalized)

    if not final_headers:
        return final_headers

    # ì»¬ëŸ¼ ìˆ˜ ë¶€ì¡± ì‹œ í™•ì¥
    if getattr(worksheet, "col_count", len(final_headers)) < len(final_headers):
        worksheet.add_cols(len(final_headers) - worksheet.col_count)

    # í—¤ë”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¬ì‘ì„±í•´ BOM/ê³µë°± ë¬¸ì œë¥¼ ê³ ì •
    last_col = col_num_to_letter(len(final_headers))
    worksheet.update(f"A1:{last_col}1", [final_headers], value_input_option='RAW')
    return final_headers


def load_existing_links_from_sheets(spreadsheet, sheet_name: str = "raw_data") -> set:
    """
    Google Sheetsì—ì„œ ê¸°ì¡´ ê¸°ì‚¬ ë§í¬ ëª©ë¡ ë¡œë“œ

    Args:
        spreadsheet: gspread Spreadsheet ê°ì²´
        sheet_name: ì›Œí¬ì‹œíŠ¸ ì´ë¦„ (ê¸°ë³¸: raw_data)

    Returns:
        ê¸°ì¡´ ê¸°ì‚¬ ë§í¬ set (ì¤‘ë³µ ì œê±°ìš©)
    """
    try:
        # ì›Œí¬ì‹œíŠ¸ ì„ íƒ
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
        except Exception:
            print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²« ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
            return set()

        headers, rows = _read_sheet_values(worksheet)
        if not headers or not rows:
            print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return set()

        link_idx = _find_header_index(headers, "link")
        if link_idx is None:
            print(f"  âš ï¸  '{sheet_name}'ì—ì„œ link ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return set()

        existing_links = set()
        for row in rows:
            link = clean_bom(row[link_idx]) if len(row) > link_idx else ""
            if link:
                existing_links.add(link)

        print(f"ğŸ“‚ Google Sheetsì—ì„œ {len(existing_links)}ê°œ ê¸°ì¡´ ê¸°ì‚¬ ë¡œë“œ")
        return existing_links

    except Exception as e:
        print(f"âš ï¸  Google Sheets ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("  â†’ ì¦ë¶„ ìˆ˜ì§‘ ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return set()


def load_analysis_status_from_sheets(
    spreadsheet,
    sheet_name: str = "total_result",
    analysis_cols: Optional[List[str]] = None
) -> Dict[str, set]:
    """
    [DEPRECATED] reprocess_checker.pyì˜ check_reprocess_targets()ë¡œ ëŒ€ì²´ë¨.
    í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ, main.pyì—ì„œëŠ” ë” ì´ìƒ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ.

    Google Sheetsì—ì„œ ë¶„ì„ ì™„ë£Œ/ë¯¸ì™„ë£Œ ë§í¬ ì§‘í•© ë¡œë“œ

    Returns:
        {"processed_links": set, "missing_analysis_links": set}
    """
    if analysis_cols is None:
        # LLM ë¶„ì„ + ì „ì²˜ë¦¬ í•„ë“œ ì²´í¬
        analysis_cols = [
            "brand_relevance", "sentiment_stage",  # LLM ë¶„ì„
            "source", "media_domain", "date_only"  # ì „ì²˜ë¦¬ í•„ë“œ
        ]

    try:
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
        except Exception:
            print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²« ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
            return {"processed_links": set(), "missing_analysis_links": set()}

        headers, rows = _read_sheet_values(worksheet)
        if not headers or not rows:
            print(f"  â„¹ï¸  '{sheet_name}' ì›Œí¬ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return {"processed_links": set(), "missing_analysis_links": set()}

        link_idx = _find_header_index(headers, "link")
        if link_idx is None:
            print(f"  âš ï¸  '{sheet_name}'ì—ì„œ link ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {"processed_links": set(), "missing_analysis_links": set()}

        analysis_indexes = {
            col: _find_header_index(headers, col)
            for col in analysis_cols
        }

        processed_links = set()
        missing_analysis_links = set()

        for row in rows:
            link = clean_bom(row[link_idx]) if len(row) > link_idx else ""
            if not link:
                continue
            processed_links.add(link)
            # ë¶„ì„ í•„ë“œê°€ í•˜ë‚˜ë¼ë„ ë¹„ì–´ ìˆìœ¼ë©´ ì¬ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ê°„ì£¼ (BOM ë¬¸ìë„ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬)
            for col in analysis_cols:
                col_idx = analysis_indexes.get(col)
                val = row[col_idx] if col_idx is not None and len(row) > col_idx else ""
                # BOM ë¬¸ì ì œê±° í›„ ì²´í¬
                cleaned_val = clean_bom(val)
                if cleaned_val == "":
                    missing_analysis_links.add(link)
                    break

        print(f"ğŸ“‚ Google Sheetsì—ì„œ {len(processed_links)}ê°œ ê¸°ì¡´ ê¸°ì‚¬ ë¡œë“œ (total_result)")
        if missing_analysis_links:
            print(f"  â„¹ï¸  ë¶„ì„ ëˆ„ë½ ë§í¬ {len(missing_analysis_links)}ê°œ ë°œê²¬")

        return {
            "processed_links": processed_links,
            "missing_analysis_links": missing_analysis_links
        }

    except Exception as e:
        print(f"âš ï¸  Google Sheets ë¶„ì„ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("  â†’ result.csv ê¸°ì¤€ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return {"processed_links": set(), "missing_analysis_links": set()}


def filter_new_articles_from_sheets(df_raw: pd.DataFrame, existing_links: set) -> pd.DataFrame:
    """
    Google Sheets ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ìƒˆ ê¸°ì‚¬ë§Œ í•„í„°ë§

    Args:
        df_raw: ìˆ˜ì§‘í•œ ì›ë³¸ DataFrame
        existing_links: Google Sheetsì˜ ê¸°ì¡´ ë§í¬ set

    Returns:
        ìƒˆ ê¸°ì‚¬ë§Œ í¬í•¨í•œ DataFrame
    """
    if len(existing_links) == 0:
        print(f"âœ… ëª¨ë“  {len(df_raw)}ê°œ ê¸°ì‚¬ê°€ ìƒˆ ê¸°ì‚¬ì…ë‹ˆë‹¤ (ê¸°ì¡´ ë°ì´í„° ì—†ìŒ)")
        return df_raw

    # link ì»¬ëŸ¼ì´ ê¸°ì¡´ ë§í¬ì— ì—†ëŠ” í–‰ë§Œ í•„í„°ë§
    df_new = df_raw[~df_raw["link"].isin(existing_links)].copy()

    skipped = len(df_raw) - len(df_new)
    print(f"âœ… {len(df_new)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬ ({skipped}ê°œ ì¤‘ë³µ ê±´ë„ˆëœ€)")

    return df_new


def sync_to_sheets(df: pd.DataFrame, spreadsheet,
                  sheet_name: str = "ì „ì²´ë°ì´í„°",
                  key_column: str = "link",
                  update_fields: list = None,
                  force_update_existing: bool = False) -> Dict[str, int]:
    """
    DataFrameì„ Google Sheetsì— upsert (update or insert)

    Args:
        df: ì—…ë¡œë“œí•  DataFrame
        spreadsheet: gspread Spreadsheet ê°ì²´
        sheet_name: ì›Œí¬ì‹œíŠ¸ ì´ë¦„
        key_column: ì¤‘ë³µ ì œê±° ê¸°ì¤€ ì»¬ëŸ¼
        update_fields: ì—…ë°ì´íŠ¸í•  í•„ë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ë¶„ì„ í•„ë“œ ìë™ ê°ì§€)
        force_update_existing: Trueë©´ ê¸°ì¡´ í‚¤ í–‰ë„ ê°•ì œ ì—…ë°ì´íŠ¸

    Returns:
        {"attempted": N, "added": N, "updated": N, "skipped": N, "errors": N}
        - attempted: ì´ë²ˆì— ì—…ë¡œë“œ ëŒ€ìƒìœ¼ë¡œ ë„˜ê¸´ ê¸°ì‚¬ ìˆ˜
        - added: ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì‚¬ ìˆ˜
        - updated: ê¸°ì¡´ í–‰ ì—…ë°ì´íŠ¸ëœ ê¸°ì‚¬ ìˆ˜
        - skipped: ì‹œíŠ¸ì— ì´ë¯¸ ì¡´ì¬í•˜ê³  ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”í•œ ê¸°ì‚¬ ìˆ˜
    """
    # ì—…ë°ì´íŠ¸í•  ë¶„ì„ ë° ì „ì²˜ë¦¬ í•„ë“œ (ê¸°ë³¸ê°’)
    if update_fields is None:
        update_fields = [
            # LLM ë¶„ì„ í•„ë“œ
            "brand_relevance", "brand_relevance_query_keywords",
            "sentiment_stage", "danger_level", "issue_category",
            "news_category", "news_keyword_summary", "classified_at",
            # ì „ì²˜ë¦¬ í•„ë“œ
            "cluster_id", "cluster_summary", "source",
            "media_domain", "media_name", "media_group", "media_type",
            # Looker Studio ì‹œê³„ì—´ í•„ë“œ
            "date_only", "week_number", "month", "article_count"
        ]

    try:
        if df is None or len(df) == 0:
            return {"attempted": 0, "added": 0, "updated": 0, "skipped": 0, "errors": 0}

        # DataFrame ì»¬ëŸ¼ ì •ê·œí™” ë§¤í•‘ (ì†Œë¬¸ì í‚¤ ê¸°ì¤€)
        df_column_lookup = {}
        for col in df.columns:
            normalized = _normalize_header(col).lower()
            if normalized and normalized not in df_column_lookup:
                df_column_lookup[normalized] = col

        key_col_norm = _normalize_header(key_column).lower()
        if key_col_norm not in df_column_lookup:
            raise ValueError(f"'{key_column}' ì»¬ëŸ¼ì´ DataFrameì— ì—†ìŠµë‹ˆë‹¤.")

        # ì›Œí¬ì‹œíŠ¸ ì„ íƒ ë˜ëŠ” ìƒì„±
        worksheet = get_or_create_worksheet(spreadsheet, sheet_name, rows=1000, cols=30)

        # ê¸°ì¡´ ì‹œíŠ¸ ì½ê¸° + í—¤ë” ë³´ì •
        sheet_headers, raw_rows = _read_sheet_values(worksheet)
        key_idx_before_fix = _find_header_index(sheet_headers, key_column)
        sheet_headers = _ensure_sheet_headers(worksheet, sheet_headers, list(df.columns))
        if not sheet_headers:
            raise ValueError("ì‹œíŠ¸ í—¤ë”ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ë°ì´í„° í–‰ì´ ì´ë¯¸ ìˆëŠ”ë° key í—¤ë”ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨
        if raw_rows and key_idx_before_fix is None:
            raise ValueError(
                f"'{sheet_name}' ì‹œíŠ¸ì— ë°ì´í„°ê°€ ìˆì§€ë§Œ key í—¤ë” '{key_column}'ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                "í—¤ë”ë¥¼ í™•ì¸/ë³µêµ¬ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        key_idx = _find_header_index(sheet_headers, key_column)
        if key_idx is None:
            raise ValueError(f"'{sheet_name}' ì‹œíŠ¸ í—¤ë”ì— key ì»¬ëŸ¼ '{key_column}'ì´ ì—†ìŠµë‹ˆë‹¤.")

        attempted = len(df)
        added_count = 0
        updated_count = 0
        skipped_count = 0

        # ê¸°ì¡´ ë°ì´í„°ë¥¼ dictë¡œ ë³€í™˜ (key â†’ row_index, row_data)
        existing_by_key = {}
        for row_idx, row in enumerate(raw_rows, start=2):  # í—¤ë”ëŠ” 1í–‰, ë°ì´í„°ëŠ” 2í–‰ë¶€í„°
            row_values = [
                clean_bom(row[col_idx]) if col_idx < len(row) else ""
                for col_idx in range(len(sheet_headers))
            ]
            key_val = row_values[key_idx] if key_idx < len(row_values) else ""
            if not key_val:
                continue

            row_dict = {}
            for col_idx, header in enumerate(sheet_headers):
                header_norm = _normalize_header(header).lower()
                # ì¤‘ë³µ í—¤ë”ê°€ ìˆì–´ë„ ì²« ë²ˆì§¸ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
                if header_norm and header_norm not in row_dict:
                    row_dict[header_norm] = row_values[col_idx]
            existing_by_key[key_val] = {"row_idx": row_idx, "data": row_dict}

        update_field_norms = {_normalize_header(field).lower() for field in update_fields}

        # ìƒˆë¡œìš´ í–‰ê³¼ ì—…ë°ì´íŠ¸ ëŒ€ìƒ í–‰ ë¶„ë¥˜
        new_rows = []
        rows_to_update = []  # (row_idx, row_dict, existing_row_dict)
        for row in df.to_dict("records"):
            key_val = clean_bom(row.get(df_column_lookup[key_col_norm], ""))

            if not key_val or key_val not in existing_by_key:
                new_rows.append(row)
                continue

            existing_row_info = existing_by_key[key_val]
            existing_row_data = existing_row_info["data"]
            row_idx = existing_row_info["row_idx"]

            if force_update_existing:
                needs_update = True
            else:
                needs_update = False
                for field_norm in update_field_norms:
                    if field_norm not in df_column_lookup:
                        continue
                    field_col = df_column_lookup[field_norm]
                    new_val = clean_bom(row.get(field_col, ""))
                    existing_val = clean_bom(existing_row_data.get(field_norm, ""))

                    # ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°:
                    # 1. ë¹ˆ ê°’ì— ì‹¤ì œ ê°’ì´ ë“¤ì–´ê°ˆ ë•Œ (ê¸°ì¡´: ë¹ˆê°’, ìƒˆë¡œìš´: ê°’ ìˆìŒ)
                    # 2. ë‘˜ ë‹¤ ê°’ì´ ìˆê³  ë‹¤ë¥¼ ë•Œ (ê¸°ì¡´: ê°’A, ìƒˆë¡œìš´: ê°’B)
                    # ì ˆëŒ€ í•˜ì§€ ì•ŠëŠ” ê²½ìš°:
                    # - ë¹ˆ ê°’ â†’ ë¹ˆ ê°’ (ë³€ê²½ ì—†ìŒ)
                    # - ê¸°ì¡´ ê°’ â†’ ë¹ˆ ê°’ (ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë³´í˜¸!)
                    if existing_val == "" and new_val != "":
                        needs_update = True
                        break
                    if existing_val != "" and new_val != "" and new_val != existing_val:
                        needs_update = True
                        break

            if needs_update:
                rows_to_update.append((row_idx, row, existing_row_data))
            else:
                skipped_count += 1

        # ìƒˆ í–‰ ì¶”ê°€ (batch append)
        if new_rows:
            values_to_append = []
            for row in new_rows:
                row_values = []
                for header in sheet_headers:
                    header_norm = _normalize_header(header).lower()
                    if header_norm in df_column_lookup:
                        col_name = df_column_lookup[header_norm]
                        row_values.append(clean_bom(row.get(col_name, "")))
                    else:
                        row_values.append("")
                values_to_append.append(row_values)

            # ì¼ê´„ ì¶”ê°€ (ìµœëŒ€ 1000í–‰ì”©)
            batch_size = 1000
            for i in range(0, len(values_to_append), batch_size):
                batch = values_to_append[i:i + batch_size]
                worksheet.append_rows(batch)
                time.sleep(1.0)  # Rate limit ë°©ì§€

            added_count = len(new_rows)
            print(f"  âœ… {sheet_name}: {added_count}ê°œ í–‰ ì¶”ê°€")

        # ê¸°ì¡´ í–‰ ì—…ë°ì´íŠ¸ (batch update)
        if rows_to_update:
            updates = []
            for row_idx, row_data, existing_row_data in rows_to_update:
                row_values = []
                for header in sheet_headers:
                    header_norm = _normalize_header(header).lower()
                    existing_val = clean_bom(existing_row_data.get(header_norm, ""))

                    if header_norm in df_column_lookup:
                        col_name = df_column_lookup[header_norm]
                        new_val = clean_bom(row_data.get(col_name, ""))
                    else:
                        new_val = ""

                    # ìƒˆ ê°’ì´ ë¹„ì–´ìˆê³  ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ â†’ ê¸°ì¡´ ê°’ ë³´í˜¸
                    if (header_norm not in df_column_lookup) or (new_val == "" and existing_val != ""):
                        cleaned_val = existing_val
                    else:
                        cleaned_val = new_val
                    row_values.append(cleaned_val)

                last_col_letter = col_num_to_letter(len(sheet_headers))
                range_name = f"A{row_idx}:{last_col_letter}{row_idx}"
                updates.append({"range": range_name, "values": [row_values]})

            if len(updates) > 0:
                first_range = updates[0]["range"]
                last_range = updates[-1]["range"]
                if len(updates) == 1:
                    print(f"    ğŸ” ì—…ë°ì´íŠ¸ ë²”ìœ„: {first_range} (1ê°œ í–‰)")
                else:
                    print(f"    ğŸ” ì—…ë°ì´íŠ¸ ë²”ìœ„: {first_range} ~ {last_range} ({len(updates)}ê°œ í–‰)")

            # batch_update ì‹¤í–‰ (ìµœëŒ€ 100ê°œì”©)
            update_batch_size = 100
            for i in range(0, len(updates), update_batch_size):
                batch_updates = updates[i:i + update_batch_size]
                try:
                    worksheet.batch_update(batch_updates, value_input_option='RAW')
                    time.sleep(1.0)  # Rate limit ë°©ì§€
                except Exception as e:
                    error_msg = str(e)
                    print(f"    âš ï¸  batch_update ì‹¤íŒ¨: {error_msg}")
                    if batch_updates:
                        print(f"    ğŸ” ì²« ë²ˆì§¸ range ì˜ˆì‹œ: {batch_updates[0]['range']}")
                    # Fallback: ê°œë³„ update
                    for idx, update in enumerate(batch_updates):
                        try:
                            range_str = update["range"]
                            if idx < 3:
                                print(f"    ğŸ” ê°œë³„ update ì‹œë„ [{idx + 1}]: range='{range_str}'")
                            worksheet.update(range_str, update["values"], value_input_option='RAW')
                            time.sleep(0.5)
                        except Exception as e2:
                            print(f"    âš ï¸  ê°œë³„ update ì‹¤íŒ¨ [range={update.get('range', 'N/A')}]: {e2}")

            updated_count = len(rows_to_update)
            print(f"  ğŸ”„ {sheet_name}: {updated_count}ê°œ í–‰ ì—…ë°ì´íŠ¸")

        if added_count == 0 and updated_count == 0:
            print(f"  â„¹ï¸  {sheet_name}: ë³€ê²½ ì‚¬í•­ ì—†ìŒ ({skipped_count}ê°œ ê±´ë„ˆëœ€)")

        return {
            "attempted": attempted,
            "added": added_count,
            "updated": updated_count,
            "skipped": skipped_count,
            "errors": 0
        }

    except Exception as e:
        print(f"  âŒ {sheet_name} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"attempted": len(df), "added": 0, "updated": 0, "skipped": 0, "errors": len(df)}


def configure_sheet_schema(worksheet) -> None:
    """
    Google Sheets ì›Œí¬ì‹œíŠ¸ ìŠ¤í‚¤ë§ˆ ì„¤ì •

    Args:
        worksheet: gspread Worksheet ê°ì²´
    """
    try:
        import gspread

        # ë‚ ì§œ ì»¬ëŸ¼ ì„¤ì •
        date_columns = {
            "pub_datetime": "DATETIME",
            "classified_at": "DATETIME",
            "full_text_scraped_at": "DATETIME",
            "scraped_at": "DATETIME",
            "date_only": "DATE"
        }

        # ìˆ«ì ì»¬ëŸ¼
        number_columns = ["article_count"]

        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼
        text_columns = ["full_text"]

        # í—¤ë” í–‰ ì½ê¸°
        headers = worksheet.row_values(1)

        for idx, header in enumerate(headers, 1):
            if header in date_columns:
                # ë‚ ì§œ í¬ë§· ì„¤ì • (APIë¡œëŠ” ì§ì ‘ ì„¤ì • ë¶ˆê°€, ìˆ˜ë™ ì„¤ì • í•„ìš”)
                pass
            elif header in number_columns:
                # ìˆ«ì í¬ë§·
                pass
            elif header in text_columns:
                # í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ í™œì„±í™”
                pass

        print("  â„¹ï¸  ì›Œí¬ì‹œíŠ¸ ìŠ¤í‚¤ë§ˆ ì„¤ì • ì™„ë£Œ")

    except Exception as e:
        print(f"  âš ï¸  ìŠ¤í‚¤ë§ˆ ì„¤ì • ì‹¤íŒ¨: {e}")


TOTAL_RESULT_MIN_DATE = "2026-02-01"
TOTAL_RESULT_DATE_COLUMNS = [
    "pub_datetime",
    "date_only",
    "pubDate",
    "pub_date",
    "published_at",
    "date",
]


def filter_total_result_by_date(
    df_result: pd.DataFrame,
    min_date: str = TOTAL_RESULT_MIN_DATE,
) -> pd.DataFrame:
    """
    Keep only rows on/after min_date for total_result upload.
    raw_dataëŠ” ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤.
    """
    if df_result.empty:
        return df_result

    candidate_cols = [col for col in TOTAL_RESULT_DATE_COLUMNS if col in df_result.columns]
    if not candidate_cols:
        print("  âš ï¸  total_result ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ì–´ ë‚ ì§œ í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df_result

    cutoff = pd.Timestamp(min_date, tz="UTC")

    # ì»¬ëŸ¼ë³„ íŒŒì‹± ì„±ê³µë¥ /ìœ ì§€ ê±´ìˆ˜ë¥¼ ë¹„êµí•´ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ë‚ ì§œ ì»¬ëŸ¼ ì„ íƒ
    best_col = None
    best_parsed = None
    best_score = (-1, -1)  # (kept_count, valid_count)
    for col in candidate_cols:
        parsed = pd.to_datetime(df_result[col], errors="coerce", utc=True)
        valid_count = int(parsed.notna().sum())
        kept_count = int((parsed >= cutoff).sum())
        score = (kept_count, valid_count)
        if score > best_score:
            best_col = col
            best_parsed = parsed
            best_score = score

    if best_col is None or best_parsed is None:
        print("  âš ï¸  total_result ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ë¡œ ë‚ ì§œ í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df_result

    keep_mask = best_parsed >= cutoff

    before_count = len(df_result)
    filtered = df_result[keep_mask].copy()
    removed_count = before_count - len(filtered)
    print(
        f"  ğŸ” total_result ë‚ ì§œ í•„í„°({best_col}): "
        f"{removed_count}ê°œ ì œì™¸ (< {min_date}), {len(filtered)}ê°œ ìœ ì§€"
    )

    return filtered


def deduplicate_sheet(
    spreadsheet,
    sheet_name: str,
    key_column: str = "link",
) -> Dict[str, int]:
    """
    Sheets íƒ­ ë‚´ ì¤‘ë³µ í–‰ ì œê±° (key_column ê¸°ì¤€ ì²« ë²ˆì§¸ í–‰ ìœ ì§€).

    Args:
        spreadsheet: gspread Spreadsheet ê°ì²´
        sheet_name: ëŒ€ìƒ ì›Œí¬ì‹œíŠ¸ ì´ë¦„
        key_column: ì¤‘ë³µ ê¸°ì¤€ ì»¬ëŸ¼ (ê¸°ë³¸: link)

    Returns:
        {"before": N, "after": N, "removed": N}
    """
    try:
        worksheet = spreadsheet.worksheet(sheet_name)
    except Exception:
        return {"before": 0, "after": 0, "removed": 0}

    all_values = worksheet.get_all_values()
    if not all_values or len(all_values) <= 1:
        return {"before": 0, "after": 0, "removed": 0}

    headers = [_normalize_header(h) for h in all_values[0]]
    rows = all_values[1:]
    before = len(rows)

    key_idx = _find_header_index(headers, key_column)
    if key_idx is None:
        return {"before": before, "after": before, "removed": 0}

    seen: set = set()
    deduped = []
    for row in rows:
        key = clean_bom(row[key_idx]) if key_idx < len(row) else ""
        if not key or key not in seen:
            deduped.append(row)
            if key:
                seen.add(key)

    removed = before - len(deduped)
    if removed == 0:
        return {"before": before, "after": before, "removed": 0}

    # í—¤ë” ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”© í›„ ì „ì²´ ì‹œíŠ¸ ì¬ì‘ì„±
    n_cols = len(headers)
    padded = [
        (row + [""] * n_cols)[:n_cols]
        for row in deduped
    ]
    all_data = [headers] + padded

    worksheet.clear()
    batch_size = 2000
    last_col = col_num_to_letter(n_cols)
    for i in range(0, len(all_data), batch_size):
        batch = all_data[i:i + batch_size]
        start_row = i + 1
        end_row = i + len(batch)
        worksheet.update(f"A{start_row}:{last_col}{end_row}", batch, value_input_option="RAW")
        if i + batch_size < len(all_data):
            time.sleep(1.0)

    return {"before": before, "after": len(deduped), "removed": removed}


def sync_raw_and_processed(df_raw: pd.DataFrame, df_result: pd.DataFrame, spreadsheet) -> Dict[str, Dict]:
    """
    ì›ë³¸ ë°ì´í„°ì™€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ Google Sheetsì— upsert (update or insert)

    ì‹œíŠ¸ êµ¬ì¡°:
    - raw_data: ì›ë³¸ ë°ì´í„° (ìˆ˜ì§‘ëœ ê·¸ëŒ€ë¡œ)
    - total_result: ì „ì²´ ë¶„ë¥˜ ê²°ê³¼ (ë…ë¦½ê¸°ì‚¬ + ë³´ë„ìë£Œ) - ê¸°ì¡´ í–‰ ì—…ë°ì´íŠ¸ ì§€ì›

    ë™ì‘:
    - ìƒˆ ê¸°ì‚¬: append
    - ê¸°ì¡´ ê¸°ì‚¬ (ë¶„ì„ í•„ë“œ ë¹„ì–´ìˆìŒ): update
    - ê¸°ì¡´ ê¸°ì‚¬ (ë¶„ì„ í•„ë“œ ìˆìŒ): skip

    Args:
        df_raw: ì›ë³¸ ë°ì´í„° (ìˆ˜ì§‘ëœ ê·¸ëŒ€ë¡œ)
        df_result: ë¶„ë¥˜ ê²°ê³¼ (AI ë¶„ë¥˜ ì™„ë£Œ)
        spreadsheet: gspread Spreadsheet ê°ì²´

    Returns:
        {sheet_name: {added, updated, skipped, errors}}
    """
    results = {}

    print("ğŸ“Š Google Sheets ë™ê¸°í™” ì¤‘...")

    # 1. raw_data - ì›ë³¸ ë°ì´í„°
    print("\n  [1/2] raw_data (ì›ë³¸ ë°ì´í„°)")
    results["raw_data"] = sync_to_sheets(df_raw, spreadsheet, "raw_data")

    # 2. total_result - ì „ì²´ ë¶„ë¥˜ ê²°ê³¼ (upsert ì§€ì›)
    df_result_for_total = filter_total_result_by_date(df_result, TOTAL_RESULT_MIN_DATE)
    print("  [2/2] total_result (ì „ì²´ ë¶„ë¥˜ ê²°ê³¼)")
    results["total_result"] = sync_to_sheets(
        df_result_for_total,
        spreadsheet,
        "total_result",
        force_update_existing=True
    )

    # í†µê³„
    print("\nâœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")
    total_attempted = sum(r.get("attempted", 0) for r in results.values())
    total_added = sum(r.get("added", 0) for r in results.values())
    total_updated = sum(r.get("updated", 0) for r in results.values())
    total_skipped = sum(r.get("skipped", 0) for r in results.values())
    total_errors = sum(r.get("errors", 0) for r in results.values())

    print(f"  - ì‹œë„ë¨: {total_attempted}ê°œ")
    print(f"  - ì¶”ê°€ë¨: {total_added}ê°œ")
    print(f"  - ì—…ë°ì´íŠ¸ë¨: {total_updated}ê°œ")
    print(f"  - ê±´ë„ˆëœ€(ë³€ê²½ ì—†ìŒ): {total_skipped}ê°œ")
    if total_errors > 0:
        print(f"  - ì˜¤ë¥˜: {total_errors}ê°œ")
    return results


def sync_all_sheets(df: pd.DataFrame, spreadsheet) -> Dict[str, Dict]:
    """
    (deprecated) ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì›Œí¬ì‹œíŠ¸ì— ë™ì‹œ ì—…ë¡œë“œ

    ëŒ€ì‹  sync_raw_and_processed()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        df: ë¶„ë¥˜ëœ DataFrame
        spreadsheet: gspread Spreadsheet ê°ì²´

    Returns:
        {sheet_name: {added, skipped, errors}}
    """
    print("âš ï¸  sync_all_sheets()ëŠ” deprecated ë¨. sync_raw_and_processed()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    results = {}

    print("ğŸ“Š Google Sheets ë™ê¸°í™” ì¤‘...")

    # 1. ì „ì²´ ë°ì´í„°
    print("\n  [1/4] ì „ì²´ë°ì´í„°")
    results["ì „ì²´ë°ì´í„°"] = sync_to_sheets(df, spreadsheet, "ì „ì²´ë°ì´í„°")

    # 2. ìš°ë¦¬ ë¸Œëœë“œ ë¶€ì •
    print("  [2/4] ìš°ë¦¬_ë¶€ì •")
    our_negative = df[(df["group"].map(is_our_group)) & (df["sentiment"] == "ë¶€ì •")]
    results["ìš°ë¦¬_ë¶€ì •"] = sync_to_sheets(our_negative, spreadsheet, "ìš°ë¦¬_ë¶€ì •")

    # 3. ìš°ë¦¬ ë¸Œëœë“œ ê¸ì •
    print("  [3/4] ìš°ë¦¬_ê¸ì •")
    our_positive = df[(df["group"].map(is_our_group)) & (df["sentiment"] == "ê¸ì •")]
    results["ìš°ë¦¬_ê¸ì •"] = sync_to_sheets(our_positive, spreadsheet, "ìš°ë¦¬_ê¸ì •")

    # 4. ê²½ìŸì‚¬
    print("  [4/4] ê²½ìŸì‚¬")
    competitor = df[df["group"].map(is_competitor_group)]
    results["ê²½ìŸì‚¬"] = sync_to_sheets(competitor, spreadsheet, "ê²½ìŸì‚¬")

    # í†µê³„
    print("\nâœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")
    total_attempted = sum(r.get("attempted", 0) for r in results.values())
    total_added = sum(r["added"] for r in results.values())
    total_skipped = sum(r["skipped"] for r in results.values())
    total_errors = sum(r["errors"] for r in results.values())

    print(f"  - ì‹œë„ë¨: {total_attempted}ê°œ")
    print(f"  - ì¶”ê°€ë¨: {total_added}ê°œ")
    print(f"  - ê±´ë„ˆëœ€(ì‹œíŠ¸ì— ì´ë¯¸ ì¡´ì¬): {total_skipped}ê°œ")
    print(f"  - ì˜¤ë¥˜: {total_errors}ê°œ")

    return results


def sync_result_to_sheets(
    df_result,
    raw_df,
    spreadsheet,
    verbose: bool = True,
):
    """
    ë¶„ë¥˜ ê²°ê³¼ DataFrameì„ Google Sheetsì— ë™ê¸°í™” (upsert ë°©ì‹).

    classify_llm, classify_press_releasesì˜ ì²­í¬ë³„ ì½œë°±ì—ì„œ ì‚¬ìš©.

    Args:
        df_result: ë¶„ë¥˜ ê²°ê³¼ DataFrame
        raw_df: ì›ë³¸ raw DataFrame
        spreadsheet: gspread Spreadsheet ê°ì²´
        verbose: ì§„í–‰ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€

    Returns:
        ë™ê¸°í™” ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not spreadsheet or df_result is None or len(df_result) == 0:
        return None

    try:
        sync_results = sync_raw_and_processed(raw_df, df_result, spreadsheet)

        added_count = sum(r.get("added", 0) for r in sync_results.values())
        updated_count = sum(r.get("updated", 0) for r in sync_results.values())

        if verbose and (added_count > 0 or updated_count > 0):
            msg_parts = []
            if added_count > 0:
                msg_parts.append(f"{added_count}ê°œ ì¶”ê°€")
            if updated_count > 0:
                msg_parts.append(f"{updated_count}ê°œ ì—…ë°ì´íŠ¸")
            print(f"    â˜ï¸  Sheets ë™ê¸°í™”: {', '.join(msg_parts)}")

        return sync_results

    except Exception as e:
        if verbose:
            print(f"    âš ï¸  Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        return None
