"""Step 1: News collection, reprocess check, date filtering."""

import pandas as pd

from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
from src.modules.processing.reprocess_checker import (
    check_reprocess_targets,
    load_raw_data_from_sheets,
    clear_classified_at_for_targets,
    print_reprocess_stats,
)
from src.modules.export.sheets import (
    load_existing_links_from_sheets,
    filter_new_articles_from_sheets,
    sync_raw_and_processed,
)


def run_collection(ctx) -> bool:
    """
    Collect news, check reprocess targets, filter by date.

    Sets ctx.df_raw, ctx.df_raw_new, ctx.df_to_process.
    Returns False if no articles to process (triggers early exit with sync).
    """
    ctx.current_stage = "collection"
    ctx.logger.start_stage("collection")
    print("\n" + "=" * 80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("=" * 80)

    # Load existing links from Sheets (for deduplication)
    existing_links = set()
    if ctx.spreadsheet:
        existing_links = load_existing_links_from_sheets(ctx.spreadsheet)

    if ctx.args.recheck_only:
        _collect_recheck_only(ctx)
    else:
        _collect_from_api(ctx, existing_links)

    # STEP 1.5a: Sheets íƒ­ ë‚´ ì¤‘ë³µ í–‰ ì œê±°
    _dedup_sheets(ctx)

    # STEP 1.5: ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²€ì‚¬
    _reprocess_check(ctx)

    # STEP 1.6: 2026-02-01 ì´í›„ ê¸°ì‚¬ë§Œ í•„í„°ë§
    _date_filter(ctx)

    if len(ctx.df_to_process) == 0:
        _handle_no_articles(ctx)
        return False

    return True


def _collect_recheck_only(ctx):
    """--recheck_only ëª¨ë“œ: API ìˆ˜ì§‘ ìƒëµ, Sheetsì—ì„œ raw_data ë¡œë“œ"""
    if not ctx.spreadsheet:
        print("âŒ --recheck_only ì‚¬ìš© ì‹œ Google Sheets ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        raise RuntimeError("--recheck_only requires Google Sheets connection")

    print("\nğŸ“‹ --recheck_only ëª¨ë“œ: API ìˆ˜ì§‘ ìƒëµ, Sheets raw_data ë¡œë“œ")
    ctx.df_raw = load_raw_data_from_sheets(ctx.spreadsheet)
    if len(ctx.df_raw) == 0:
        print("âŒ Sheets raw_dataê°€ ë¹„ì–´ìˆì–´ ì¬ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise RuntimeError("--recheck_only requires non-empty raw_data in Sheets")

    ctx.df_raw_new = pd.DataFrame(columns=ctx.df_raw.columns)

    ctx.logger.log_dict({
        "articles_collected_total": 0,
        "articles_collected_per_query": {},
        "existing_links_skipped": 0,
    })
    ctx.logger.end_stage("collection")


def _collect_from_api(ctx, existing_links):
    """API ë°©ì‹ ìˆ˜ì§‘ (Sheets ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)"""
    ctx.df_raw_new = collect_all_news(
        OUR_BRANDS, COMPETITORS,
        ctx.args.display, ctx.args.max_api_pages, ctx.args.sort,
        ctx.env["naver_id"], ctx.env["naver_secret"],
        existing_links=existing_links,
        spreadsheet=ctx.spreadsheet
    )

    # API ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
    if len(ctx.df_raw_new) == 0:
        print("\nâ„¹ï¸  APIì—ì„œ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ… APIì—ì„œ {len(ctx.df_raw_new)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    # Filter new articles (skip duplicates from Google Sheets)
    existing_links_skipped = 0
    if len(existing_links) > 0 and len(ctx.df_raw_new) > 0:
        before_filter = len(ctx.df_raw_new)
        ctx.df_raw_new = filter_new_articles_from_sheets(ctx.df_raw_new, existing_links)
        existing_links_skipped = before_filter - len(ctx.df_raw_new)

    # df_raw: ì „ì²´ (Sheets raw_data + ì‹ ê·œ ê¸°ì‚¬) ë©”ëª¨ë¦¬ì—ì„œ ë³‘í•©
    if ctx.spreadsheet:
        df_raw_sheets = load_raw_data_from_sheets(ctx.spreadsheet)
        if len(df_raw_sheets) > 0:
            ctx.df_raw = pd.concat([df_raw_sheets, ctx.df_raw_new], ignore_index=True)
            ctx.df_raw = ctx.df_raw.drop_duplicates(subset=['link'], keep='last')
            print(f"ğŸ“‚ Sheets raw_data + ì‹ ê·œ: {len(df_raw_sheets)} + {len(ctx.df_raw_new)} = {len(ctx.df_raw)}ê°œ ê¸°ì‚¬")
        else:
            ctx.df_raw = ctx.df_raw_new
    else:
        ctx.df_raw = ctx.df_raw_new

    # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ìˆ˜ì§‘ ì§í›„) â€” _save_immediatelyì—ì„œ ì´ë¯¸ ë™ê¸°í™”ë¨
    if ctx.spreadsheet and len(ctx.df_raw_new) > 0:
        print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (raw_data)...")
        try:
            from src.modules.export.sheets import sync_to_sheets
            sync_result = sync_to_sheets(ctx.df_raw, ctx.spreadsheet, "raw_data")
            msg_parts = [
                f"{sync_result.get('attempted', 0)}ê°œ ì‹œë„",
                f"{sync_result.get('added', 0)}ê°œ ì¶”ê°€"
            ]
            if sync_result.get('updated', 0) > 0:
                msg_parts.append(f"{sync_result['updated']}ê°œ ì—…ë°ì´íŠ¸")
            msg_parts.append(f"{sync_result.get('skipped', 0)}ê°œ ê±´ë„ˆëœ€")
            print(f"âœ… raw_data ì‹œíŠ¸ ë™ê¸°í™” ì™„ë£Œ: {', '.join(msg_parts)}")
            ctx.logger.log_dict({
                "sheets_rows_uploaded_raw": sync_result.get("added", 0) + sync_result.get("updated", 0),
            })
            ctx.logger.log_event("sheets_sync_raw_data", sync_result, category="sheets_sync", stage="collection")
        except Exception as e:
            ctx.record_error(f"raw_data ì‹œíŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

    # ìˆ˜ì§‘ ë‹¨ê³„ ë©”íŠ¸ë¦­
    articles_per_query = ctx.df_raw_new.groupby('query').size().to_dict() if 'query' in ctx.df_raw_new.columns else {}
    ctx.logger.log_dict({
        "articles_collected_total": len(ctx.df_raw_new),
        "articles_collected_per_query": articles_per_query,
        "existing_links_skipped": existing_links_skipped
    })
    ctx.logger.log_event("collection_completed", {
        "articles_collected_total": len(ctx.df_raw_new),
        "articles_collected_per_query": articles_per_query,
        "existing_links_skipped": existing_links_skipped
    }, stage="collection")
    if ctx.spreadsheet:
        ctx.logger.flush_all_to_sheets(ctx.spreadsheet)
    ctx.logger.end_stage("collection")


def _dedup_sheets(ctx):
    """STEP 1.5a: Sheets íƒ­(raw_data, total_result) ë‚´ ì¤‘ë³µ í–‰ ì œê±°."""
    if not ctx.spreadsheet:
        return
    from src.modules.export.sheets import deduplicate_sheet

    print("\nğŸ§¹ Sheets ì¤‘ë³µ í–‰ ê²€ì‚¬ ì¤‘...")
    for tab in ("raw_data", "total_result"):
        try:
            result = deduplicate_sheet(ctx.spreadsheet, tab)
            if result["removed"] > 0:
                print(f"  âœ… {tab}: ì¤‘ë³µ {result['removed']}ê°œ ì œê±° ({result['before']} â†’ {result['after']}ê°œ)")
            else:
                print(f"  âœ… {tab}: ì¤‘ë³µ ì—†ìŒ ({result['after']}ê°œ)")
        except Exception as e:
            ctx.record_error(f"{tab} ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}", category="sheets_sync")


def _reprocess_check(ctx):
    """STEP 1.5: ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²€ì‚¬"""
    try:
        recheck = check_reprocess_targets(ctx.df_raw, ctx.spreadsheet)
        print_reprocess_stats(recheck["stats"])

        df_reprocess = recheck["df_to_reprocess"]
        if len(df_reprocess) > 0:
            df_reprocess = clear_classified_at_for_targets(df_reprocess, recheck["reprocess_links"])
            ctx.df_to_process = pd.concat([ctx.df_raw_new, df_reprocess], ignore_index=True)
            ctx.df_to_process = ctx.df_to_process.drop_duplicates(subset=['link'], keep='first')
        else:
            ctx.df_to_process = ctx.df_raw_new

        # ì¬ì²˜ë¦¬ ë©”íŠ¸ë¦­ ë¡œê¹…
        ctx.logger.log_dict({
            "reprocess_targets_total": recheck["stats"]["total_reprocess_targets"],
            "reprocess_missing_from_result": recheck["stats"]["missing_from_result"],
            "reprocess_field_missing": recheck["stats"].get("field_missing", {}),
        })
    except Exception as e:
        ctx.record_error(f"ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²€ì‚¬ ì‹¤íŒ¨: {e}", category="system")
        ctx.df_to_process = ctx.df_raw_new


def _date_filter(ctx):
    """STEP 1.6: 2026-02-01 ì´í›„ ê¸°ì‚¬ë§Œ í•„í„°ë§"""
    if len(ctx.df_to_process) > 0 and 'pubDate' in ctx.df_to_process.columns:
        before_date_filter = len(ctx.df_to_process)
        ctx.df_to_process['pub_datetime_temp'] = pd.to_datetime(ctx.df_to_process['pubDate'], errors='coerce')
        ctx.df_to_process = ctx.df_to_process[ctx.df_to_process['pub_datetime_temp'] >= '2026-02-01'].copy()
        ctx.df_to_process = ctx.df_to_process.drop(columns=['pub_datetime_temp'])
        date_filtered = before_date_filter - len(ctx.df_to_process)
        print(f"ğŸ”§ ë‚ ì§œ í•„í„°ë§: {date_filtered}ê°œ ì œì™¸ (2026-02-01 ì´ì „), {len(ctx.df_to_process)}ê°œ ìœ ì§€")


def _handle_no_articles(ctx):
    """ì²˜ë¦¬í•  ê¸°ì‚¬ ì—†ì„ ë•Œ ê¸°ì¡´ ë°ì´í„° Sheets ë™ê¸°í™” í›„ early exit"""
    print("â„¹ï¸  ì²˜ë¦¬í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ Google Sheets ë™ê¸°í™” ì‹œë„
    if ctx.spreadsheet:
        ctx.logger.start_stage("sheets_sync")
        print("\n" + "=" * 80)
        print("STEP 5: Google Sheets ì—…ë¡œë“œ (ê¸°ì¡´ ë°ì´í„°)")
        print("=" * 80)
        try:
            # Sheets total_result ë¡œë“œ
            worksheet = ctx.spreadsheet.worksheet("total_result")
            records = worksheet.get_all_records()
            if records:
                df_result_existing = pd.DataFrame(records)
                sync_results = sync_raw_and_processed(ctx.df_raw, df_result_existing, ctx.spreadsheet)
                print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")

                raw_sync = sync_results.get("raw_data", {})
                result_sync = sync_results.get("total_result", {})
                ctx.logger.log_dict({
                    "sheets_sync_enabled": True,
                    "sheets_rows_uploaded_raw": raw_sync.get("added", 0) + raw_sync.get("updated", 0),
                    "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0)
                })
            else:
                print("  â„¹ï¸  total_result ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                ctx.logger.log("sheets_sync_enabled", True)
        except Exception as e:
            ctx.record_error(f"Google Sheets ì—…ë¡œë“œ ì‹¤íŒ¨ (ê¸°ì¡´ ë°ì´í„°): {e}", category="sheets_sync")
            ctx.logger.log("sheets_sync_enabled", False)
        ctx.logger.end_stage("sheets_sync")
        ctx.logger.log_event("sheets_sync_completed", {
            "sheets_rows_uploaded_raw": ctx.logger.metrics.get("sheets_rows_uploaded_raw", 0),
            "sheets_rows_uploaded_result": ctx.logger.metrics.get("sheets_rows_uploaded_result", 0)
        }, category="sheets_sync", stage="sheets_sync")
        ctx.logger.flush_all_to_sheets(ctx.spreadsheet)

    print("\n" + "=" * 80)
    print("âœ… ì‘ì—… ì™„ë£Œ (ì‹ ê·œ ì²˜ë¦¬ ì—†ìŒ)")
    print("=" * 80)
    # ë¡œê·¸/ìš”ì•½/ë°°ë„ˆëŠ” main.pyì—ì„œ finalize_pipeline()ì´ ì²˜ë¦¬í•¨
