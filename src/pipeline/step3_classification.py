"""Step 3: LLM classification (PR clusters, general, source verification)."""

import pandas as pd

from src.modules.analysis.classify_press_releases import classify_press_releases
from src.modules.analysis.classify_llm import classify_llm
from src.modules.analysis.source_verifier import verify_and_regroup_sources
from src.modules.analysis.reasoning_writer import ReasoningCollector
from src.modules.processing.press_release_detector import summarize_clusters
from src.modules.processing.looker_prep import add_time_series_columns
from src.modules.analysis.classification_stats import get_classification_stats, print_classification_stats
from src.modules.export.sheets import sync_raw_and_processed


def run_classification(ctx):
    """Run all classification steps. Sets ctx.df_result."""
    if ctx.run_mode == "raw_only":
        return  # Already set in step2

    if ctx.run_mode == "preprocess_only":
        _handle_preprocess_only(ctx)
        return

    # Full classification pipeline
    print("\n" + "=" * 80)
    print("STEP 3: ë¶„ë¥˜")
    print("=" * 80)

    # ReasoningCollector ì´ˆê¸°í™” (Sheetsê°€ ì—°ê²°ëœ ê²½ìš°ì—ë§Œ ìˆ˜ì§‘)
    ctx.reasoning_collector = ReasoningCollector() if ctx.spreadsheet else None

    # Step 3-1: ë³´ë„ìë£Œ LLM ë¶„ë¥˜ (ëŒ€í‘œ ê¸°ì‚¬ ë¶„ì„ -> í´ëŸ¬ìŠ¤í„° ê³µìœ )
    _classify_press_releases(ctx)

    # Step 3-2: LLM ë¶„ë¥˜ (ë³´ë„ìë£ŒëŠ” ìŠ¤í‚µ)
    _classify_general(ctx)

    # í†µê³„ ì¶œë ¥
    stats = get_classification_stats(ctx.df_processed)
    print_classification_stats(stats)

    # ë³´ë„ìë£Œ ì •ë³´ ë° ì–¸ë¡ ì‚¬ ì •ë³´ ë³‘í•©
    _merge_source_columns(ctx)

    # Step 3-3: Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™”
    if not ctx.args.dry_run:
        _verify_sources(ctx)

        # Step 3-3 ì´í›„: ìœ ì‚¬ì£¼ì œ í´ëŸ¬ìŠ¤í„° ìš”ì•½ (ì´ë¯¸ ìš”ì•½ëœ ë³´ë„ìë£Œ í´ëŸ¬ìŠ¤í„°ëŠ” ìŠ¤í‚µ)
        ctx.df_processed = summarize_clusters(ctx.df_processed, ctx.env["openai_key"])

    # Step 3.7: Looker Studio ì¤€ë¹„ (í•­ìƒ ì‹¤í–‰)
    _add_looker_columns(ctx)

    ctx.logger.log_event("classification_completed", {
        "articles_classified_llm": ctx.logger.metrics.get("articles_classified_llm", 0),
        "llm_api_calls": ctx.logger.metrics.get("llm_api_calls", 0),
        "press_releases_skipped": ctx.logger.metrics.get("press_releases_skipped", 0),
        "classification_errors": ctx.logger.metrics.get("classification_errors", 0)
    }, stage="general_classification")
    if ctx.spreadsheet:
        ctx.logger.flush_all_to_sheets(ctx.spreadsheet)

    # ê¸°ì¡´ result.csvì™€ ë³‘í•©
    _merge_with_existing_result(ctx)

    # ê²°ê³¼ ì €ì¥
    _save_and_sync(ctx)

    # reasoning íƒ­ ì—…ë¡œë“œ (Sheets ì—°ê²° ì‹œ)
    if ctx.reasoning_collector is not None and ctx.spreadsheet:
        print("\nğŸ“ reasoning íƒ­ ì—…ë¡œë“œ ì¤‘...")
        ctx.reasoning_collector.flush_to_sheets(ctx.spreadsheet)


def _classify_press_releases(ctx):
    """Step 3-1: Press release cluster classification"""
    ctx.current_stage = "pr_classification"
    ctx.logger.start_stage("pr_classification")
    ctx.df_processed, pr_metrics = classify_press_releases(
        ctx.df_processed,
        ctx.env["openai_key"],
        chunk_size=ctx.args.chunk_size,
        max_workers=ctx.args.max_workers,
        spreadsheet=ctx.spreadsheet,
        raw_df=ctx.df_raw,
        reasoning_collector=ctx.reasoning_collector,
    )
    ctx.logger.log_dict(pr_metrics)
    ctx.logger.end_stage("pr_classification")
    ctx.logger.log_event("pr_classification_completed", pr_metrics, stage="pr_classification")
    if ctx.spreadsheet:
        ctx.logger.flush_all_to_sheets(ctx.spreadsheet)


def _classify_general(ctx):
    """Step 3-2: General LLM classification"""
    ctx.current_stage = "general_classification"
    ctx.logger.start_stage("general_classification")
    ctx.df_processed, llm_metrics = classify_llm(
        ctx.df_processed,
        ctx.env["openai_key"],
        chunk_size=ctx.args.chunk_size,
        dry_run=ctx.args.dry_run,
        max_competitor_classify=ctx.args.max_competitor_classify,
        max_workers=ctx.args.max_workers,
        spreadsheet=ctx.spreadsheet,
        raw_df=ctx.df_raw,
        reasoning_collector=ctx.reasoning_collector,
    )
    ctx.logger.log_dict(llm_metrics)
    ctx.logger.end_stage("general_classification")


def _merge_source_columns(ctx):
    """Merge source/media columns from preprocessing"""
    source_cols = ['link', 'source', 'cluster_id', 'cluster_summary', 'media_domain', 'media_name', 'media_group',
                   'media_type']
    source_data = ctx.df_processed[source_cols].copy()

    # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    cols_to_drop = [col for col in ctx.df_processed.columns if col in source_cols and col != 'link']
    ctx.df_processed = ctx.df_processed.drop(columns=cols_to_drop, errors='ignore')

    # merge
    ctx.df_processed = ctx.df_processed.merge(source_data, on='link', how='left')

    # ë‚˜ë¨¸ì§€ NaN -> ê³µë€ ë³€í™˜ (FutureWarning ë°©ì§€)
    ctx.df_processed = ctx.df_processed.fillna("").infer_objects(copy=False)


def _verify_sources(ctx):
    """Step 3-3: Source verification and topic grouping"""
    ctx.current_stage = "source_verification"
    ctx.logger.start_stage("source_verification")
    ctx.df_processed, sv_metrics = verify_and_regroup_sources(ctx.df_processed, openai_key=ctx.env["openai_key"])
    ctx.logger.log_dict(sv_metrics)
    ctx.logger.end_stage("source_verification")
    ctx.logger.log_event("source_verification_completed", sv_metrics, stage="source_verification")
    if ctx.spreadsheet:
        ctx.logger.flush_all_to_sheets(ctx.spreadsheet)


def _add_looker_columns(ctx):
    """Add Looker Studio time-series columns"""
    print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
    ctx.df_processed = add_time_series_columns(ctx.df_processed)


def _merge_with_existing_result(ctx):
    """Merge with existing result from Sheets total_result"""
    if ctx.spreadsheet:
        try:
            worksheet = ctx.spreadsheet.worksheet("total_result")
            records = worksheet.get_all_records()
            if records:
                df_result_existing = pd.DataFrame(records)
                ctx.df_result = pd.concat([df_result_existing, ctx.df_processed], ignore_index=True)
                ctx.df_result = ctx.df_result.drop_duplicates(subset=['link'], keep='last')
                print(f"\nğŸ“‚ Sheets total_result ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(ctx.df_processed)} = {len(ctx.df_result)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ ì œê±° í›„)")
                return
        except Exception as e:
            ctx.record_error(f"ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨ (Sheets): {e}", category="sheets_sync")

    ctx.df_result = ctx.df_processed


def _save_and_sync(ctx):
    """Sync result to Google Sheets"""
    # pub_datetimeì„ ëª…ì‹œì ìœ¼ë¡œ datetimeìœ¼ë¡œ ë³€í™˜ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
    if 'pub_datetime' in ctx.df_result.columns:
        ctx.df_result['pub_datetime'] = pd.to_datetime(ctx.df_result['pub_datetime'], errors='coerce')

    # Google Sheets ë™ê¸°í™” (ë¶„ë¥˜ ì™„ë£Œ ì§í›„)
    if ctx.spreadsheet:
        print("\nğŸ“Š Google Sheets ë™ê¸°í™” ì¤‘ (ë¶„ë¥˜ ê²°ê³¼)...")
        try:
            sync_results = sync_raw_and_processed(ctx.df_raw, ctx.df_result, ctx.spreadsheet)
            result_sync = sync_results.get("total_result", {})
            ctx.logger.log_dict({
                "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0),
            })
            print("âœ… ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
        except Exception as e:
            ctx.record_error(f"ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")


def _handle_preprocess_only(ctx):
    """Handle preprocess_only mode: add looker columns, merge, sync to Sheets"""
    # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜
    ctx.df_processed = ctx.df_processed.fillna("")

    # Step 2-6: Looker Studio time-series columns
    print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
    ctx.df_processed = add_time_series_columns(ctx.df_processed)

    # ê¸°ì¡´ Sheets total_resultì™€ ë³‘í•©
    if ctx.spreadsheet:
        try:
            worksheet = ctx.spreadsheet.worksheet("total_result")
            records = worksheet.get_all_records()
            if records:
                df_result_existing = pd.DataFrame(records)
                ctx.df_result = pd.concat([df_result_existing, ctx.df_processed], ignore_index=True)
                ctx.df_result = ctx.df_result.drop_duplicates(subset=['link'], keep='last')
                print(f"ğŸ“‚ Sheets total_result ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(ctx.df_processed)} = {len(ctx.df_result)}ê°œ ê¸°ì‚¬")
            else:
                ctx.df_result = ctx.df_processed
        except Exception:
            ctx.df_result = ctx.df_processed
    else:
        ctx.df_result = ctx.df_processed

    # total_result_count
    ctx.logger.log("total_result_count", len(ctx.df_result))

    # Google Sheets ë™ê¸°í™” (ì „ì²˜ë¦¬ ì™„ë£Œ ì§í›„)
    if ctx.spreadsheet:
        print("\nğŸ“Š Google Sheets ë™ê¸°í™” ì¤‘ (ì „ì²˜ë¦¬ ê²°ê³¼)...")
        try:
            sync_raw_and_processed(ctx.df_raw, ctx.df_result, ctx.spreadsheet)
            print("âœ… ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
        except Exception as e:
            ctx.record_error(f"ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")
