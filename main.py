#!/usr/bin/env python3
"""
main.py - News Monitoring System
ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import os
import argparse
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# Pandas FutureWarning ë°©ì§€
pd.set_option('future.no_silent_downcasting', True)

# ëª¨ë“ˆ ì„í¬íŠ¸
from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
from src.modules.processing.process import (
    normalize_df, dedupe_df, detect_similar_articles,
    enrich_with_media_info, save_csv, summarize_press_release_groups
)
from src.modules.analysis.classify import classify_all
from src.modules.analysis.hybrid import classify_hybrid, get_classification_stats, print_classification_stats
from src.modules.export.report import generate_console_report, create_word_report
from src.modules.collection.scrape import collect_with_scraping, merge_api_and_scrape
from src.modules.processing.fulltext import batch_fetch_full_text
from src.modules.processing.looker_prep import add_time_series_columns
from src.modules.export.sheets import (
    connect_sheets, sync_raw_and_processed, load_existing_links_from_sheets, filter_new_articles_from_sheets
)


def load_env():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    load_dotenv()

    naver_id = os.getenv("NAVER_CLIENT_ID")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET")
    openai_key = os.getenv("OPENAI_API_KEY")

    if not naver_id or not naver_secret:
        raise ValueError("âŒ .env íŒŒì¼ì— NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRETì´ ì—†ìŠµë‹ˆë‹¤")
    if not openai_key:
        raise ValueError("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤")

    return {
        "naver_id": naver_id,
        "naver_secret": naver_secret,
        "openai_key": openai_key
    }


def main():
    parser = argparse.ArgumentParser(
        description="ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                                    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Sheets ìë™ ë™ê¸°í™”)
  python main.py --display 200                      # API ê²°ê³¼ ê°œìˆ˜ ì§€ì •
  python main.py --scrape --start_date 2026-01-01   # ë‚ ì§œ ë²”ìœ„ ìŠ¤í¬ë˜í•‘
  python main.py --fulltext --fulltext_risk_levels ìƒ,ì¤‘  # ì „ë¬¸ ìŠ¤í¬ë˜í•‘
  python main.py --sheets_id YOUR_SHEET_ID          # Google Sheets ID ì§€ì •
  python main.py --raw_only                         # ìˆ˜ì§‘ë§Œ (Sheets ìë™ ë™ê¸°í™”)

ì£¼ì˜:
  - Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤ (credentials ì„¤ì • ê¶Œì¥)
  - CSV íŒŒì¼ì€ troubleshooting ìš©ë„ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤
  - .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_ID ì„¤ì • í•„ìš”
        """
    )
    # ê¸°ì¡´ ì˜µì…˜
    parser.add_argument("--display", type=int, default=100,
                        help="ë„¤ì´ë²„ APIì—ì„œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 100)")
    parser.add_argument("--start", type=int, default=1,
                        help="ë„¤ì´ë²„ API ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸: 1)")
    parser.add_argument("--sort", type=str, default="date", choices=["date", "sim"],
                        help="ì •ë ¬ ë°©ì‹: date(ìµœì‹ ìˆœ) ë˜ëŠ” sim(ê´€ë ¨ë„ìˆœ) (ê¸°ë³¸: date)")
    parser.add_argument("--outdir", type=str, default="data",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)")
    parser.add_argument("--max_competitor_classify", type=int, default=20,
                        help="ê²½ìŸì‚¬ë³„ ë¶„ë¥˜í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--chunk_size", type=int, default=100,
                        help="AI ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 100)")
    parser.add_argument("--max_workers", type=int, default=10,
                        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 10, ê¶Œì¥: 5-15)")
    parser.add_argument("--dry_run", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # ìŠ¤í¬ë˜í•‘ ì˜µì…˜
    parser.add_argument("--scrape", action="store_true",
                        help="Naver ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜)")
    parser.add_argument("--start_date", type=str, default="2026-01-01",
                        help="ìŠ¤í¬ë˜í•‘ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: 2026-01-01)")
    parser.add_argument("--end_date", type=str, default="2026-02-07",
                        help="ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: 2026-02-07)")
    parser.add_argument("--max_scrape_pages", type=int, default=10,
                        help="ìŠ¤í¬ë˜í•‘ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 10)")

    # ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ì˜µì…˜
    parser.add_argument("--fulltext", action="store_true",
                        help="ê¸°ì‚¬ ì „ë¬¸ ìŠ¤í¬ë˜í•‘")
    parser.add_argument("--fulltext_risk_levels", type=str, default="ìƒ,ì¤‘",
                        help="ì „ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•  ìœ„í—˜ë„ (ê¸°ë³¸: ìƒ,ì¤‘)")
    parser.add_argument("--fulltext_max_articles", type=int, default=None,
                        help="ìµœëŒ€ ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: ë¬´ì œí•œ)")

    # Looker ì¤€ë¹„ ì˜µì…˜
    parser.add_argument("--looker_prep", action="store_true",
                        help="Looker Studioìš© ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€")

    # Google Sheets ì˜µì…˜ (í•­ìƒ í™œì„±í™”ë¨)
    parser.add_argument("--sheets_id", type=str, default=None,
                        help="Google Sheets ID (.envì˜ GOOGLE_SHEET_ID ëŒ€ì‹  ì‚¬ìš©)")

    # API í˜ì´ì§€ë„¤ì´ì…˜ ì˜µì…˜
    parser.add_argument("--max_api_pages", type=int, default=9,
                        help="API í˜ì´ì§€ë„¤ì´ì…˜ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 9, ì¿¼í„° 90%% ì•ˆì „ ë§ˆì§„)")

    # Raw only ì˜µì…˜
    parser.add_argument("--raw_only", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ API ìˆ˜ì§‘ + Google Sheets ì—…ë¡œë“œë§Œ ì‹¤í–‰")

    # Preprocess only ì˜µì…˜
    parser.add_argument("--preprocess_only", action="store_true",
                        help="ìˆ˜ì§‘ + ì „ì²˜ë¦¬ê¹Œì§€ë§Œ ì‹¤í–‰ (AI ë¶„ë¥˜, ë¦¬í¬íŠ¸ ìƒëµ, Sheets ì—…ë¡œë“œëŠ” ì‹¤í–‰)")

    # Legacy classify ì˜µì…˜
    parser.add_argument("--legacy_classify", action="store_true",
                        help="ë ˆê±°ì‹œ ë¶„ë¥˜ ì‹œìŠ¤í…œ ì‚¬ìš© (ê¸°ë³¸: í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)")

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - ìš°ë¦¬ ë¸Œëœë“œ: {', '.join(OUR_BRANDS)}")
    print(f"  - ê²½ìŸì‚¬: {', '.join(COMPETITORS)}")
    if args.scrape:
        print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: API + ìŠ¤í¬ë˜í•‘ ({args.start_date} ~ {args.end_date})")
    else:
        print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: APIë§Œ")
        print(f"  - ê¸°ì‚¬ ìˆ˜: {args.display}ê°œ/ë¸Œëœë“œ (ìµœëŒ€ {args.max_api_pages} í˜ì´ì§€)")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.outdir}/")
    print(f"  - AI ì²­í¬ í¬ê¸°: {args.chunk_size}")
    print(f"  - ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤: {args.max_workers}ê°œ")
    if args.fulltext:
        print(f"  - ì „ë¬¸ ìŠ¤í¬ë˜í•‘: {args.fulltext_risk_levels} (ìœ„í—˜ë„)")
    if args.looker_prep:
        print(f"  - Looker ì¤€ë¹„: í™œì„±í™”")
    if args.dry_run:
        print(f"  - ëª¨ë“œ: DRY RUN (AI ë¶„ë¥˜ ìƒëµ)")
    if args.raw_only:
        print(f"  - ëª¨ë“œ: RAW ONLY (API ìˆ˜ì§‘ + Sheets ì—…ë¡œë“œë§Œ)")
    if args.preprocess_only:
        print(f"  - ëª¨ë“œ: PREPROCESS ONLY (ìˆ˜ì§‘ + ì „ì²˜ë¦¬ + Sheets ì—…ë¡œë“œ)")
    print()

    # Step 0: í™˜ê²½ ì„¤ì •
    env = load_env()
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    media_csv_path = outdir / "media_directory.csv"

    # Step 1: ìˆ˜ì§‘
    print("\n" + "=" * 80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("=" * 80)

    # Google Sheets ì—°ê²° (ì£¼ ì €ì¥ì†Œ)
    existing_links = set()
    spreadsheet = None

    # Google Sheets ìë™ ì—°ê²° (credentials í•„ìˆ˜ ê¶Œì¥)
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json")
    sheet_id = args.sheets_id or os.getenv("GOOGLE_SHEET_ID")

    if os.path.exists(creds_path) and sheet_id:
        try:
            print("\nğŸ“Š Google Sheets ì—°ê²° ì¤‘...")
            spreadsheet = connect_sheets(creds_path, sheet_id)
            if spreadsheet:
                existing_links = load_existing_links_from_sheets(spreadsheet)
                print("âœ… Google Sheets ì—°ê²° ì„±ê³µ (ì£¼ ì €ì¥ì†Œ)")
                print("   CSV íŒŒì¼ì€ troubleshootingìš©ìœ¼ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸  Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
    else:
        print("\n" + "="*80)
        print("âš ï¸  ê²½ê³ : Google Sheets ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("="*80)
        if not os.path.exists(creds_path):
            print(f"  credential íŒŒì¼ ì—†ìŒ: {creds_path}")
        if not sheet_id:
            print(f"  GOOGLE_SHEET_ID í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
        print("\n  Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤. ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print("  í˜„ì¬ëŠ” CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("="*80 + "\n")

    raw_csv_path = outdir / "raw.csv"

    if args.scrape:
        # ìŠ¤í¬ë˜í•‘ ë°©ì‹
        df_scrape = collect_with_scraping(
            OUR_BRANDS, COMPETITORS,
            args.start_date, args.end_date,
            args.max_scrape_pages
        )
        # API ë°©ì‹ë„ ë™ì‹œì— ìˆ˜ì§‘ í›„ ë³‘í•© (raw.csv ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)
        df_api = collect_all_news(
            OUR_BRANDS, COMPETITORS,
            args.display, args.max_api_pages, args.sort,
            env["naver_id"], env["naver_secret"],
            raw_csv_path=str(raw_csv_path)
        )
        df_raw_new = merge_api_and_scrape(df_api, df_scrape)
    else:
        # API ë°©ì‹ë§Œ (raw.csv ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)
        df_raw_new = collect_all_news(
            OUR_BRANDS, COMPETITORS,
            args.display, args.max_api_pages, args.sort,
            env["naver_id"], env["naver_secret"],
            raw_csv_path=str(raw_csv_path)
        )

    # API ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
    if len(df_raw_new) == 0:
        print("\nâ„¹ï¸  APIì—ì„œ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ… APIì—ì„œ {len(df_raw_new)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    # Filter new articles (skip duplicates from Google Sheets)
    if len(existing_links) > 0 and len(df_raw_new) > 0:
        df_raw_new = filter_new_articles_from_sheets(df_raw_new, existing_links)

    # raw.csv ì—…ë°ì´íŠ¸ (append)
    if raw_csv_path.exists():
        df_raw_existing = pd.read_csv(raw_csv_path, encoding='utf-8-sig')
        df_raw = pd.concat([df_raw_existing, df_raw_new], ignore_index=True)
        df_raw = df_raw.drop_duplicates(subset=['link'], keep='last')
        print(f"ğŸ“‚ ê¸°ì¡´ raw.csv ì—…ë°ì´íŠ¸: {len(df_raw_existing)} + {len(df_raw_new)} = {len(df_raw)}ê°œ ê¸°ì‚¬")
    else:
        df_raw = df_raw_new

    save_csv(df_raw, raw_csv_path)

    # STEP 1.5: ë¯¸ì²˜ë¦¬/ë¯¸ë¶„ì„ í–‰ í•„í„°ë§ (result.csv ê¸°ì¤€)
    result_csv_path = outdir / "result.csv"
    df_to_process = df_raw_new  # ê¸°ë³¸: ì‹ ê·œ ìˆ˜ì§‘ ê¸°ì‚¬ë§Œ ì²˜ë¦¬

    if result_csv_path.exists():
        try:
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')

            # 1. Link ê¸°ì¤€ ë¯¸ì²˜ë¦¬ í–‰ (ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬)
            if 'link' in df_result_existing.columns:
                processed_links = set(df_result_existing['link'].dropna().tolist())
                unprocessed_rows = df_raw[~df_raw['link'].isin(processed_links)]
            else:
                unprocessed_rows = df_raw

            # 2. ë¶„ì„ í•„ë“œ ë¹„ì–´ìˆëŠ” í–‰ (ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ ë¶„ì„ ì•ˆ ëœ ê²ƒ)
            analysis_cols_to_check = ['sentiment_final', 'danger_final', 'issue_category_final']
            if all(col in df_result_existing.columns for col in analysis_cols_to_check):
                # ë¶„ì„ í•„ë“œê°€ ëª¨ë‘ ë¹„ì–´ìˆëŠ” í–‰ ì°¾ê¸°
                missing_analysis = df_result_existing[
                    df_result_existing['sentiment_final'].isna() |
                    (df_result_existing['sentiment_final'] == "")
                ].copy()

                if len(missing_analysis) > 0:
                    # raw.csvì—ì„œ í•´ë‹¹ ë§í¬ë“¤ ì°¾ê¸°
                    missing_links = set(missing_analysis['link'].dropna().tolist())
                    reanalyze_rows = df_raw[df_raw['link'].isin(missing_links)]

                    # ë¯¸ì²˜ë¦¬ í–‰ + ì¬ë¶„ì„ í–‰ í•©ì¹˜ê¸°
                    df_to_process = pd.concat([unprocessed_rows, reanalyze_rows], ignore_index=True)
                    df_to_process = df_to_process.drop_duplicates(subset=['link'], keep='first')

                    print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                    print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                    print(f"  - ì‹ ê·œ ë¯¸ì²˜ë¦¬: {len(unprocessed_rows)}ê°œ")
                    print(f"  - ë¶„ì„ ëˆ„ë½: {len(reanalyze_rows)}ê°œ")
                    print(f"  - ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(df_to_process)}ê°œ")
                else:
                    df_to_process = unprocessed_rows
                    print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                    print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                    print(f"  - ì´ë¯¸ ì²˜ë¦¬ë¨: {len(processed_links)}ê°œ")
                    print(f"  - ë¯¸ì²˜ë¦¬ í–‰: {len(unprocessed_rows)}ê°œ")
            else:
                df_to_process = unprocessed_rows
                print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                print(f"  - ë¯¸ì²˜ë¦¬ í–‰: {len(unprocessed_rows)}ê°œ")

        except Exception as e:
            print(f"âš ï¸  result.csv ë¡œë“œ ì‹¤íŒ¨: {e}, ì „ì²´ ì²˜ë¦¬ ì§„í–‰")
            df_to_process = df_raw
    else:
        print(f"\nğŸ“Š result.csvê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ raw.csv {len(df_raw)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
        df_to_process = df_raw  # ì „ì²´ raw.csv ì²˜ë¦¬

    if len(df_to_process) == 0:
        print("â„¹ï¸  ì²˜ë¦¬í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ Google Sheets ë™ê¸°í™” ì‹œë„
        if spreadsheet and result_csv_path.exists():
            print("\n" + "=" * 80)
            print("STEP 5: Google Sheets ì—…ë¡œë“œ (ê¸°ì¡´ ë°ì´í„°)")
            print("=" * 80)
            try:
                df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                sync_raw_and_processed(df_raw, df_result_existing, spreadsheet)
                print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  Google Sheets ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        print("\n" + "=" * 80)
        print("âœ… ì‘ì—… ì™„ë£Œ (ì‹ ê·œ ì²˜ë¦¬ ì—†ìŒ)")
        print("=" * 80)
        return

    # --raw_only ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬/ë¶„ë¥˜/ë¦¬í¬íŠ¸ ìŠ¤í‚µ
    if args.raw_only:
        df_result = df_raw
    # --preprocess_only ëª¨ë“œì¸ ê²½ìš° ë¶„ë¥˜ ìŠ¤í‚µ, ì²˜ë¦¬+ë¦¬í¬íŠ¸ëŠ” ì‹¤í–‰
    elif args.preprocess_only:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)
        df_normalized = normalize_df(df_to_process)
        df_processed = dedupe_df(df_normalized)
        df_processed = detect_similar_articles(df_processed, similarity_threshold=0.8)

        # ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ìƒì„± (OpenAI)
        df_processed = summarize_press_release_groups(df_processed, env["openai_key"])

        # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (spreadsheet ìˆìœ¼ë©´ ìë™ ì‚¬ìš©)
        media_csv_path = outdir / "media_directory.csv"
        df_processed = enrich_with_media_info(
            df_processed,
            spreadsheet=spreadsheet,  # Noneì´ë©´ CSV-only ëª¨ë“œ
            openai_key=env["openai_key"],
            csv_path=media_csv_path
        )

        # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜
        df_processed = df_processed.fillna("")

        # Looker Studio ì¤€ë¹„ (í•­ìƒ ì‹¤í–‰)
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_processed = add_time_series_columns(df_processed)

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_result = pd.concat([df_result_existing, df_processed], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"ğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_processed)} = {len(df_result)}ê°œ ê¸°ì‚¬")
        else:
            df_result = df_processed

        # ê²°ê³¼ ì €ì¥
        save_csv(df_result, result_csv_path)
    else:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)
        df_normalized = normalize_df(df_to_process)
        df_processed = dedupe_df(df_normalized)
        df_processed = detect_similar_articles(df_processed, similarity_threshold=0.8)

        # ë³´ë„ìë£Œ ê·¸ë£¹ ìš”ì•½ ìƒì„± (OpenAI)
        df_processed = summarize_press_release_groups(df_processed, env["openai_key"])

        # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€ (spreadsheet ìˆìœ¼ë©´ ìë™ ì‚¬ìš©)
        media_csv_path = outdir / "media_directory.csv"
        df_processed = enrich_with_media_info(
            df_processed,
            spreadsheet=spreadsheet,  # Noneì´ë©´ CSV-only ëª¨ë“œ
            openai_key=env["openai_key"],
            csv_path=media_csv_path
        )

        # Step 3: ë¶„ë¥˜ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        print("\n" + "=" * 80)
        print("STEP 3: AI ë¶„ë¥˜")
        print("=" * 80)

        if args.legacy_classify:
            print("ğŸ“š ë ˆê±°ì‹œ ë¶„ë¥˜ ì‹œìŠ¤í…œ ì‚¬ìš© ì¤‘...")
            df_classified = classify_all(
                df_processed,
                env["openai_key"],
                args.max_competitor_classify,
                args.chunk_size,
                args.dry_run
            )
        else:
            print("ğŸ”¬ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ì‹œìŠ¤í…œ ì‚¬ìš© ì¤‘...")
            df_classified = classify_hybrid(
                df_processed,
                env["openai_key"],
                chunk_size=args.chunk_size,
                dry_run=args.dry_run,
                max_competitor_classify=args.max_competitor_classify,
                max_workers=args.max_workers,
                result_csv_path=str(result_csv_path)
            )

            # í†µê³„ ì¶œë ¥
            stats = get_classification_stats(df_classified)
            print_classification_stats(stats)

        # ë³´ë„ìë£Œ ì •ë³´ ë° ì–¸ë¡ ì‚¬ ì •ë³´ ë³‘í•©
        source_cols = ['link', 'source', 'group_id', 'press_release_group', 'media_domain', 'media_name', 'media_group',
                       'media_type']
        source_data = df_processed[source_cols].copy()

        # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        cols_to_drop = [col for col in df_classified.columns if col in source_cols and col != 'link']
        df_classified = df_classified.drop(columns=cols_to_drop, errors='ignore')

        # merge
        df_classified = df_classified.merge(source_data, on='link', how='left')

        # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜ (FutureWarning ë°©ì§€)
        df_classified = df_classified.fillna("").infer_objects(copy=False)

        # Step 3.5: ì „ë¬¸ ìŠ¤í¬ë˜í•‘ (ì„ íƒì )
        if args.fulltext:
            print("\n" + "=" * 80)
            print("STEP 3.5: ê¸°ì‚¬ ì „ë¬¸ ìŠ¤í¬ë˜í•‘")
            print("=" * 80)
            risk_levels = [r.strip() for r in args.fulltext_risk_levels.split(",")]
            df_classified = batch_fetch_full_text(
                df_classified,
                risk_levels=risk_levels,
                max_articles=args.fulltext_max_articles
            )

        # Step 3.7: Looker Studio ì¤€ë¹„ (í•­ìƒ ì‹¤í–‰)
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_classified = add_time_series_columns(df_classified)

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_result = pd.concat([df_result_existing, df_classified], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"\nğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_classified)} = {len(df_result)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ ì œê±° í›„)")
        else:
            df_result = df_classified

        # pub_datetimeì„ ëª…ì‹œì ìœ¼ë¡œ datetimeìœ¼ë¡œ ë³€í™˜ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
        if 'pub_datetime' in df_result.columns:
            df_result['pub_datetime'] = pd.to_datetime(df_result['pub_datetime'], errors='coerce')

        # ê²°ê³¼ ì €ì¥ (ë‹¨ì¼ CSV íŒŒì¼)
        save_csv(df_result, result_csv_path)

        # Step 4: ë¦¬í¬íŠ¸ ìƒì„±
        print("\n" + "=" * 80)
        print("STEP 4: ë¦¬í¬íŠ¸ ìƒì„±")
        print("=" * 80)

        # ì½˜ì†” ë¦¬í¬íŠ¸
        generate_console_report(df_result)

        # Word ë¦¬í¬íŠ¸
        word_path = outdir / "report.docx"
        create_word_report(df_result, word_path)

    # Step 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)
    if spreadsheet:
        print("\n" + "=" * 80)
        print("STEP 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)")
        print("=" * 80)
        try:
            sync_raw_and_processed(df_raw, df_result, spreadsheet)
            print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Google Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
    else:
        print("\n" + "=" * 80)
        print("âš ï¸  Google Sheets ì—°ê²° ì—†ìŒ")
        print("=" * 80)
        print("  ì£¼ ì €ì¥ì†Œì¸ Google Sheetsì— ë™ê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATH ë° GOOGLE_SHEET_ID ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    # ì™„ë£Œ
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    if spreadsheet:
        print(f"  â˜ï¸  Google Sheets - ë™ê¸°í™” ì™„ë£Œ (ì£¼ ì €ì¥ì†Œ)")
    print(f"  ğŸ“Š {outdir}/raw.csv - ì›ë³¸ ë°ì´í„° (troubleshooting)")
    if not args.raw_only:
        print(f"  ğŸ“Š {outdir}/result.csv - AI ë¶„ë¥˜ ê²°ê³¼ (troubleshooting)")
        print(f"  ğŸ“„ {outdir}/report.docx - Word ë¦¬í¬íŠ¸")
    if spreadsheet or not args.raw_only:
        print(f"  ğŸ“‚ {outdir}/media_directory.csv - ì–¸ë¡ ì‚¬ ë””ë ‰í† ë¦¬")
    if not spreadsheet:
        print(f"\n  âš ï¸  Google Sheets ë¯¸ì—°ê²°: CSV íŒŒì¼ë§Œ ì €ì¥ë¨")
    print()


if __name__ == "__main__":
    main()