#!/usr/bin/env python3
"""
main.py - News Monitoring System
ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import os
import argparse
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# ëª¨ë“ˆ ì„í¬íŠ¸
from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
from src.modules.processing.process import (
    normalize_df, dedupe_df, detect_similar_articles,
    enrich_with_media_info, save_excel
)
from src.modules.analysis.classify import classify_all
from src.modules.export.report import generate_console_report, create_word_report
from src.modules.collection.scrape import collect_with_scraping, merge_api_and_scrape
from src.modules.enhancement.fulltext import batch_fetch_full_text
from src.modules.enhancement.looker_prep import add_time_series_columns
from src.modules.export.sheets import (
    connect_sheets, sync_raw_and_processed, sync_all_sheets,
    load_existing_links_from_sheets, filter_new_articles_from_sheets
)


def load_env():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    load_dotenv()
    
    naver_id = os.getenv("NAVER_CLIENT_ID")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not naver_id or not naver_secret:
        raise ValueError("âŒ .env íŒŒì¼ì— NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRETì´ ì—†ìŠµë‹ˆë‹¤")
    if not openai_key:
        raise ValueError("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤")
    
    return {
        "naver_id": naver_id,
        "naver_secret": naver_secret,
        "openai_key": openai_key
    }


def main():
    parser = argparse.ArgumentParser(
        description="ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py
  python main.py --display 200
  python main.py --scrape --start_date 2026-01-01 --end_date 2026-02-07
  python main.py --fulltext --fulltext_risk_levels ìƒ,ì¤‘
  python main.py --sheets --sheets_id YOUR_SHEET_ID
  python main.py --looker_prep
  python main.py --raw_only --sheets
        """
    )
    # ê¸°ì¡´ ì˜µì…˜
    parser.add_argument("--display", type=int, default=100,
                       help="ë„¤ì´ë²„ APIì—ì„œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 100)")
    parser.add_argument("--start", type=int, default=1,
                       help="ë„¤ì´ë²„ API ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸: 1)")
    parser.add_argument("--sort", type=str, default="date", choices=["date", "sim"],
                       help="ì •ë ¬ ë°©ì‹: date(ìµœì‹ ìˆœ) ë˜ëŠ” sim(ê´€ë ¨ë„ìˆœ) (ê¸°ë³¸: date)")
    parser.add_argument("--outdir", type=str, default="data",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)")
    parser.add_argument("--max_competitor_classify", type=int, default=20,
                       help="ê²½ìŸì‚¬ë³„ ë¶„ë¥˜í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--chunk_size", type=int, default=100,
                       help="AI ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 100)")
    parser.add_argument("--dry_run", action="store_true",
                       help="AI ë¶„ë¥˜ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # ìŠ¤í¬ë˜í•‘ ì˜µì…˜
    parser.add_argument("--scrape", action="store_true",
                       help="Naver ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (ë‚ ì§œ ë²”ìœ„ ê¸°ë°˜)")
    parser.add_argument("--start_date", type=str, default="2026-01-01",
                       help="ìŠ¤í¬ë˜í•‘ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: 2026-01-01)")
    parser.add_argument("--end_date", type=str, default="2026-02-07",
                       help="ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD, ê¸°ë³¸: 2026-02-07)")
    parser.add_argument("--max_scrape_pages", type=int, default=10,
                       help="ìŠ¤í¬ë˜í•‘ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 10)")

    # ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ì˜µì…˜
    parser.add_argument("--fulltext", action="store_true",
                       help="ê¸°ì‚¬ ì „ë¬¸ ìŠ¤í¬ë˜í•‘")
    parser.add_argument("--fulltext_risk_levels", type=str, default="ìƒ,ì¤‘",
                       help="ì „ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•  ìœ„í—˜ë„ (ê¸°ë³¸: ìƒ,ì¤‘)")
    parser.add_argument("--fulltext_max_articles", type=int, default=None,
                       help="ìµœëŒ€ ì „ë¬¸ ìŠ¤í¬ë˜í•‘ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: ë¬´ì œí•œ)")

    # Looker ì¤€ë¹„ ì˜µì…˜
    parser.add_argument("--looker_prep", action="store_true",
                       help="Looker Studioìš© ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€")

    # Google Sheets ì˜µì…˜
    parser.add_argument("--sheets", action="store_true",
                       help="Google Sheetsë¡œ ë°ì´í„° ì—…ë¡œë“œ")
    parser.add_argument("--sheets_id", type=str, default=None,
                       help="Google Sheets ID (ê¸°ë³¸: .envì˜ GOOGLE_SHEET_ID)")

    # API í˜ì´ì§€ë„¤ì´ì…˜ ì˜µì…˜
    parser.add_argument("--max_api_pages", type=int, default=9,
                       help="API í˜ì´ì§€ë„¤ì´ì…˜ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 9, ì¿¼í„° 90%% ì•ˆì „ ë§ˆì§„)")

    # Raw only ì˜µì…˜
    parser.add_argument("--raw_only", action="store_true",
                       help="AI ë¶„ë¥˜ ì—†ì´ API ìˆ˜ì§‘ + Google Sheets ì—…ë¡œë“œë§Œ ì‹¤í–‰")

    args = parser.parse_args()

    print("="*80)
    print("ğŸš€ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    print(f"\nì„¤ì •:")
    print(f"  - ìš°ë¦¬ ë¸Œëœë“œ: {', '.join(OUR_BRANDS)}")
    print(f"  - ê²½ìŸì‚¬: {', '.join(COMPETITORS)}")
    if args.scrape:
        print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: API + ìŠ¤í¬ë˜í•‘ ({args.start_date} ~ {args.end_date})")
    else:
        print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: APIë§Œ")
        print(f"  - ê¸°ì‚¬ ìˆ˜: {args.display}ê°œ/ë¸Œëœë“œ (ìµœëŒ€ {args.max_api_pages} í˜ì´ì§€)")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.outdir}/")
    print(f"  - AI ì²­í¬ í¬ê¸°: {args.chunk_size}")
    if args.fulltext:
        print(f"  - ì „ë¬¸ ìŠ¤í¬ë˜í•‘: {args.fulltext_risk_levels} (ìœ„í—˜ë„)")
    if args.looker_prep:
        print(f"  - Looker ì¤€ë¹„: í™œì„±í™”")
    if args.sheets:
        print(f"  - Google Sheets ì—…ë¡œë“œ: í™œì„±í™”")
    if args.dry_run:
        print(f"  - ëª¨ë“œ: DRY RUN (AI ë¶„ë¥˜ ìƒëµ)")
    if args.raw_only:
        print(f"  - ëª¨ë“œ: RAW ONLY (API ìˆ˜ì§‘ + Sheets ì—…ë¡œë“œë§Œ)")
    print()
    
    # Step 0: í™˜ê²½ ì„¤ì •
    env = load_env()
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: ìˆ˜ì§‘
    print("\n" + "="*80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("="*80)

    # Load existing articles from Google Sheets (if --sheets flag enabled)
    existing_links = set()
    if args.sheets:
        print("\nğŸ“Š Google Sheets ì—°ê²° ì¤‘...")
        spreadsheet = connect_sheets(
            os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json"),
            args.sheets_id or os.getenv("GOOGLE_SHEET_ID")
        )
        if spreadsheet:
            existing_links = load_existing_links_from_sheets(spreadsheet)

    if args.scrape:
        # ìŠ¤í¬ë˜í•‘ ë°©ì‹
        df_scrape = collect_with_scraping(
            OUR_BRANDS, COMPETITORS,
            args.start_date, args.end_date,
            args.max_scrape_pages
        )
        # API ë°©ì‹ë„ ë™ì‹œì— ìˆ˜ì§‘ í›„ ë³‘í•©
        df_api = collect_all_news(
            OUR_BRANDS, COMPETITORS,
            args.display, args.max_api_pages, args.sort,
            env["naver_id"], env["naver_secret"]
        )
        df_raw = merge_api_and_scrape(df_api, df_scrape)
    else:
        # API ë°©ì‹ë§Œ (ê¸°ì¡´ ë™ì‘)
        df_raw = collect_all_news(
            OUR_BRANDS, COMPETITORS,
            args.display, args.max_api_pages, args.sort,
            env["naver_id"], env["naver_secret"]
        )

    # Filter new articles (skip duplicates from Google Sheets)
    if args.sheets and len(existing_links) > 0:
        df_raw = filter_new_articles_from_sheets(df_raw, existing_links)

    if len(df_raw) == 0:
        print("âŒ ìˆ˜ì§‘ëœ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    save_excel(df_raw, outdir / "raw.xlsx")

    # --raw_only ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬/ë¶„ë¥˜/ë¦¬í¬íŠ¸ ìŠ¤í‚µ
    if args.raw_only:
        df_result = df_raw
    else:
        # Step 2: ì²˜ë¦¬
        print("\n" + "="*80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬")
        print("="*80)
        df_normalized = normalize_df(df_raw)
        df_processed = dedupe_df(df_normalized)
        df_processed = detect_similar_articles(df_processed, similarity_threshold=0.8)

        # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€
        if args.sheets and 'spreadsheet' in locals() and spreadsheet:
            df_processed = enrich_with_media_info(
                df_processed,
                spreadsheet=spreadsheet,
                openai_key=env["openai_key"]
            )
        else:
            # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€
            df_processed["media_domain"] = ""
            df_processed["media_name"] = ""
            df_processed["media_group"] = ""
            df_processed["media_type"] = ""

        save_excel(df_processed, outdir / "processed.xlsx")

        # Step 3: ë¶„ë¥˜
        print("\n" + "="*80)
        print("STEP 3: AI ë¶„ë¥˜")
        print("="*80)
        df_classified = classify_all(
            df_processed,
            env["openai_key"],
            args.max_competitor_classify,
            args.chunk_size,
            args.dry_run
        )

        # Step 3.5: ì „ë¬¸ ìŠ¤í¬ë˜í•‘ (ì„ íƒì )
        if args.fulltext:
            print("\n" + "="*80)
            print("STEP 3.5: ê¸°ì‚¬ ì „ë¬¸ ìŠ¤í¬ë˜í•‘")
            print("="*80)
            risk_levels = [r.strip() for r in args.fulltext_risk_levels.split(",")]
            df_classified = batch_fetch_full_text(
                df_classified,
                risk_levels=risk_levels,
                max_articles=args.fulltext_max_articles
            )

        # Step 3.7: Looker ì¤€ë¹„ (ì„ íƒì )
        if args.looker_prep:
            print("\n" + "="*80)
            print("STEP 3.7: Looker Studio ì¤€ë¹„")
            print("="*80)
            df_classified = add_time_series_columns(df_classified)

        df_result = df_classified

        # ê²°ê³¼ ì €ì¥ (ì—¬ëŸ¬ ì‹œíŠ¸)
        result_path = outdir / "result.xlsx"
        with pd.ExcelWriter(result_path, engine='openpyxl') as writer:
            df_result.to_excel(writer, sheet_name='ì „ì²´ë°ì´í„°', index=False)

            # ìš°ë¦¬ ë¸Œëœë“œ ë¶€ì • ê¸°ì‚¬
            our_negative = df_result[(df_result["group"] == "OUR") & (df_result["sentiment"] == "ë¶€ì •")]
            our_negative.to_excel(writer, sheet_name='ìš°ë¦¬_ë¶€ì •', index=False)

            # ìš°ë¦¬ ë¸Œëœë“œ ê¸ì • ê¸°ì‚¬
            our_positive = df_result[(df_result["group"] == "OUR") & (df_result["sentiment"] == "ê¸ì •")]
            our_positive.to_excel(writer, sheet_name='ìš°ë¦¬_ê¸ì •', index=False)

            # ê²½ìŸì‚¬
            competitor = df_result[df_result["group"] == "COMPETITOR"]
            competitor.to_excel(writer, sheet_name='ê²½ìŸì‚¬', index=False)

        print(f"ğŸ’¾ ì €ì¥: {result_path}")

        # Step 4: ë¦¬í¬íŠ¸ ìƒì„±
        print("\n" + "="*80)
        print("STEP 4: ë¦¬í¬íŠ¸ ìƒì„±")
        print("="*80)

        # ì½˜ì†” ë¦¬í¬íŠ¸
        generate_console_report(df_result)

        # Word ë¦¬í¬íŠ¸
        word_path = outdir / "report.docx"
        create_word_report(df_result, word_path)

    # Step 5: Google Sheets ì—…ë¡œë“œ (ì„ íƒì )
    if args.sheets:
        print("\n" + "="*80)
        print("STEP 5: Google Sheets ì—…ë¡œë“œ")
        print("="*80)

        # Reuse spreadsheet connection from STEP 1 (or reconnect if needed)
        if 'spreadsheet' not in locals() or not spreadsheet:
            spreadsheet = connect_sheets(
                os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json"),
                args.sheets_id or os.getenv("GOOGLE_SHEET_ID")
            )

        if spreadsheet:
            sync_raw_and_processed(df_raw, df_result, spreadsheet)
        else:
            print("âš ï¸  Google Sheets ì—°ê²° ì‹¤íŒ¨. Excel/Word ë¦¬í¬íŠ¸ëŠ” ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì™„ë£Œ
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*80)
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    print(f"  ğŸ“Š {outdir}/raw.xlsx - ì›ë³¸ ë°ì´í„°")
    if not args.raw_only:
        print(f"  ğŸ“Š {outdir}/processed.xlsx - ì •ì œëœ ë°ì´í„°")
        print(f"  ğŸ“Š {outdir}/result.xlsx - AI ë¶„ë¥˜ ê²°ê³¼")
        print(f"  ğŸ“„ {outdir}/report.docx - Word ë¦¬í¬íŠ¸")
    if args.sheets:
        print(f"  â˜ï¸  Google Sheets - ë™ê¸°í™” ì™„ë£Œ")
    print()


if __name__ == "__main__":
    main()
