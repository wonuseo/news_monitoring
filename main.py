#!/usr/bin/env python3
"""
main.py - News Monitoring System
ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import os
import argparse
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# Pandas FutureWarning ë°©ì§€
pd.set_option('future.no_silent_downcasting', True)

# ëª¨ë“ˆ ì„í¬íŠ¸
from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
from src.modules.processing.process import (
    normalize_df, dedupe_df,
    enrich_with_media_info, save_csv
)
from src.modules.processing.press_release_detector import detect_similar_articles, summarize_press_release_groups
from src.modules.processing.reprocess_checker import (
    check_reprocess_targets, load_raw_data_from_sheets,
    clear_classified_at_for_targets, print_reprocess_stats,
)
from src.modules.processing.looker_prep import add_time_series_columns
from src.modules.analysis.classify_llm import classify_llm
from src.modules.analysis.result_writer import invalidate_csv_header_cache
from src.modules.analysis.classification_stats import get_classification_stats, print_classification_stats
from src.modules.analysis.llm_engine import set_error_callback as set_llm_error_callback
from src.modules.processing.press_release_detector import set_error_callback as set_pr_error_callback
from src.modules.processing.media_classify import set_error_callback as set_media_error_callback
from src.modules.analysis.classify_press_releases import classify_press_releases
from src.modules.analysis.source_verifier import verify_and_regroup_sources
from src.modules.analysis.keyword_extractor import extract_all_categories
from src.modules.export.report import generate_console_report
from src.modules.export.sheets import (
    connect_sheets,
    sync_raw_and_processed,
    load_existing_links_from_sheets,
    filter_new_articles_from_sheets,
    clean_all_bom_in_sheets,
)
from src.utils.sheets_helpers import intermediate_sync
from src.modules.monitoring.logger import RunLogger, sync_run_history_to_sheets


def load_env():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    load_dotenv()

    naver_id = os.getenv("NAVER_CLIENT_ID")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET")
    openai_key = os.getenv("OPENAI_API_KEY")

    if not naver_id or not naver_secret:
        raise ValueError("âŒ .env íŒŒì¼ì— NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRETì´ ì—†ìŠµë‹ˆë‹¤")
    if not openai_key:
        raise ValueError("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤")

    return {
        "naver_id": naver_id,
        "naver_secret": naver_secret,
        "openai_key": openai_key
    }


def run_preprocessing_pipeline(
    df_to_process, df_raw, result_csv_path, outdir,
    openai_key, spreadsheet, record_error
):
    """
    ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Step 2-1 ~ 2-6).

    Args:
        df_to_process: ì²˜ë¦¬ ëŒ€ìƒ DataFrame
        df_raw: ì›ë³¸ raw DataFrame (Sheets ë™ê¸°í™”ìš©)
        result_csv_path: result.csv ê²½ë¡œ
        outdir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        openai_key: OpenAI API í‚¤
        spreadsheet: gspread Spreadsheet ê°ì²´ (Noneì´ë©´ CSV-only)
        record_error: ì—ëŸ¬ ê¸°ë¡ í•¨ìˆ˜

    Returns:
        (df_processed, metrics_dict)
    """
    sync_error_fn = lambda msg: record_error(msg, category="sheets_sync")

    # Step 2-1: Normalize (cumulative article_no numbering)
    df_normalized = normalize_df(df_to_process, spreadsheet)
    before_dedupe = len(df_normalized)

    # ë‚ ì§œ í•„í„°ë§ì€ STEP 1.6ì—ì„œ ì´ë¯¸ ìˆ˜í–‰ë¨
    filtered_count = 0

    # Step 2-2: Deduplicate
    df_processed = dedupe_df(df_normalized)
    duplicates_removed = before_dedupe - len(df_processed)

    # ì¤‘ê°„ ë™ê¸°í™” (ì¤‘ë³µ ì œê±° ì™„ë£Œ í›„)
    intermediate_sync(
        df_processed, df_raw, result_csv_path, spreadsheet,
        "ì¤‘ë³µ ì œê±°", save_csv, sync_error_fn
    )

    # Step 2-3: Detect similar articles (Press Release, cumulative cluster_id numbering)
    df_processed = detect_similar_articles(df_processed, spreadsheet)
    press_releases = len(df_processed[df_processed['source'] == 'ë³´ë„ìë£Œ']) if 'source' in df_processed.columns else 0

    # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ íƒì§€ ì™„ë£Œ í›„)
    intermediate_sync(
        df_processed, df_raw, result_csv_path, spreadsheet,
        "ë³´ë„ìë£Œ íƒì§€", save_csv, sync_error_fn
    )

    # Step 2-4: Summarize press release groups (OpenAI)
    df_processed = summarize_press_release_groups(df_processed, openai_key)

    # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ ìš”ì•½ ì™„ë£Œ í›„)
    intermediate_sync(
        df_processed, df_raw, result_csv_path, spreadsheet,
        "ë³´ë„ìë£Œ ìš”ì•½", save_csv, sync_error_fn
    )

    # Step 2-5: Media classification (OpenAI)
    media_csv_path = outdir / "media_directory.csv"
    df_processed, media_stats = enrich_with_media_info(
        df_processed,
        spreadsheet=spreadsheet,
        openai_key=openai_key,
        csv_path=media_csv_path
    )

    # ì¤‘ê°„ ë™ê¸°í™” (ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ì™„ë£Œ í›„)
    intermediate_sync(
        df_processed, df_raw, result_csv_path, spreadsheet,
        "ì–¸ë¡ ì‚¬ ë¶„ë¥˜", save_csv, sync_error_fn
    )

    # Press release avg cluster size
    press_release_groups = df_processed['cluster_id'].nunique() if 'cluster_id' in df_processed.columns else 0
    avg_cluster_size = round(press_releases / press_release_groups, 1) if press_release_groups > 0 else 0

    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    metrics = {
        "articles_processed": len(df_processed),
        "duplicates_removed": duplicates_removed,
        "articles_filtered_by_date": filtered_count,
        "press_releases_detected": press_releases,
        "press_release_groups": press_release_groups,
        "press_release_avg_cluster_size": avg_cluster_size,
    }
    metrics.update(media_stats)

    return df_processed, metrics


def main():
    # RunLogger ì´ˆê¸°í™”
    logger = RunLogger()

    parser = argparse.ArgumentParser(
        description="ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                                    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìì‚¬+ê²½ìŸì‚¬ ì „ì²´ ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ ìë™)
  python main.py --display 200                      # API ê²°ê³¼ ê°œìˆ˜ ì§€ì •
  python main.py --keyword_top_k 30                 # í‚¤ì›Œë“œ ì¶”ì¶œ ê°œìˆ˜ ì¡°ì • (ê¸°ë³¸: 20)
  python main.py --max_competitor_classify 20       # ê²½ìŸì‚¬ ë¶„ì„ ê°œìˆ˜ ì œí•œ
  python main.py --sheets_id YOUR_SHEET_ID          # Google Sheets ID ì§€ì •
  python main.py --raw_only                         # ìˆ˜ì§‘ë§Œ (Sheets ìë™ ë™ê¸°í™”)

ì£¼ì˜:
  - Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤ (credentials ì„¤ì • ê¶Œì¥)
  - CSV íŒŒì¼ì€ troubleshooting ìš©ë„ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤
  - .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_ID ì„¤ì • í•„ìš”
        """
    )
    # ê¸°ì¡´ ì˜µì…˜
    parser.add_argument("--display", type=int, default=100,
                        help="ë„¤ì´ë²„ APIì—ì„œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 100)")
    parser.add_argument("--start", type=int, default=1,
                        help="ë„¤ì´ë²„ API ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸: 1)")
    parser.add_argument("--sort", type=str, default="date", choices=["date", "sim"],
                        help="ì •ë ¬ ë°©ì‹: date(ìµœì‹ ìˆœ) ë˜ëŠ” sim(ê´€ë ¨ë„ìˆœ) (ê¸°ë³¸: date)")
    parser.add_argument("--outdir", type=str, default="data",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)")
    parser.add_argument("--max_competitor_classify", type=int, default=0,
                        help="ê²½ìŸì‚¬ë³„ ë¶„ë¥˜í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 0=ë¬´ì œí•œ, ì „ì²´ ë¶„ì„)")
    parser.add_argument("--chunk_size", type=int, default=100,
                        help="AI ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 100)")
    parser.add_argument("--max_workers", type=int, default=3,
                        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 3, ê¶Œì¥: 3-10)")
    parser.add_argument("--dry_run", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # Google Sheets ì˜µì…˜ (í•­ìƒ í™œì„±í™”ë¨)
    parser.add_argument("--sheets_id", type=str, default=None,
                        help="Google Sheets ID (.envì˜ GOOGLE_SHEET_ID ëŒ€ì‹  ì‚¬ìš©)")

    # API í˜ì´ì§€ë„¤ì´ì…˜ ì˜µì…˜
    parser.add_argument("--max_api_pages", type=int, default=9,
                        help="API í˜ì´ì§€ë„¤ì´ì…˜ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 9, ì¿¼í„° 90%% ì•ˆì „ ë§ˆì§„)")

    # Raw only ì˜µì…˜
    parser.add_argument("--raw_only", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ API ìˆ˜ì§‘ + Google Sheets ì—…ë¡œë“œë§Œ ì‹¤í–‰")

    # Preprocess only ì˜µì…˜
    parser.add_argument("--preprocess_only", action="store_true",
                        help="ìˆ˜ì§‘ + ì „ì²˜ë¦¬ê¹Œì§€ë§Œ ì‹¤í–‰ (AI ë¶„ë¥˜, ë¦¬í¬íŠ¸ ìƒëµ, Sheets ì—…ë¡œë“œëŠ” ì‹¤í–‰)")

    # Keyword extraction ì˜µì…˜
    parser.add_argument("--extract_keywords", action="store_true",
                        help="(ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, í•­ìƒ ìë™ ì‹¤í–‰ë¨) ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ")
    parser.add_argument("--keyword_top_k", type=int, default=20,
                        help="í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ìƒìœ„ Kê°œ ì„ íƒ (ê¸°ë³¸: 20)")

    # BOM ì •ë¦¬ ì˜µì…˜
    parser.add_argument("--clean_bom", action="store_true",
                        help="Google Sheets ì „ì²´ BOM ë¬¸ì ì •ë¦¬ í›„ ì¢…ë£Œ")

    # ì¬ì²˜ë¦¬ ì „ìš© ì˜µì…˜
    parser.add_argument("--recheck_only", action="store_true",
                        help="API ìˆ˜ì§‘ ì—†ì´ ì¬ì²˜ë¦¬ ëŒ€ìƒë§Œ ê²€ì‚¬/ì¬ì²˜ë¦¬ (Sheets í•„ìˆ˜)")

    args = parser.parse_args()

    # run_mode íŒë³„
    if args.dry_run:
        run_mode = "dry_run"
    elif args.raw_only:
        run_mode = "raw_only"
    elif args.preprocess_only:
        run_mode = "preprocess_only"
    elif args.recheck_only:
        run_mode = "recheck_only"
    else:
        run_mode = "full"

    # CLI args ë¡œê¹…
    logger.log("run_mode", run_mode)
    logger.log("cli_args", vars(args))
    logger.log_event("run_started", {"cli_args": vars(args)}, stage="init")

    # current_stage for error callback propagation
    current_stage = "init"

    print("=" * 80)
    print("ğŸš€ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - ìš°ë¦¬ ë¸Œëœë“œ: {', '.join(OUR_BRANDS)}")
    print(f"  - ê²½ìŸì‚¬: {', '.join(COMPETITORS)}")
    print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: Naver API")
    print(f"  - ê¸°ì‚¬ ìˆ˜: {args.display}ê°œ/ë¸Œëœë“œ (ìµœëŒ€ {args.max_api_pages} í˜ì´ì§€)")
    print(f"  - ë‚ ì§œ í•„í„°: 2026-02-01 ì´í›„ë§Œ ë¶„ì„")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.outdir}/")
    if args.max_competitor_classify == 0:
        print(f"  - ë¶„ë¥˜ ëª¨ë“œ: ìì‚¬+ê²½ìŸì‚¬ ì „ì²´ ë¶„ì„")
    else:
        print(f"  - ë¶„ë¥˜ ëª¨ë“œ: ìì‚¬ ì „ì²´ + ê²½ìŸì‚¬ ìµœëŒ€ {args.max_competitor_classify}ê°œ/ë¸Œëœë“œ")
    print(f"  - AI ì²­í¬ í¬ê¸°: {args.chunk_size}")
    print(f"  - ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤: {args.max_workers}ê°œ")
    print(f"  - í‚¤ì›Œë“œ ì¶”ì¶œ: ìë™ ì‹¤í–‰ (ìƒìœ„ {args.keyword_top_k}ê°œ)")
    if args.dry_run:
        print(f"  - ëª¨ë“œ: DRY RUN (AI ë¶„ë¥˜ ìƒëµ)")
    if args.raw_only:
        print(f"  - ëª¨ë“œ: RAW ONLY (API ìˆ˜ì§‘ + Sheets ì—…ë¡œë“œë§Œ)")
    if args.preprocess_only:
        print(f"  - ëª¨ë“œ: PREPROCESS ONLY (ìˆ˜ì§‘ + ì „ì²˜ë¦¬ + Sheets ì—…ë¡œë“œ)")
    if args.recheck_only:
        print(f"  - ëª¨ë“œ: RECHECK ONLY (API ìˆ˜ì§‘ ìƒëµ, Sheets ê¸°ì¤€ ì¬ì²˜ë¦¬)")
    print()

    # Step 0: í™˜ê²½ ì„¤ì •
    env = load_env()
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    media_csv_path = outdir / "media_directory.csv"

    # Step 1: ìˆ˜ì§‘
    current_stage = "collection"
    logger.start_stage("collection")
    print("\n" + "=" * 80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("=" * 80)

    # Google Sheets ì—°ê²° (ì£¼ ì €ì¥ì†Œ)
    existing_links = set()
    spreadsheet = None
    def record_error(message, data=None, category="system"):
        logger.log_error(message, data, category=category, stage=current_stage)
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)

    # Error callback registration for OpenAI wrappers (log failures only)
    set_llm_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))
    set_pr_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))
    set_media_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))

    # Google Sheets ìë™ ì—°ê²° (credentials í•„ìˆ˜ ê¶Œì¥)
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json")
    sheet_id = args.sheets_id or os.getenv("GOOGLE_SHEET_ID")

    if os.path.exists(creds_path) and sheet_id:
        try:
            print("\nğŸ“Š Google Sheets ì—°ê²° ì¤‘...")
            spreadsheet = connect_sheets(creds_path, sheet_id)
            if spreadsheet:
                # Sheets ê¸°ì¡´ BOM ìë™ ì •ë¦¬ (ë°ì´í„° ë¡œë“œ ì „)
                print("ğŸ§¹ Sheets BOM ë¬¸ì ìë™ ì •ë¦¬ ì¤‘...")
                bom_results = clean_all_bom_in_sheets(spreadsheet)
                bom_total = sum(bom_results.values())
                if bom_total > 0:
                    print(f"  âœ… {bom_total}ê°œ ì…€ BOM ì œê±° ì™„ë£Œ")

                existing_links = load_existing_links_from_sheets(spreadsheet)
                print("âœ… Google Sheets ì—°ê²° ì„±ê³µ (ì£¼ ì €ì¥ì†Œ)")
                print("   CSV íŒŒì¼ì€ troubleshootingìš©ìœ¼ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.")
                logger.log_event("sheets_connected", {"sheet_id": sheet_id}, category="sheets_sync", stage="init")
                logger.flush_all_to_sheets(spreadsheet)
        except Exception as e:
            record_error(f"Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}", {"sheet_id": sheet_id}, category="sheets_sync")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
            logger.log_event("sheets_connect_failed", {"error": str(e)}, category="sheets_sync", stage="init")
    else:
        print("\n" + "="*80)
        print("âš ï¸  ê²½ê³ : Google Sheets ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("="*80)
        if not os.path.exists(creds_path):
            print(f"  credential íŒŒì¼ ì—†ìŒ: {creds_path}")
        if not sheet_id:
            print(f"  GOOGLE_SHEET_ID í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
        print("\n  Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤. ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print("  í˜„ì¬ëŠ” CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("="*80 + "\n")
        logger.log_event("sheets_not_configured", {"credentials_path": creds_path, "sheet_id": sheet_id}, category="sheets_sync", stage="init")

    # --clean_bom ëª¨ë“œ: Sheets ì „ì²´ BOM ì •ë¦¬ í›„ ì¢…ë£Œ
    if args.clean_bom:
        if not spreadsheet:
            print("âŒ --clean_bom ì‚¬ìš© ì‹œ Google Sheets ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            return
        print("\n" + "=" * 80)
        print("BOM ë¬¸ì ì •ë¦¬ ëª¨ë“œ")
        print("=" * 80)
        results = clean_all_bom_in_sheets(spreadsheet)
        total_cleaned = sum(results.values())
        print(f"\nâœ… BOM ì •ë¦¬ ì™„ë£Œ: ì´ {total_cleaned}ê°œ ì…€ ì •ë¦¬")
        return

    raw_csv_path = outdir / "raw.csv"

    # --recheck_only ëª¨ë“œ: API ìˆ˜ì§‘ ìƒëµ, Sheetsì—ì„œ raw_data ë¡œë“œ
    if args.recheck_only:
        if not spreadsheet:
            print("âŒ --recheck_only ì‚¬ìš© ì‹œ Google Sheets ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            return
        print("\nğŸ“‹ --recheck_only ëª¨ë“œ: API ìˆ˜ì§‘ ìƒëµ, Sheets raw_data ë¡œë“œ")
        df_raw = load_raw_data_from_sheets(spreadsheet)
        if len(df_raw) == 0:
            print("âŒ Sheets raw_dataê°€ ë¹„ì–´ìˆì–´ ì¬ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        df_raw_new = pd.DataFrame(columns=df_raw.columns)
        save_csv(df_raw, raw_csv_path)
        logger.log_dict({
            "articles_collected_total": 0,
            "articles_collected_per_query": {},
            "existing_links_skipped": 0,
        })
        logger.end_stage("collection")
    else:
        # API ë°©ì‹ ìˆ˜ì§‘ (raw.csv ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)
        df_raw_new = collect_all_news(
            OUR_BRANDS, COMPETITORS,
            args.display, args.max_api_pages, args.sort,
            env["naver_id"], env["naver_secret"],
            raw_csv_path=str(raw_csv_path),
            spreadsheet=spreadsheet
        )

        # API ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
        if len(df_raw_new) == 0:
            print("\nâ„¹ï¸  APIì—ì„œ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâœ… APIì—ì„œ {len(df_raw_new)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

        # Filter new articles (skip duplicates from Google Sheets)
        existing_links_skipped = 0
        if len(existing_links) > 0 and len(df_raw_new) > 0:
            before_filter = len(df_raw_new)
            df_raw_new = filter_new_articles_from_sheets(df_raw_new, existing_links)
            existing_links_skipped = before_filter - len(df_raw_new)

        # raw.csv ì—…ë°ì´íŠ¸ (append)
        if raw_csv_path.exists():
            df_raw_existing = pd.read_csv(raw_csv_path, encoding='utf-8-sig')
            df_raw = pd.concat([df_raw_existing, df_raw_new], ignore_index=True)
            df_raw = df_raw.drop_duplicates(subset=['link'], keep='last')
            print(f"ğŸ“‚ ê¸°ì¡´ raw.csv ì—…ë°ì´íŠ¸: {len(df_raw_existing)} + {len(df_raw_new)} = {len(df_raw)}ê°œ ê¸°ì‚¬")
        else:
            df_raw = df_raw_new

        save_csv(df_raw, raw_csv_path)

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ìˆ˜ì§‘ ì§í›„)
        if spreadsheet and len(df_raw_new) > 0:
            print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (raw_data)...")
            try:
                from src.modules.export.sheets import sync_to_sheets
                sync_result = sync_to_sheets(df_raw, spreadsheet, "raw_data")
                msg_parts = [
                    f"{sync_result.get('attempted', 0)}ê°œ ì‹œë„",
                    f"{sync_result.get('added', 0)}ê°œ ì¶”ê°€"
                ]
                if sync_result.get('updated', 0) > 0:
                    msg_parts.append(f"{sync_result['updated']}ê°œ ì—…ë°ì´íŠ¸")
                msg_parts.append(f"{sync_result.get('skipped', 0)}ê°œ ê±´ë„ˆëœ€")
                print(f"âœ… raw_data ì‹œíŠ¸ ë™ê¸°í™” ì™„ë£Œ: {', '.join(msg_parts)}")
                logger.log_event("sheets_sync_raw_data", sync_result, category="sheets_sync", stage="collection")
            except Exception as e:
                record_error(f"raw_data ì‹œíŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

        # ìˆ˜ì§‘ ë‹¨ê³„ ë©”íŠ¸ë¦­
        articles_per_query = df_raw_new.groupby('query').size().to_dict() if 'query' in df_raw_new.columns else {}
        logger.log_dict({
            "articles_collected_total": len(df_raw_new),
            "articles_collected_per_query": articles_per_query,
            "existing_links_skipped": existing_links_skipped
        })
        logger.log_event("collection_completed", {
            "articles_collected_total": len(df_raw_new),
            "articles_collected_per_query": articles_per_query,
            "existing_links_skipped": existing_links_skipped
        }, stage="collection")
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)
        logger.end_stage("collection")

    # STEP 1.5: ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²€ì‚¬
    result_csv_path = outdir / "result.csv"

    try:
        recheck = check_reprocess_targets(df_raw, spreadsheet, str(result_csv_path))
        print_reprocess_stats(recheck["stats"])

        df_reprocess = recheck["df_to_reprocess"]
        if len(df_reprocess) > 0:
            df_reprocess = clear_classified_at_for_targets(df_reprocess, recheck["reprocess_links"])
            df_to_process = pd.concat([df_raw_new, df_reprocess], ignore_index=True)
            df_to_process = df_to_process.drop_duplicates(subset=['link'], keep='first')
        else:
            df_to_process = df_raw_new

        # ì¬ì²˜ë¦¬ ë©”íŠ¸ë¦­ ë¡œê¹…
        logger.log_dict({
            "reprocess_targets_total": recheck["stats"]["total_reprocess_targets"],
            "reprocess_missing_from_result": recheck["stats"]["missing_from_result"],
            "reprocess_field_missing": recheck["stats"].get("field_missing", {}),
        })
    except Exception as e:
        record_error(f"ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²€ì‚¬ ì‹¤íŒ¨: {e}", category="system")
        df_to_process = df_raw_new

    # STEP 1.6: 2026-02-01 ì´í›„ ê¸°ì‚¬ë§Œ í•„í„°ë§
    if len(df_to_process) > 0 and 'pubDate' in df_to_process.columns:
        before_date_filter = len(df_to_process)
        df_to_process['pub_datetime_temp'] = pd.to_datetime(df_to_process['pubDate'], errors='coerce')
        df_to_process = df_to_process[df_to_process['pub_datetime_temp'] >= '2026-02-01'].copy()
        df_to_process = df_to_process.drop(columns=['pub_datetime_temp'])
        date_filtered = before_date_filter - len(df_to_process)
        print(f"ğŸ”§ ë‚ ì§œ í•„í„°ë§: {date_filtered}ê°œ ì œì™¸ (2026-02-01 ì´ì „), {len(df_to_process)}ê°œ ìœ ì§€")

    if len(df_to_process) == 0:
        print("â„¹ï¸  ì²˜ë¦¬í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ Google Sheets ë™ê¸°í™” ì‹œë„
        if spreadsheet and result_csv_path.exists():
            logger.start_stage("sheets_sync")
            print("\n" + "=" * 80)
            print("STEP 5: Google Sheets ì—…ë¡œë“œ (ê¸°ì¡´ ë°ì´í„°)")
            print("=" * 80)
            try:
                df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                sync_results = sync_raw_and_processed(df_raw, df_result_existing, spreadsheet)
                print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")

                # Sheets ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (added + updated)
                raw_sync = sync_results.get("raw_data", {})
                result_sync = sync_results.get("total_result", {})
                logger.log_dict({
                    "sheets_sync_enabled": True,
                    "sheets_rows_uploaded_raw": raw_sync.get("added", 0) + raw_sync.get("updated", 0),
                    "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0)
                })
            except Exception as e:
                record_error(f"Google Sheets ì—…ë¡œë“œ ì‹¤íŒ¨ (ê¸°ì¡´ ë°ì´í„°): {e}", category="sheets_sync")
                logger.log("sheets_sync_enabled", False)
            logger.end_stage("sheets_sync")
            logger.log_event("sheets_sync_completed", {
                "sheets_rows_uploaded_raw": logger.metrics.get("sheets_rows_uploaded_raw", 0),
                "sheets_rows_uploaded_result": logger.metrics.get("sheets_rows_uploaded_result", 0)
            }, category="sheets_sync", stage="sheets_sync")
            if spreadsheet:
                logger.flush_all_to_sheets(spreadsheet)

        # ë¡œê·¸ ì €ì¥
        logger.finalize()
        logs_csv_path = outdir / "logs" / "run_history.csv"
        logger.save_csv(str(logs_csv_path))

        # Sheets ë¡œê·¸ ë™ê¸°í™”
        if spreadsheet:
            sync_run_history_to_sheets(str(logs_csv_path), spreadsheet)

        logger.print_summary()

        print("\n" + "=" * 80)
        print("âœ… ì‘ì—… ì™„ë£Œ (ì‹ ê·œ ì²˜ë¦¬ ì—†ìŒ)")
        print("=" * 80)
        return

    # --raw_only ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬/ë¶„ë¥˜/ë¦¬í¬íŠ¸ ìŠ¤í‚µ
    if args.raw_only:
        df_result = df_raw
        logger.log_dict({
            "articles_processed": 0,
            "duplicates_removed": 0,
            "articles_filtered_by_date": 0,
            "press_releases_detected": 0,
            "press_release_groups": 0
        })
    # --preprocess_only ëª¨ë“œì¸ ê²½ìš° ë¶„ë¥˜ ìŠ¤í‚µ, ì²˜ë¦¬+ë¦¬í¬íŠ¸ëŠ” ì‹¤í–‰
    elif args.preprocess_only:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        current_stage = "processing"
        logger.start_stage("processing")
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)

        df_processed, proc_metrics = run_preprocessing_pipeline(
            df_to_process, df_raw, result_csv_path, outdir,
            env["openai_key"], spreadsheet, record_error
        )

        # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜
        df_processed = df_processed.fillna("")

        # Step 2-6: Looker Studio time-series columns
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_processed = add_time_series_columns(df_processed)

        # ì „ì²˜ë¦¬ ë©”íŠ¸ë¦­
        current_stage = "processing"
        logger.log_dict(proc_metrics)
        logger.end_stage("processing")
        logger.log_event("processing_completed", proc_metrics, stage="processing")
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_result = pd.concat([df_result_existing, df_processed], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"ğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_processed)} = {len(df_result)}ê°œ ê¸°ì‚¬")
        else:
            df_result = df_processed

        # total_result_count
        logger.log("total_result_count", len(df_result))

        # ê²°ê³¼ ì €ì¥
        save_csv(df_result, result_csv_path)
        invalidate_csv_header_cache(str(result_csv_path))

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ì „ì²˜ë¦¬ ì™„ë£Œ ì§í›„)
        if spreadsheet:
            print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (ì „ì²˜ë¦¬ ê²°ê³¼)...")
            try:
                sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
                print("âœ… ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
            except Exception as e:
                record_error(f"ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")
    else:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        current_stage = "processing"
        logger.start_stage("processing")
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)

        df_processed, proc_metrics = run_preprocessing_pipeline(
            df_to_process, df_raw, result_csv_path, outdir,
            env["openai_key"], spreadsheet, record_error
        )

        # ì „ì²˜ë¦¬ ë©”íŠ¸ë¦­
        current_stage = "processing"
        logger.log_dict(proc_metrics)
        logger.end_stage("processing")

        # Step 3: ë¶„ë¥˜ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        print("\n" + "=" * 80)
        print("STEP 3: ë¶„ë¥˜")
        print("=" * 80)

        # Step 3-1: ë³´ë„ìë£Œ LLM ë¶„ë¥˜ (ëŒ€í‘œ ê¸°ì‚¬ ë¶„ì„ -> í´ëŸ¬ìŠ¤í„° ê³µìœ )
        current_stage = "pr_classification"
        logger.start_stage("pr_classification")
        df_processed, pr_metrics = classify_press_releases(
            df_processed,
            env["openai_key"],
            chunk_size=args.chunk_size,
            max_workers=args.max_workers,
            result_csv_path=str(result_csv_path),
            spreadsheet=spreadsheet,
            raw_df=df_raw
        )
        logger.log_dict(pr_metrics)
        logger.end_stage("pr_classification")
        logger.log_event("pr_classification_completed", pr_metrics, stage="pr_classification")
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)

        # Step 3-2: LLM ë¶„ë¥˜ (ë³´ë„ìë£ŒëŠ” ìŠ¤í‚µ)
        current_stage = "general_classification"
        logger.start_stage("general_classification")
        df_classified, llm_metrics = classify_llm(
            df_processed,
            env["openai_key"],
            chunk_size=args.chunk_size,
            dry_run=args.dry_run,
            max_competitor_classify=args.max_competitor_classify,
            max_workers=args.max_workers,
            result_csv_path=str(result_csv_path),
            spreadsheet=spreadsheet,
            raw_df=df_raw
        )

        # LLM ë©”íŠ¸ë¦­ ë¡œê¹…
        logger.log_dict(llm_metrics)
        logger.end_stage("general_classification")

        # í†µê³„ ì¶œë ¥
        stats = get_classification_stats(df_classified)
        print_classification_stats(stats)

        # ë³´ë„ìë£Œ ì •ë³´ ë° ì–¸ë¡ ì‚¬ ì •ë³´ ë³‘í•©
        source_cols = ['link', 'source', 'cluster_id', 'press_release_group', 'media_domain', 'media_name', 'media_group',
                       'media_type']
        source_data = df_processed[source_cols].copy()

        # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        cols_to_drop = [col for col in df_classified.columns if col in source_cols and col != 'link']
        df_classified = df_classified.drop(columns=cols_to_drop, errors='ignore')

        # merge
        df_classified = df_classified.merge(source_data, on='link', how='left')

        # ë‚˜ë¨¸ì§€ NaN -> ê³µë€ ë³€í™˜ (FutureWarning ë°©ì§€)
        df_classified = df_classified.fillna("").infer_objects(copy=False)

        # Step 3-3: Source ê²€ì¦ ë° ì£¼ì œ ê·¸ë£¹í™”
        if not args.dry_run:
            current_stage = "source_verification"
            logger.start_stage("source_verification")
            df_classified, sv_metrics = verify_and_regroup_sources(df_classified, openai_key=env["openai_key"])
            logger.log_dict(sv_metrics)
            logger.end_stage("source_verification")
            logger.log_event("source_verification_completed", sv_metrics, stage="source_verification")
            if spreadsheet:
                logger.flush_all_to_sheets(spreadsheet)

        # Step 3.7: Looker Studio ì¤€ë¹„ (í•­ìƒ ì‹¤í–‰)
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_classified = add_time_series_columns(df_classified)

        logger.log_event("classification_completed", {
            "articles_classified_llm": llm_metrics.get("articles_classified_llm", 0),
            "llm_api_calls": llm_metrics.get("llm_api_calls", 0),
            "press_releases_skipped": llm_metrics.get("press_releases_skipped", 0),
            "classification_errors": llm_metrics.get("classification_errors", 0)
        }, stage="general_classification")
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig', on_bad_lines='skip', engine='python')
            df_result = pd.concat([df_result_existing, df_classified], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"\nğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_classified)} = {len(df_result)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ ì œê±° í›„)")
        else:
            df_result = df_classified

        # pub_datetimeì„ ëª…ì‹œì ìœ¼ë¡œ datetimeìœ¼ë¡œ ë³€í™˜ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
        if 'pub_datetime' in df_result.columns:
            df_result['pub_datetime'] = pd.to_datetime(df_result['pub_datetime'], errors='coerce')

        # ê²°ê³¼ ì €ì¥ (ë‹¨ì¼ CSV íŒŒì¼)
        save_csv(df_result, result_csv_path)
        invalidate_csv_header_cache(str(result_csv_path))

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ë¶„ë¥˜ ì™„ë£Œ ì§í›„)
        if spreadsheet:
            print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (ë¶„ë¥˜ ê²°ê³¼)...")
            try:
                sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
                print("âœ… ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
            except Exception as e:
                record_error(f"ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

        # Step 4: ë¦¬í¬íŠ¸ ìƒì„±
        print("\n" + "=" * 80)
        print("STEP 4: ë¦¬í¬íŠ¸ ìƒì„±")
        print("=" * 80)

        # ì½˜ì†” ë¦¬í¬íŠ¸
        generate_console_report(df_result)

        # ë¶„ë¥˜ ê²°ê³¼ ë©”íŠ¸ë¦­
        our_brands_relevant = len(df_result[(df_result['group'] == 'OUR') & (df_result['brand_relevance'].isin(['ê´€ë ¨', 'ì–¸ê¸‰']))]) if 'brand_relevance' in df_result.columns else 0
        our_brands_negative = len(df_result[(df_result['group'] == 'OUR') & (df_result['sentiment_stage'].isin(['ë¶€ì • í›„ë³´', 'ë¶€ì • í™•ì •']))]) if 'sentiment_stage' in df_result.columns else 0
        danger_high = len(df_result[df_result['danger_level'] == 'ìƒ']) if 'danger_level' in df_result.columns else 0
        danger_medium = len(df_result[df_result['danger_level'] == 'ì¤‘']) if 'danger_level' in df_result.columns else 0
        competitor_articles = len(df_result[df_result['group'] == 'COMPETITOR']) if 'group' in df_result.columns else 0

        logger.log_dict({
            "our_brands_relevant": our_brands_relevant,
            "our_brands_negative": our_brands_negative,
            "danger_high": danger_high,
            "danger_medium": danger_medium,
            "competitor_articles": competitor_articles,
            "total_result_count": len(df_result),
        })

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ìë™ ì‹¤í–‰)
        print("\n" + "=" * 80)
        print("STEP 4.5: ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ")
        print("=" * 80)
        extract_all_categories(
            df=df_result,
            output_dir=outdir / "keywords",
            top_k=args.keyword_top_k,
            max_display=10,
            spreadsheet=spreadsheet  # Google Sheets ì—°ê²° ì „ë‹¬
        )

    # Step 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)
    current_stage = "sheets_sync"
    if spreadsheet:
        logger.start_stage("sheets_sync")
        print("\n" + "=" * 80)
        print("STEP 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)")
        print("=" * 80)
        try:
            sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
            print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")

            # Sheets ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (added + updated)
            raw_sync = sync_results.get("raw_data", {})
            result_sync = sync_results.get("total_result", {})
            logger.log_dict({
                "sheets_sync_enabled": True,
                "sheets_rows_uploaded_raw": raw_sync.get("added", 0) + raw_sync.get("updated", 0),
                "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0)
            })
        except Exception as e:
            record_error(f"Google Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ìµœì¢…): {e}", category="sheets_sync")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
            logger.log("sheets_sync_enabled", False)
        logger.end_stage("sheets_sync")
        logger.log_event("sheets_sync_completed", {
            "sheets_rows_uploaded_raw": logger.metrics.get("sheets_rows_uploaded_raw", 0),
            "sheets_rows_uploaded_result": logger.metrics.get("sheets_rows_uploaded_result", 0)
        }, category="sheets_sync", stage="sheets_sync")
        if spreadsheet:
            logger.flush_all_to_sheets(spreadsheet)
    else:
        logger.log("sheets_sync_enabled", False)
        print("\n" + "=" * 80)
        print("âš ï¸  Google Sheets ì—°ê²° ì—†ìŒ")
        print("=" * 80)
        print("  ì£¼ ì €ì¥ì†Œì¸ Google Sheetsì— ë™ê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATH ë° GOOGLE_SHEET_ID ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    # ë¡œê·¸ ì €ì¥
    logger.finalize()

    logs_csv_path = outdir / "logs" / "run_history.csv"
    logger.save_csv(str(logs_csv_path))

    # Sheets run_history ë™ê¸°í™”
    if spreadsheet:
        try:
            sync_run_history_to_sheets(str(logs_csv_path), spreadsheet)
        except Exception as e:
            record_error(f"run_history Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

    # ë¡œê·¸ ìš”ì•½ ì¶œë ¥
    logger.print_summary()

    # ì™„ë£Œ
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    if spreadsheet:
        print(f"  â˜ï¸  Google Sheets - ë™ê¸°í™” ì™„ë£Œ (ì£¼ ì €ì¥ì†Œ)")
    print(f"  ğŸ“Š {outdir}/raw.csv - ì›ë³¸ ë°ì´í„° (troubleshooting)")
    if not args.raw_only:
        print(f"  ğŸ“Š {outdir}/result.csv - AI ë¶„ë¥˜ ê²°ê³¼ (troubleshooting)")
        print(f"  ğŸ“‚ {outdir}/keywords/ - ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ CSV")
    if spreadsheet or not args.raw_only:
        print(f"  ğŸ“‚ {outdir}/media_directory.csv - ì–¸ë¡ ì‚¬ ë””ë ‰í† ë¦¬")
    if not spreadsheet:
        print(f"\n  âš ï¸  Google Sheets ë¯¸ì—°ê²°: CSV íŒŒì¼ë§Œ ì €ì¥ë¨")
    print()


if __name__ == "__main__":
    main()
