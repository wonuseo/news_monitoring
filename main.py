#!/usr/bin/env python3
"""
main.py - News Monitoring System
ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import os
import argparse
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# Pandas FutureWarning ë°©ì§€
pd.set_option('future.no_silent_downcasting', True)

# ëª¨ë“ˆ ì„í¬íŠ¸
from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
from src.modules.processing.process import (
    normalize_df, dedupe_df,
    enrich_with_media_info, save_csv
)
from src.modules.processing.press_release_detector import detect_similar_articles, summarize_press_release_groups
from src.modules.processing.looker_prep import add_time_series_columns
from src.modules.analysis.classify_llm import classify_llm, get_classification_stats, print_classification_stats
from src.modules.analysis.llm_engine import set_error_callback as set_llm_error_callback
from src.modules.analysis.classify import set_error_callback as set_batch_error_callback
from src.modules.processing.press_release_detector import set_error_callback as set_pr_error_callback
from src.modules.processing.media_classify import set_error_callback as set_media_error_callback
from src.modules.analysis.preset_pr import preset_press_release_values
from src.modules.analysis.keyword_extractor import extract_all_categories
from src.modules.export.report import generate_console_report
from src.modules.export.sheets import (
    connect_sheets,
    sync_raw_and_processed,
    load_existing_links_from_sheets,
    filter_new_articles_from_sheets,
    load_analysis_status_from_sheets,
    clean_all_bom_in_sheets,
    clean_bom
)
from src.modules.monitoring.logger import RunLogger, sync_logs_to_sheets


def load_env():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
    load_dotenv()

    naver_id = os.getenv("NAVER_CLIENT_ID")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET")
    openai_key = os.getenv("OPENAI_API_KEY")

    if not naver_id or not naver_secret:
        raise ValueError("âŒ .env íŒŒì¼ì— NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRETì´ ì—†ìŠµë‹ˆë‹¤")
    if not openai_key:
        raise ValueError("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤")

    return {
        "naver_id": naver_id,
        "naver_secret": naver_secret,
        "openai_key": openai_key
    }


def main():
    # RunLogger ì´ˆê¸°í™”
    logger = RunLogger()
    logger.start_stage("total")

    parser = argparse.ArgumentParser(
        description="ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                                    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìì‚¬+ê²½ìŸì‚¬ ì „ì²´ ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ ìë™)
  python main.py --display 200                      # API ê²°ê³¼ ê°œìˆ˜ ì§€ì •
  python main.py --keyword_top_k 30                 # í‚¤ì›Œë“œ ì¶”ì¶œ ê°œìˆ˜ ì¡°ì • (ê¸°ë³¸: 20)
  python main.py --max_competitor_classify 20       # ê²½ìŸì‚¬ ë¶„ì„ ê°œìˆ˜ ì œí•œ
  python main.py --sheets_id YOUR_SHEET_ID          # Google Sheets ID ì§€ì •
  python main.py --raw_only                         # ìˆ˜ì§‘ë§Œ (Sheets ìë™ ë™ê¸°í™”)

ì£¼ì˜:
  - Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤ (credentials ì„¤ì • ê¶Œì¥)
  - CSV íŒŒì¼ì€ troubleshooting ìš©ë„ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤
  - .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_ID ì„¤ì • í•„ìš”
        """
    )
    # ê¸°ì¡´ ì˜µì…˜
    parser.add_argument("--display", type=int, default=100,
                        help="ë„¤ì´ë²„ APIì—ì„œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 100)")
    parser.add_argument("--start", type=int, default=1,
                        help="ë„¤ì´ë²„ API ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸: 1)")
    parser.add_argument("--sort", type=str, default="date", choices=["date", "sim"],
                        help="ì •ë ¬ ë°©ì‹: date(ìµœì‹ ìˆœ) ë˜ëŠ” sim(ê´€ë ¨ë„ìˆœ) (ê¸°ë³¸: date)")
    parser.add_argument("--outdir", type=str, default="data",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)")
    parser.add_argument("--max_competitor_classify", type=int, default=0,
                        help="ê²½ìŸì‚¬ë³„ ë¶„ë¥˜í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 0=ë¬´ì œí•œ, ì „ì²´ ë¶„ì„)")
    parser.add_argument("--chunk_size", type=int, default=100,
                        help="AI ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 100)")
    parser.add_argument("--max_workers", type=int, default=3,
                        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 3, ê¶Œì¥: 3-10)")
    parser.add_argument("--dry_run", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # Google Sheets ì˜µì…˜ (í•­ìƒ í™œì„±í™”ë¨)
    parser.add_argument("--sheets_id", type=str, default=None,
                        help="Google Sheets ID (.envì˜ GOOGLE_SHEET_ID ëŒ€ì‹  ì‚¬ìš©)")

    # API í˜ì´ì§€ë„¤ì´ì…˜ ì˜µì…˜
    parser.add_argument("--max_api_pages", type=int, default=9,
                        help="API í˜ì´ì§€ë„¤ì´ì…˜ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 9, ì¿¼í„° 90%% ì•ˆì „ ë§ˆì§„)")

    # Raw only ì˜µì…˜
    parser.add_argument("--raw_only", action="store_true",
                        help="AI ë¶„ë¥˜ ì—†ì´ API ìˆ˜ì§‘ + Google Sheets ì—…ë¡œë“œë§Œ ì‹¤í–‰")

    # Preprocess only ì˜µì…˜
    parser.add_argument("--preprocess_only", action="store_true",
                        help="ìˆ˜ì§‘ + ì „ì²˜ë¦¬ê¹Œì§€ë§Œ ì‹¤í–‰ (AI ë¶„ë¥˜, ë¦¬í¬íŠ¸ ìƒëµ, Sheets ì—…ë¡œë“œëŠ” ì‹¤í–‰)")

    # Keyword extraction ì˜µì…˜
    parser.add_argument("--extract_keywords", action="store_true",
                        help="(ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, í•­ìƒ ìë™ ì‹¤í–‰ë¨) ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ")
    parser.add_argument("--keyword_top_k", type=int, default=20,
                        help="í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ìƒìœ„ Kê°œ ì„ íƒ (ê¸°ë³¸: 20)")

    # BOM ì •ë¦¬ ì˜µì…˜
    parser.add_argument("--clean_bom", action="store_true",
                        help="Google Sheets ì „ì²´ BOM ë¬¸ì ì •ë¦¬ í›„ ì¢…ë£Œ")

    args = parser.parse_args()

    # CLI args ë¡œê¹…
    logger.log("cli_args", vars(args))
    logger.log_event("run_started", {"cli_args": vars(args)})

    print("=" * 80)
    print("ğŸš€ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - ìš°ë¦¬ ë¸Œëœë“œ: {', '.join(OUR_BRANDS)}")
    print(f"  - ê²½ìŸì‚¬: {', '.join(COMPETITORS)}")
    print(f"  - ìˆ˜ì§‘ ëª¨ë“œ: Naver API")
    print(f"  - ê¸°ì‚¬ ìˆ˜: {args.display}ê°œ/ë¸Œëœë“œ (ìµœëŒ€ {args.max_api_pages} í˜ì´ì§€)")
    print(f"  - ë‚ ì§œ í•„í„°: 2026-02-01 ì´í›„ë§Œ ë¶„ì„")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.outdir}/")
    if args.max_competitor_classify == 0:
        print(f"  - ë¶„ë¥˜ ëª¨ë“œ: ìì‚¬+ê²½ìŸì‚¬ ì „ì²´ ë¶„ì„")
    else:
        print(f"  - ë¶„ë¥˜ ëª¨ë“œ: ìì‚¬ ì „ì²´ + ê²½ìŸì‚¬ ìµœëŒ€ {args.max_competitor_classify}ê°œ/ë¸Œëœë“œ")
    print(f"  - AI ì²­í¬ í¬ê¸°: {args.chunk_size}")
    print(f"  - ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤: {args.max_workers}ê°œ")
    print(f"  - í‚¤ì›Œë“œ ì¶”ì¶œ: ìë™ ì‹¤í–‰ (ìƒìœ„ {args.keyword_top_k}ê°œ)")
    if args.dry_run:
        print(f"  - ëª¨ë“œ: DRY RUN (AI ë¶„ë¥˜ ìƒëµ)")
    if args.raw_only:
        print(f"  - ëª¨ë“œ: RAW ONLY (API ìˆ˜ì§‘ + Sheets ì—…ë¡œë“œë§Œ)")
    if args.preprocess_only:
        print(f"  - ëª¨ë“œ: PREPROCESS ONLY (ìˆ˜ì§‘ + ì „ì²˜ë¦¬ + Sheets ì—…ë¡œë“œ)")
    print()

    # Step 0: í™˜ê²½ ì„¤ì •
    env = load_env()
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    media_csv_path = outdir / "media_directory.csv"

    # Step 1: ìˆ˜ì§‘
    logger.start_stage("collection")
    print("\n" + "=" * 80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("=" * 80)

    # Google Sheets ì—°ê²° (ì£¼ ì €ì¥ì†Œ)
    existing_links = set()
    spreadsheet = None
    def record_error(message, data=None, category="system"):
        logger.log_error(message, data, category=category)
        if spreadsheet:
            if logger.flush_logs_to_sheets(spreadsheet):
                logger.log("sheets_logs_uploaded", len(logger._logs))

    # Error callback registration for OpenAI wrappers (log failures only)
    set_llm_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))
    set_batch_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))
    set_pr_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))
    set_media_error_callback(lambda msg, data=None: record_error(msg, data, category="openai_api"))

    # Google Sheets ìë™ ì—°ê²° (credentials í•„ìˆ˜ ê¶Œì¥)
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json")
    sheet_id = args.sheets_id or os.getenv("GOOGLE_SHEET_ID")

    if os.path.exists(creds_path) and sheet_id:
        try:
            print("\nğŸ“Š Google Sheets ì—°ê²° ì¤‘...")
            spreadsheet = connect_sheets(creds_path, sheet_id)
            if spreadsheet:
                # Sheets ê¸°ì¡´ BOM ìë™ ì •ë¦¬ (ë°ì´í„° ë¡œë“œ ì „)
                print("ğŸ§¹ Sheets BOM ë¬¸ì ìë™ ì •ë¦¬ ì¤‘...")
                bom_results = clean_all_bom_in_sheets(spreadsheet)
                bom_total = sum(bom_results.values())
                if bom_total > 0:
                    print(f"  âœ… {bom_total}ê°œ ì…€ BOM ì œê±° ì™„ë£Œ")

                existing_links = load_existing_links_from_sheets(spreadsheet)
                print("âœ… Google Sheets ì—°ê²° ì„±ê³µ (ì£¼ ì €ì¥ì†Œ)")
                print("   CSV íŒŒì¼ì€ troubleshootingìš©ìœ¼ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.")
                logger.log_event("sheets_connected", {"sheet_id": sheet_id}, category="sheets_sync")
                if logger.flush_logs_to_sheets(spreadsheet):
                    logger.log("sheets_logs_uploaded", len(logger._logs))
        except Exception as e:
            record_error(f"Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}", {"sheet_id": sheet_id}, category="sheets_sync")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
            logger.log_event("sheets_connect_failed", {"error": str(e)}, category="sheets_sync")
    else:
        print("\n" + "="*80)
        print("âš ï¸  ê²½ê³ : Google Sheets ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("="*80)
        if not os.path.exists(creds_path):
            print(f"  credential íŒŒì¼ ì—†ìŒ: {creds_path}")
        if not sheet_id:
            print(f"  GOOGLE_SHEET_ID í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
        print("\n  Google SheetsëŠ” ì£¼ ì €ì¥ì†Œì…ë‹ˆë‹¤. ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print("  í˜„ì¬ëŠ” CSV íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("="*80 + "\n")
        logger.log_event("sheets_not_configured", {"credentials_path": creds_path, "sheet_id": sheet_id}, category="sheets_sync")

    # --clean_bom ëª¨ë“œ: Sheets ì „ì²´ BOM ì •ë¦¬ í›„ ì¢…ë£Œ
    if args.clean_bom:
        if not spreadsheet:
            print("âŒ --clean_bom ì‚¬ìš© ì‹œ Google Sheets ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATHì™€ GOOGLE_SHEET_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            return
        print("\n" + "=" * 80)
        print("BOM ë¬¸ì ì •ë¦¬ ëª¨ë“œ")
        print("=" * 80)
        results = clean_all_bom_in_sheets(spreadsheet)
        total_cleaned = sum(results.values())
        print(f"\nâœ… BOM ì •ë¦¬ ì™„ë£Œ: ì´ {total_cleaned}ê°œ ì…€ ì •ë¦¬")
        return

    raw_csv_path = outdir / "raw.csv"

    # API ë°©ì‹ ìˆ˜ì§‘ (raw.csv ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)
    df_raw_new = collect_all_news(
        OUR_BRANDS, COMPETITORS,
        args.display, args.max_api_pages, args.sort,
        env["naver_id"], env["naver_secret"],
        raw_csv_path=str(raw_csv_path),
        spreadsheet=spreadsheet
    )

    # API ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
    if len(df_raw_new) == 0:
        print("\nâ„¹ï¸  APIì—ì„œ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ… APIì—ì„œ {len(df_raw_new)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    # Filter new articles (skip duplicates from Google Sheets)
    existing_links_skipped = 0
    if len(existing_links) > 0 and len(df_raw_new) > 0:
        before_filter = len(df_raw_new)
        df_raw_new = filter_new_articles_from_sheets(df_raw_new, existing_links)
        existing_links_skipped = before_filter - len(df_raw_new)

    # raw.csv ì—…ë°ì´íŠ¸ (append)
    if raw_csv_path.exists():
        df_raw_existing = pd.read_csv(raw_csv_path, encoding='utf-8-sig')
        df_raw = pd.concat([df_raw_existing, df_raw_new], ignore_index=True)
        df_raw = df_raw.drop_duplicates(subset=['link'], keep='last')
        print(f"ğŸ“‚ ê¸°ì¡´ raw.csv ì—…ë°ì´íŠ¸: {len(df_raw_existing)} + {len(df_raw_new)} = {len(df_raw)}ê°œ ê¸°ì‚¬")
    else:
        df_raw = df_raw_new

    save_csv(df_raw, raw_csv_path)

    # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ìˆ˜ì§‘ ì§í›„)
    if spreadsheet and len(df_raw_new) > 0:
        print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (raw_data)...")
        try:
            from src.modules.export.sheets import sync_to_sheets
            sync_result = sync_to_sheets(df_raw, spreadsheet, "raw_data")
            msg_parts = [
                f"{sync_result.get('attempted', 0)}ê°œ ì‹œë„",
                f"{sync_result.get('added', 0)}ê°œ ì¶”ê°€"
            ]
            if sync_result.get('updated', 0) > 0:
                msg_parts.append(f"{sync_result['updated']}ê°œ ì—…ë°ì´íŠ¸")
            msg_parts.append(f"{sync_result.get('skipped', 0)}ê°œ ê±´ë„ˆëœ€")
            print(f"âœ… raw_data ì‹œíŠ¸ ë™ê¸°í™” ì™„ë£Œ: {', '.join(msg_parts)}")
            logger.log_event("sheets_sync_raw_data", sync_result, category="sheets_sync")
        except Exception as e:
            record_error(f"raw_data ì‹œíŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

    # ìˆ˜ì§‘ ë‹¨ê³„ ë©”íŠ¸ë¦­
    articles_per_query = df_raw_new.groupby('query').size().to_dict() if 'query' in df_raw_new.columns else {}
    logger.log_dict({
        "articles_collected_total": len(df_raw_new),
        "articles_collected_per_query": articles_per_query,
        "existing_links_skipped": existing_links_skipped
    })
    logger.log_event("collection_completed", {
        "articles_collected_total": len(df_raw_new),
        "articles_collected_per_query": articles_per_query,
        "existing_links_skipped": existing_links_skipped
    })
    if spreadsheet:
        if logger.flush_logs_to_sheets(spreadsheet):
            logger.log("sheets_logs_uploaded", len(logger._logs))
    logger.end_stage("collection")

    # STEP 1.5: ë¯¸ì²˜ë¦¬/ë¯¸ë¶„ì„ í–‰ í•„í„°ë§
    result_csv_path = outdir / "result.csv"
    df_to_process = df_raw_new  # ê¸°ë³¸: ì‹ ê·œ ìˆ˜ì§‘ ê¸°ì‚¬ë§Œ ì²˜ë¦¬

    # 1) Google Sheets ê¸°ì¤€ ì„ ë³„ (ìš°ì„ )
    if spreadsheet:
        try:
            sheet_status = load_analysis_status_from_sheets(spreadsheet, sheet_name="total_result")
            processed_links = sheet_status.get("processed_links", set())
            missing_links = sheet_status.get("missing_analysis_links", set())

            if processed_links:
                unprocessed_rows = df_raw[~df_raw['link'].isin(processed_links)]
            else:
                unprocessed_rows = df_raw

            if missing_links:
                reanalyze_rows = df_raw[df_raw['link'].isin(missing_links)]
                df_to_process = pd.concat([unprocessed_rows, reanalyze_rows], ignore_index=True)
                df_to_process = df_to_process.drop_duplicates(subset=['link'], keep='first')
            else:
                df_to_process = unprocessed_rows

            print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸ (Sheets ê¸°ì¤€):")
            print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
            print(f"  - ì´ë¯¸ ì²˜ë¦¬ë¨(ì‹œíŠ¸): {len(processed_links)}ê°œ")
            print(f"  - ë¯¸ì²˜ë¦¬ í–‰: {len(unprocessed_rows)}ê°œ")
            if missing_links:
                print(f"  - ë¶„ì„ ëˆ„ë½(ì‹œíŠ¸): {len(missing_links)}ê°œ")
            print(f"  - ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(df_to_process)}ê°œ")

        except Exception as e:
            record_error(f"Google Sheets ë¶„ì„ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}", category="sheets_sync")
            df_to_process = df_raw_new

    # 2) Google Sheets ë¯¸ì—°ê²° ì‹œ result.csv ê¸°ì¤€
    elif result_csv_path.exists():
        try:
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')

            # 1. Link ê¸°ì¤€ ë¯¸ì²˜ë¦¬ í–‰ (ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬)
            if 'link' in df_result_existing.columns:
                processed_links = set(df_result_existing['link'].dropna().tolist())
                unprocessed_rows = df_raw[~df_raw['link'].isin(processed_links)]
            else:
                unprocessed_rows = df_raw

            # 2. LLM ë¶„ì„ ë˜ëŠ” ì „ì²˜ë¦¬ í•„ë“œ ë¹„ì–´ìˆëŠ” í–‰ (ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ ë¯¸ì™„ë£Œ)
            analysis_cols_to_check = [
                'brand_relevance', 'sentiment_stage',  # LLM ë¶„ì„
                'source', 'media_domain', 'date_only'  # ì „ì²˜ë¦¬ í•„ë“œ
            ]
            # ìµœì†Œ í•˜ë‚˜ì˜ í•„ë“œë¼ë„ ì¡´ì¬í•˜ë©´ ì²´í¬
            existing_cols = [col for col in analysis_cols_to_check if col in df_result_existing.columns]
            if existing_cols:
                # BOM ë¬¸ì í¬í•¨ ë¹ˆ ê°’ ì²´í¬ (clean_bom ì¬ì‚¬ìš©)
                def is_empty_or_bom(val):
                    return clean_bom(val) == ""

                # LLM ë¶„ì„ ë˜ëŠ” ì „ì²˜ë¦¬ í•„ë“œê°€ ë¹„ì–´ìˆëŠ” í–‰ ì°¾ê¸° (BOM ë¬¸ì í¬í•¨)
                missing_analysis = df_result_existing[
                    df_result_existing['brand_relevance'].apply(is_empty_or_bom) |
                    df_result_existing['source'].apply(is_empty_or_bom) |
                    df_result_existing['date_only'].apply(is_empty_or_bom)
                ].copy()

                if len(missing_analysis) > 0:
                    # raw.csvì—ì„œ í•´ë‹¹ ë§í¬ë“¤ ì°¾ê¸°
                    missing_links = set(missing_analysis['link'].dropna().tolist())
                    reanalyze_rows = df_raw[df_raw['link'].isin(missing_links)]

                    # ë¯¸ì²˜ë¦¬ í–‰ + ì¬ë¶„ì„ í–‰ í•©ì¹˜ê¸°
                    df_to_process = pd.concat([unprocessed_rows, reanalyze_rows], ignore_index=True)
                    df_to_process = df_to_process.drop_duplicates(subset=['link'], keep='first')

                    print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                    print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                    print(f"  - ì‹ ê·œ ë¯¸ì²˜ë¦¬: {len(unprocessed_rows)}ê°œ")
                    print(f"  - ë¶„ì„ ëˆ„ë½: {len(reanalyze_rows)}ê°œ")
                    print(f"  - ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(df_to_process)}ê°œ")
                else:
                    df_to_process = unprocessed_rows
                    print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                    print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                    print(f"  - ì´ë¯¸ ì²˜ë¦¬ë¨: {len(processed_links)}ê°œ")
                    print(f"  - ë¯¸ì²˜ë¦¬ í–‰: {len(unprocessed_rows)}ê°œ")
            else:
                df_to_process = unprocessed_rows
                print(f"\nğŸ“Š ì²˜ë¦¬ ìƒíƒœ í™•ì¸:")
                print(f"  - ì „ì²´ raw.csv: {len(df_raw)}ê°œ")
                print(f"  - ë¯¸ì²˜ë¦¬ í–‰: {len(unprocessed_rows)}ê°œ")

        except Exception as e:
            record_error(f"result.csv ë¡œë“œ ì‹¤íŒ¨: {e}", category="system")
            df_to_process = df_raw
    else:
        print(f"\nğŸ“Š result.csvê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ raw.csv {len(df_raw)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
        df_to_process = df_raw  # ì „ì²´ raw.csv ì²˜ë¦¬

    if len(df_to_process) == 0:
        print("â„¹ï¸  ì²˜ë¦¬í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ Google Sheets ë™ê¸°í™” ì‹œë„
        if spreadsheet and result_csv_path.exists():
            logger.start_stage("sheets_sync")
            print("\n" + "=" * 80)
            print("STEP 5: Google Sheets ì—…ë¡œë“œ (ê¸°ì¡´ ë°ì´í„°)")
            print("=" * 80)
            try:
                df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                sync_results = sync_raw_and_processed(df_raw, df_result_existing, spreadsheet)
                print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")

                # Sheets ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (added + updated)
                raw_sync = sync_results.get("raw_data", {})
                result_sync = sync_results.get("total_result", {})
                logger.log_dict({
                    "sheets_sync_enabled": True,
                    "sheets_rows_uploaded_raw": raw_sync.get("added", 0) + raw_sync.get("updated", 0),
                    "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0)
                })
            except Exception as e:
                record_error(f"Google Sheets ì—…ë¡œë“œ ì‹¤íŒ¨ (ê¸°ì¡´ ë°ì´í„°): {e}", category="sheets_sync")
                logger.log("sheets_sync_enabled", False)
            logger.end_stage("sheets_sync")
            logger.log_event("sheets_sync_completed", {
                "sheets_rows_uploaded_raw": logger.metrics.get("sheets_rows_uploaded_raw", 0),
                "sheets_rows_uploaded_result": logger.metrics.get("sheets_rows_uploaded_result", 0)
            }, category="sheets_sync")
            if spreadsheet:
                if logger.flush_logs_to_sheets(spreadsheet):
                    logger.log("sheets_logs_uploaded", len(logger._logs))

        # ë¡œê·¸ ì €ì¥
        logger.finalize()
        logs_csv_path = outdir / "logs" / "run_history.csv"
        logger.save_csv(str(logs_csv_path))

        # Sheets ë¡œê·¸ ë™ê¸°í™”
        if spreadsheet:
            sync_logs_to_sheets(str(logs_csv_path), spreadsheet)

        logger.print_summary()

        print("\n" + "=" * 80)
        print("âœ… ì‘ì—… ì™„ë£Œ (ì‹ ê·œ ì²˜ë¦¬ ì—†ìŒ)")
        print("=" * 80)
        return

    # --raw_only ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬/ë¶„ë¥˜/ë¦¬í¬íŠ¸ ìŠ¤í‚µ
    if args.raw_only:
        df_result = df_raw
        logger.log_dict({
            "articles_processed": 0,
            "duplicates_removed": 0,
            "articles_filtered_by_date": 0,
            "press_releases_detected": 0,
            "press_release_groups": 0
        })
    # --preprocess_only ëª¨ë“œì¸ ê²½ìš° ë¶„ë¥˜ ìŠ¤í‚µ, ì²˜ë¦¬+ë¦¬í¬íŠ¸ëŠ” ì‹¤í–‰
    elif args.preprocess_only:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        logger.start_stage("processing")
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)
        # Step 2-1: Normalize
        df_normalized = normalize_df(df_to_process)

        # Step 2-1.5: Date filter (2026-02-01 ì´í›„ë§Œ)
        print("ğŸ”§ ë‚ ì§œ í•„í„°ë§ ì¤‘ (2026-02-01 ì´í›„ë§Œ)...")
        before_filter = len(df_normalized)
        df_normalized['pub_datetime_dt'] = pd.to_datetime(df_normalized['pub_datetime'], errors='coerce')
        df_normalized = df_normalized[df_normalized['pub_datetime_dt'] >= '2026-02-01'].copy()
        df_normalized = df_normalized.drop(columns=['pub_datetime_dt'])
        filtered_count = before_filter - len(df_normalized)
        print(f"âœ… {filtered_count}ê°œ ê¸°ì‚¬ í•„í„°ë§ë¨ (2026-02-01 ì´ì „), {len(df_normalized)}ê°œ ê¸°ì‚¬ ìœ ì§€")
        before_dedupe = len(df_normalized)

        # Step 2-2: Deduplicate
        df_processed = dedupe_df(df_normalized)
        duplicates_removed = before_dedupe - len(df_processed)

        # ì¤‘ê°„ ë™ê¸°í™” (ì¤‘ë³µ ì œê±° ì™„ë£Œ í›„)
        if spreadsheet:
            print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)...")
            if result_csv_path.exists():
                df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
                df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
            else:
                df_temp = df_processed
            save_csv(df_temp, result_csv_path)
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ì¤‘ë³µ ì œê±°)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ì¤‘ë³µ ì œê±°): {e}", category="sheets_sync")

        # Step 2-3: Detect similar articles (Press Release)
        df_processed = detect_similar_articles(df_processed)
        press_releases = len(df_processed[df_processed['source'] == 'ë³´ë„ìë£Œ']) if 'source' in df_processed.columns else 0

        # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ íƒì§€ ì™„ë£Œ í›„)
        if spreadsheet:
            print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ë³´ë„ìë£Œ íƒì§€ ì™„ë£Œ)...")
            if result_csv_path.exists():
                df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
                df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
            else:
                df_temp = df_processed
            save_csv(df_temp, result_csv_path)
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ë³´ë„ìë£Œ íƒì§€)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ë³´ë„ìë£Œ íƒì§€): {e}", category="sheets_sync")

        # Step 2-4: Summarize press release groups (OpenAI)
        df_processed = summarize_press_release_groups(df_processed, env["openai_key"])

        # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ ìš”ì•½ ì™„ë£Œ í›„)
        print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ë³´ë„ìë£Œ ìš”ì•½ ì™„ë£Œ)...")
        if result_csv_path.exists():
            df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
            df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
        else:
            df_temp = df_processed
        save_csv(df_temp, result_csv_path)
        if spreadsheet:
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ë³´ë„ìë£Œ ìš”ì•½)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ë³´ë„ìë£Œ ìš”ì•½): {e}", category="sheets_sync")

        # Step 2-5: Media classification (OpenAI)
        media_csv_path = outdir / "media_directory.csv"
        df_processed = enrich_with_media_info(
            df_processed,
            spreadsheet=spreadsheet,  # Noneì´ë©´ CSV-only ëª¨ë“œ
            openai_key=env["openai_key"],
            csv_path=media_csv_path
        )

        # ì¤‘ê°„ ë™ê¸°í™” (ì–¸ë¡ ì‚¬ ì •ë³´ ì™„ë£Œ í›„)
        print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ì™„ë£Œ)...")
        if result_csv_path.exists():
            df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
            df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
        else:
            df_temp = df_processed
        save_csv(df_temp, result_csv_path)
        if spreadsheet:
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜): {e}", category="sheets_sync")

        # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜
        df_processed = df_processed.fillna("")

        # Step 2-6: Looker Studio time-series columns
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_processed = add_time_series_columns(df_processed)

        # ì „ì²˜ë¦¬ ë©”íŠ¸ë¦­
        press_release_groups = df_processed['group_id'].nunique() if 'group_id' in df_processed.columns else 0
        logger.log_dict({
            "articles_processed": len(df_processed),
            "duplicates_removed": duplicates_removed,
            "articles_filtered_by_date": filtered_count,
            "press_releases_detected": press_releases,
            "press_release_groups": press_release_groups
        })
        logger.end_stage("processing")
        logger.log_event("processing_completed", {
            "articles_processed": len(df_processed),
            "duplicates_removed": duplicates_removed,
            "articles_filtered_by_date": filtered_count,
            "press_releases_detected": press_releases,
            "press_release_groups": press_release_groups
        })
        if spreadsheet:
            if logger.flush_logs_to_sheets(spreadsheet):
                logger.log("sheets_logs_uploaded", len(logger._logs))

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_result = pd.concat([df_result_existing, df_processed], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"ğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_processed)} = {len(df_result)}ê°œ ê¸°ì‚¬")
        else:
            df_result = df_processed

        # ê²°ê³¼ ì €ì¥
        save_csv(df_result, result_csv_path)

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ì „ì²˜ë¦¬ ì™„ë£Œ ì§í›„)
        if spreadsheet:
            print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (ì „ì²˜ë¦¬ ê²°ê³¼)...")
            try:
                sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
                print("âœ… ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
            except Exception as e:
                record_error(f"ì „ì²˜ë¦¬ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")
    else:
        # Step 2: ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        logger.start_stage("processing")
        print("\n" + "=" * 80)
        print("STEP 2: ë°ì´í„° ì²˜ë¦¬ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)")
        print("=" * 80)

        # Step 2-1: Normalize
        df_normalized = normalize_df(df_to_process)

        # Step 2-1.5: Date filter (2026-02-01 ì´í›„ë§Œ)
        print("ğŸ”§ ë‚ ì§œ í•„í„°ë§ ì¤‘ (2026-02-01 ì´í›„ë§Œ)...")
        before_filter = len(df_normalized)
        df_normalized['pub_datetime_dt'] = pd.to_datetime(df_normalized['pub_datetime'], errors='coerce')
        df_normalized = df_normalized[df_normalized['pub_datetime_dt'] >= '2026-02-01'].copy()
        df_normalized = df_normalized.drop(columns=['pub_datetime_dt'])
        filtered_count = before_filter - len(df_normalized)
        print(f"âœ… {filtered_count}ê°œ ê¸°ì‚¬ í•„í„°ë§ë¨ (2026-02-01 ì´ì „), {len(df_normalized)}ê°œ ê¸°ì‚¬ ìœ ì§€")
        before_dedupe = len(df_normalized)

        # Step 2-2: Deduplicate
        df_processed = dedupe_df(df_normalized)
        duplicates_removed = before_dedupe - len(df_processed)

        # ì¤‘ê°„ ë™ê¸°í™” (ì¤‘ë³µ ì œê±° ì™„ë£Œ í›„)
        if spreadsheet:
            print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)...")
            if result_csv_path.exists():
                df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
                df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
            else:
                df_temp = df_processed
            save_csv(df_temp, result_csv_path)
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ì¤‘ë³µ ì œê±°)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ì¤‘ë³µ ì œê±°): {e}", category="sheets_sync")

        # Step 2-3: Detect similar articles (Press Release)
        df_processed = detect_similar_articles(df_processed)
        press_releases = len(df_processed[df_processed['source'] == 'ë³´ë„ìë£Œ']) if 'source' in df_processed.columns else 0

        # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ íƒì§€ ì™„ë£Œ í›„)
        if spreadsheet:
            print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ë³´ë„ìë£Œ íƒì§€ ì™„ë£Œ)...")
            if result_csv_path.exists():
                df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
                df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
                df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
            else:
                df_temp = df_processed
            save_csv(df_temp, result_csv_path)
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ë³´ë„ìë£Œ íƒì§€)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ë³´ë„ìë£Œ íƒì§€): {e}", category="sheets_sync")

        # Step 2-4: Summarize press release groups (OpenAI)
        df_processed = summarize_press_release_groups(df_processed, env["openai_key"])

        # ì¤‘ê°„ ë™ê¸°í™” (ë³´ë„ìë£Œ ìš”ì•½ ì™„ë£Œ í›„)
        print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ë³´ë„ìë£Œ ìš”ì•½ ì™„ë£Œ)...")
        if result_csv_path.exists():
            df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
            df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
        else:
            df_temp = df_processed
        save_csv(df_temp, result_csv_path)
        if spreadsheet:
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ë³´ë„ìë£Œ ìš”ì•½)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ë³´ë„ìë£Œ ìš”ì•½): {e}", category="sheets_sync")

        # Step 2-5: Media classification (OpenAI)
        media_csv_path = outdir / "media_directory.csv"
        df_processed = enrich_with_media_info(
            df_processed,
            spreadsheet=spreadsheet,  # Noneì´ë©´ CSV-only ëª¨ë“œ
            openai_key=env["openai_key"],
            csv_path=media_csv_path
        )

        # ì¤‘ê°„ ë™ê¸°í™” (ì–¸ë¡ ì‚¬ ì •ë³´ ì™„ë£Œ í›„)
        print("ğŸ’¾ ì¤‘ê°„ ë™ê¸°í™” ì¤‘ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ì™„ë£Œ)...")
        if result_csv_path.exists():
            df_result_temp = pd.read_csv(result_csv_path, encoding='utf-8-sig')
            df_temp = pd.concat([df_result_temp, df_processed], ignore_index=True)
            df_temp = df_temp.drop_duplicates(subset=['link'], keep='last')
        else:
            df_temp = df_processed
        save_csv(df_temp, result_csv_path)
        if spreadsheet:
            try:
                from src.modules.export.sheets import sync_raw_and_processed
                sync_raw_and_processed(df_raw, df_temp, spreadsheet)
                print("âœ… Sheets ë™ê¸°í™” ì™„ë£Œ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜)")
            except Exception as e:
                record_error(f"Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ì–¸ë¡ ì‚¬ ë¶„ë¥˜): {e}", category="sheets_sync")

        # ì „ì²˜ë¦¬ ë©”íŠ¸ë¦­
        press_release_groups = df_processed['group_id'].nunique() if 'group_id' in df_processed.columns else 0
        logger.log_dict({
            "articles_processed": len(df_processed),
            "duplicates_removed": duplicates_removed,
            "articles_filtered_by_date": filtered_count,
            "press_releases_detected": press_releases,
            "press_release_groups": press_release_groups
        })
        logger.end_stage("processing")

        # Step 3: ë¶„ë¥˜ (ë¯¸ì²˜ë¦¬ í–‰ë§Œ)
        logger.start_stage("classification")
        print("\n" + "=" * 80)
        print("STEP 3: ë¶„ë¥˜")
        print("=" * 80)

        # Step 3-1: ë³´ë„ìë£Œ ì „ì²˜ë¦¬ (LLM ìŠ¤í‚µìš© ê³ ì •ê°’ ì„¤ì •)
        df_processed = preset_press_release_values(df_processed)

        # Step 3-2: LLM ë¶„ë¥˜ (ë³´ë„ìë£ŒëŠ” ìŠ¤í‚µ)
        df_classified, llm_metrics = classify_llm(
            df_processed,
            env["openai_key"],
            chunk_size=args.chunk_size,
            dry_run=args.dry_run,
            max_competitor_classify=args.max_competitor_classify,
            max_workers=args.max_workers,
            result_csv_path=str(result_csv_path),
            spreadsheet=spreadsheet,
            raw_df=df_raw
        )

        # LLM ë©”íŠ¸ë¦­ ë¡œê¹…
        logger.log_dict(llm_metrics)

        # í†µê³„ ì¶œë ¥
        stats = get_classification_stats(df_classified)
        print_classification_stats(stats)

        # ë³´ë„ìë£Œ ì •ë³´ ë° ì–¸ë¡ ì‚¬ ì •ë³´ ë³‘í•©
        source_cols = ['link', 'source', 'cluster_id', 'press_release_group', 'media_domain', 'media_name', 'media_group',
                       'media_type']
        source_data = df_processed[source_cols].copy()

        # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        cols_to_drop = [col for col in df_classified.columns if col in source_cols and col != 'link']
        df_classified = df_classified.drop(columns=cols_to_drop, errors='ignore')

        # merge
        df_classified = df_classified.merge(source_data, on='link', how='left')

        # ë‚˜ë¨¸ì§€ NaN â†’ ê³µë€ ë³€í™˜ (FutureWarning ë°©ì§€)
        df_classified = df_classified.fillna("").infer_objects(copy=False)

        # Step 3.7: Looker Studio ì¤€ë¹„ (í•­ìƒ ì‹¤í–‰)
        print("\nğŸ•’ Looker Studio ì‹œê³„ì—´ ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        df_classified = add_time_series_columns(df_classified)

        logger.end_stage("classification")
        logger.log_event("classification_completed", {
            "articles_classified_llm": llm_metrics.get("articles_classified_llm", 0),
            "llm_api_calls": llm_metrics.get("llm_api_calls", 0),
            "press_releases_skipped": llm_metrics.get("press_releases_skipped", 0),
            "classification_errors": llm_metrics.get("classification_errors", 0)
        })
        if spreadsheet:
            if logger.flush_logs_to_sheets(spreadsheet):
                logger.log("sheets_logs_uploaded", len(logger._logs))

        # ê¸°ì¡´ result.csvì™€ ë³‘í•©
        if result_csv_path.exists():
            df_result_existing = pd.read_csv(result_csv_path, encoding='utf-8-sig', on_bad_lines='skip', engine='python')
            df_result = pd.concat([df_result_existing, df_classified], ignore_index=True)
            df_result = df_result.drop_duplicates(subset=['link'], keep='last')
            print(f"\nğŸ“‚ ê¸°ì¡´ result.csv ì—…ë°ì´íŠ¸: {len(df_result_existing)} + {len(df_classified)} = {len(df_result)}ê°œ ê¸°ì‚¬ (ì¤‘ë³µ ì œê±° í›„)")
        else:
            df_result = df_classified

        # pub_datetimeì„ ëª…ì‹œì ìœ¼ë¡œ datetimeìœ¼ë¡œ ë³€í™˜ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
        if 'pub_datetime' in df_result.columns:
            df_result['pub_datetime'] = pd.to_datetime(df_result['pub_datetime'], errors='coerce')

        # ê²°ê³¼ ì €ì¥ (ë‹¨ì¼ CSV íŒŒì¼)
        save_csv(df_result, result_csv_path)

        # Google Sheets ì¦‰ì‹œ ë™ê¸°í™” (ë¶„ë¥˜ ì™„ë£Œ ì§í›„)
        if spreadsheet:
            print("\nğŸ“Š Google Sheets ì¦‰ì‹œ ë™ê¸°í™” ì¤‘ (ë¶„ë¥˜ ê²°ê³¼)...")
            try:
                sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
                print("âœ… ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì™„ë£Œ")
            except Exception as e:
                record_error(f"ë¶„ë¥˜ ê²°ê³¼ Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")

        # Step 4: ë¦¬í¬íŠ¸ ìƒì„±
        print("\n" + "=" * 80)
        print("STEP 4: ë¦¬í¬íŠ¸ ìƒì„±")
        print("=" * 80)

        # ì½˜ì†” ë¦¬í¬íŠ¸
        generate_console_report(df_result)

        # ë¶„ë¥˜ ê²°ê³¼ ë©”íŠ¸ë¦­
        our_brands_relevant = len(df_result[(df_result['group'] == 'OUR') & (df_result['brand_relevance'].isin(['ê´€ë ¨', 'ì–¸ê¸‰']))]) if 'brand_relevance' in df_result.columns else 0
        our_brands_negative = len(df_result[(df_result['group'] == 'OUR') & (df_result['sentiment_stage'].isin(['ë¶€ì • í›„ë³´', 'ë¶€ì • í™•ì •']))]) if 'sentiment_stage' in df_result.columns else 0
        danger_high = len(df_result[df_result['danger_level'] == 'ìƒ']) if 'danger_level' in df_result.columns else 0
        danger_medium = len(df_result[df_result['danger_level'] == 'ì¤‘']) if 'danger_level' in df_result.columns else 0
        competitor_articles = len(df_result[df_result['group'] == 'COMPETITOR']) if 'group' in df_result.columns else 0

        logger.log_dict({
            "our_brands_relevant": our_brands_relevant,
            "our_brands_negative": our_brands_negative,
            "danger_high": danger_high,
            "danger_medium": danger_medium,
            "competitor_articles": competitor_articles
        })

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ìë™ ì‹¤í–‰)
        print("\n" + "=" * 80)
        print("STEP 4.5: ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ")
        print("=" * 80)
        extract_all_categories(
            df=df_result,
            output_dir=outdir / "keywords",
            top_k=args.keyword_top_k,
            max_display=10,
            spreadsheet=spreadsheet  # Google Sheets ì—°ê²° ì „ë‹¬
        )

    # Step 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)
    if spreadsheet:
        logger.start_stage("sheets_sync")
        print("\n" + "=" * 80)
        print("STEP 5: Google Sheets ë™ê¸°í™” (ì£¼ ì €ì¥ì†Œ)")
        print("=" * 80)
        try:
            sync_results = sync_raw_and_processed(df_raw, df_result, spreadsheet)
            print("âœ… Google Sheets ë™ê¸°í™” ì™„ë£Œ")

            # Sheets ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (added + updated)
            raw_sync = sync_results.get("raw_data", {})
            result_sync = sync_results.get("total_result", {})
            logger.log_dict({
                "sheets_sync_enabled": True,
                "sheets_rows_uploaded_raw": raw_sync.get("added", 0) + raw_sync.get("updated", 0),
                "sheets_rows_uploaded_result": result_sync.get("added", 0) + result_sync.get("updated", 0)
            })
        except Exception as e:
            record_error(f"Google Sheets ë™ê¸°í™” ì‹¤íŒ¨ (ìµœì¢…): {e}", category="sheets_sync")
            print("   âš ï¸  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
            logger.log("sheets_sync_enabled", False)
        logger.end_stage("sheets_sync")
        logger.log_event("sheets_sync_completed", {
            "sheets_rows_uploaded_raw": logger.metrics.get("sheets_rows_uploaded_raw", 0),
            "sheets_rows_uploaded_result": logger.metrics.get("sheets_rows_uploaded_result", 0)
        }, category="sheets_sync")
        if spreadsheet:
            if logger.flush_logs_to_sheets(spreadsheet):
                logger.log("sheets_logs_uploaded", len(logger._logs))
    else:
        logger.log("sheets_sync_enabled", False)
        print("\n" + "=" * 80)
        print("âš ï¸  Google Sheets ì—°ê²° ì—†ìŒ")
        print("=" * 80)
        print("  ì£¼ ì €ì¥ì†Œì¸ Google Sheetsì— ë™ê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  CSV íŒŒì¼ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (troubleshooting ëª¨ë“œ)")
        print("  .env íŒŒì¼ì— GOOGLE_SHEETS_CREDENTIALS_PATH ë° GOOGLE_SHEET_ID ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    # ë¡œê·¸ ì €ì¥
    logger.end_stage("total")
    logger.finalize()

    logs_csv_path = outdir / "logs" / "run_history.csv"
    logger.save_csv(str(logs_csv_path))

    # Sheets run_history ë™ê¸°í™”
    if spreadsheet:
        try:
            sync_logs_to_sheets(str(logs_csv_path), spreadsheet)
            logger.log("sheets_run_history_uploaded", 1)
        except Exception as e:
            record_error(f"run_history Sheets ë™ê¸°í™” ì‹¤íŒ¨: {e}", category="sheets_sync")
            logger.log("sheets_run_history_uploaded", 0)

    # ë¡œê·¸ ìš”ì•½ ì¶œë ¥
    logger.print_summary()

    # ì™„ë£Œ
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    if spreadsheet:
        print(f"  â˜ï¸  Google Sheets - ë™ê¸°í™” ì™„ë£Œ (ì£¼ ì €ì¥ì†Œ)")
    print(f"  ğŸ“Š {outdir}/raw.csv - ì›ë³¸ ë°ì´í„° (troubleshooting)")
    if not args.raw_only:
        print(f"  ğŸ“Š {outdir}/result.csv - AI ë¶„ë¥˜ ê²°ê³¼ (troubleshooting)")
        print(f"  ğŸ“‚ {outdir}/keywords/ - ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ CSV")
    if spreadsheet or not args.raw_only:
        print(f"  ğŸ“‚ {outdir}/media_directory.csv - ì–¸ë¡ ì‚¬ ë””ë ‰í† ë¦¬")
    if not spreadsheet:
        print(f"\n  âš ï¸  Google Sheets ë¯¸ì—°ê²°: CSV íŒŒì¼ë§Œ ì €ì¥ë¨")
    print()


if __name__ == "__main__":
    main()
