#!/usr/bin/env python3
"""
main.py - News Monitoring System
ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import pandas as pd
from pathlib import Path

# Pandas FutureWarning ë°©ì§€
pd.set_option('future.no_silent_downcasting', True)

# Pipeline imports
from src.cli import parse_args
from src.pipeline.context import PipelineContext
from src.pipeline.setup import load_env, connect_sheets_and_setup, print_banner
from src.pipeline.step1_collection import run_collection
from src.pipeline.step2_processing import run_processing
from src.pipeline.step3_classification import run_classification
from src.pipeline.step4_reporting import run_reporting
from src.pipeline.step5_sheets_sync import run_sheets_sync
from src.pipeline.finalize import finalize_pipeline
from src.modules.monitoring.logger import RunLogger


def main():
    # Parse CLI & detect run mode
    args, run_mode = parse_args()

    # Initialize logger & context
    logger = RunLogger()
    env = load_env()
    outdir = Path(args.outdir)

    ctx = PipelineContext(
        args=args,
        run_mode=run_mode,
        env=env,
        outdir=outdir,
        logger=logger,
    )

    # Log run metadata
    logger.log("run_mode", run_mode)
    logger.log("cli_args", vars(args))
    logger.log_event("run_started", {"cli_args": vars(args)}, stage="init")

    # Setup: Sheets connection, error callbacks, banner
    print_banner(ctx)
    should_continue = connect_sheets_and_setup(ctx)
    if not should_continue:  # --clean_bom exits here
        return

    # â”€â”€ Sheets ë¯¸ì—°ê²° ì‹œ: raw ìˆ˜ì§‘ â†’ CSV ì €ì¥ â†’ ì¢…ë£Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ctx.spreadsheet is None:
        print("\nâš ï¸  Google Sheets ë¯¸ì—°ê²°: raw ìˆ˜ì§‘ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        _emergency_raw_collection(ctx)
        finalize_pipeline(ctx)
        return

    # â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ (Sheets ì—°ê²° í•„ìˆ˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 1: Collection + Reprocess + Date filter
    has_articles = run_collection(ctx)
    if not has_articles:
        finalize_pipeline(ctx)
        return

    # Step 2: Processing (normalize, dedupe, press release, media)
    run_processing(ctx)

    # Step 3: Classification (PR clusters, general LLM, source verification)
    run_classification(ctx)

    # Step 4: Reporting + Keywords
    run_reporting(ctx)

    # Step 5: Google Sheets sync
    run_sheets_sync(ctx)

    # Finalize: logs, summary, completion banner
    finalize_pipeline(ctx)


def _emergency_raw_collection(ctx):
    """Sheets ë¯¸ì—°ê²° ì‹œ Naver API raw ìˆ˜ì§‘ â†’ outdir/raw.csv ì €ì¥"""
    from src.modules.collection.collect import OUR_BRANDS, COMPETITORS, collect_all_news
    from src.modules.processing.process import save_csv

    ctx.current_stage = "collection"
    ctx.logger.start_stage("collection")

    outdir = ctx.outdir
    outdir.mkdir(parents=True, exist_ok=True)

    print("\n" + "=" * 80)
    print("STEP 1: ë‰´ìŠ¤ ìˆ˜ì§‘ (ë¹„ìƒ ëª¨ë“œ - Sheets ë¯¸ì—°ê²°)")
    print("=" * 80)

    df_raw = collect_all_news(
        OUR_BRANDS, COMPETITORS,
        ctx.args.display, ctx.args.max_api_pages, ctx.args.sort,
        ctx.env["naver_id"], ctx.env["naver_secret"],
        existing_links=set(),
        spreadsheet=None
    )

    raw_csv_path = outdir / "raw.csv"
    if len(df_raw) > 0:
        save_csv(df_raw, raw_csv_path)
        print(f"\nâœ… {len(df_raw)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"ğŸ’¾ raw.csv ì €ì¥: {raw_csv_path}")
    else:
        print("\nâ„¹ï¸  ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    ctx.df_raw = df_raw
    ctx.df_raw_new = df_raw

    articles_per_query = df_raw.groupby('query').size().to_dict() if 'query' in df_raw.columns else {}
    ctx.logger.log_dict({
        "articles_collected_total": len(df_raw),
        "articles_collected_per_query": articles_per_query,
        "existing_links_skipped": 0,
    })
    ctx.logger.end_stage("collection")


if __name__ == "__main__":
    main()
