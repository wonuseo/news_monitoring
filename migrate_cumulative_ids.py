"""
migrate_cumulative_ids.py - ê¸°ì¡´ ë°ì´í„°ì˜ article_noì™€ cluster_idë¥¼ cumulativeí•˜ê²Œ ì¬í• ë‹¹

ì‹¤í–‰ ë°©ë²•:
    python migrate_cumulative_ids.py

ë™ì‘:
1. Google Sheetsì˜ total_result ë°ì´í„° ì½ê¸°
2. pub_datetime ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê³¼ê±° â†’ ìµœì‹ )
3. article_no ì¬í• ë‹¹ (1ë¶€í„° ì‹œì‘, ê³¼ê±°ì¼ìˆ˜ë¡ ë‚®ì€ ìˆ«ì)
4. cluster_id ì¬í• ë‹¹ (queryë³„ë¡œ ì •ë ¬ í›„ ì¬í• ë‹¹, í˜•ì‹: {query}_c{5ìë¦¬ìˆ«ì})
5. Google Sheetsì— ì—…ë°ì´íŠ¸

ì£¼ì˜: ì´ ì‘ì—…ì€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤. ë°±ì—… ê¶Œì¥.
"""

import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()


def migrate_cumulative_ids():
    """ê¸°ì¡´ ë°ì´í„°ì˜ article_noì™€ cluster_idë¥¼ cumulativeí•˜ê²Œ ì¬í• ë‹¹"""

    # Google Sheets ì¸ì¦
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH")
    sheet_id = os.getenv("GOOGLE_SHEET_ID")

    if not creds_path or not sheet_id:
        print("âŒ GOOGLE_SHEETS_CREDENTIALS_PATH ë˜ëŠ” GOOGLE_SHEET_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ”„ Migration ì‹œì‘: article_noì™€ cluster_idë¥¼ cumulativeí•˜ê²Œ ì¬í• ë‹¹")
    print(f"   - Credentials: {creds_path}")
    print(f"   - Sheet ID: {sheet_id}")

    # Google Sheets ì—°ê²°
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
    gc = gspread.authorize(creds)
    spreadsheet = gc.open_by_key(sheet_id)

    # total_result ë°ì´í„° ì½ê¸°
    print("\nğŸ“– total_result ë°ì´í„° ì½ê¸° ì¤‘...")
    try:
        worksheet = spreadsheet.worksheet("total_result")
    except Exception as e:
        print(f"âŒ total_result ì›Œí¬ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return

    data = worksheet.get_all_records()
    if not data:
        print("âŒ total_resultì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(data)
    original_count = len(df)
    print(f"   âœ… {original_count}ê°œ í–‰ ì½ê¸° ì™„ë£Œ")

    # pub_datetime íŒŒì‹± ë° ì •ë ¬
    print("\nğŸ”§ pub_datetime ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ì¤‘ (ê³¼ê±° â†’ ìµœì‹ )...")
    df["pub_datetime_parsed"] = pd.to_datetime(df["pub_datetime"], errors="coerce")
    df = df.sort_values("pub_datetime_parsed", ascending=True).reset_index(drop=True)
    print(f"   âœ… ì •ë ¬ ì™„ë£Œ")

    # article_no ì¬í• ë‹¹ (1ë¶€í„° ì‹œì‘, ê³¼ê±°ì¼ìˆ˜ë¡ ë‚®ì€ ìˆ«ì)
    print("\nğŸ”¢ article_no ì¬í• ë‹¹ ì¤‘...")
    df["article_no"] = range(1, len(df) + 1)
    print(f"   âœ… article_no: 1 ~ {len(df)}")

    # cluster_id ì¬í• ë‹¹ (queryë³„ë¡œ ì •ë ¬ í›„ ì¬í• ë‹¹)
    print("\nğŸ”¢ cluster_id ì¬í• ë‹¹ ì¤‘...")
    cluster_id_map = {}  # ê¸°ì¡´ cluster_id â†’ ìƒˆ cluster_id ë§¤í•‘
    global_cluster_counter = 1  # ì „ì²´ í´ëŸ¬ìŠ¤í„° ì¹´ìš´í„°

    for query in df["query"].unique():
        query_df = df[df["query"] == query]
        query_clusters = query_df[query_df["cluster_id"].notna() & (query_df["cluster_id"] != "")]["cluster_id"].unique()

        # queryë³„ í´ëŸ¬ìŠ¤í„°ë“¤ì„ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ê°€ì¥ ì˜¤ë˜ëœ ê¸°ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        cluster_oldest = {}
        for cid in query_clusters:
            cluster_articles = query_df[query_df["cluster_id"] == cid]
            oldest_date = cluster_articles["pub_datetime_parsed"].min()
            cluster_oldest[cid] = oldest_date

        # ì˜¤ë˜ëœ í´ëŸ¬ìŠ¤í„°ë¶€í„° ì •ë ¬
        sorted_clusters = sorted(cluster_oldest.items(), key=lambda x: x[1])

        # ìƒˆ cluster_id í• ë‹¹
        for old_cid, _ in sorted_clusters:
            new_cid = f"{query}_c{global_cluster_counter:05d}"
            cluster_id_map[old_cid] = new_cid
            global_cluster_counter += 1

    # cluster_id ì—…ë°ì´íŠ¸
    df["cluster_id"] = df["cluster_id"].map(lambda x: cluster_id_map.get(x, x) if x and x != "" else "")

    total_clusters = len(cluster_id_map)
    print(f"   âœ… cluster_id: {total_clusters}ê°œ í´ëŸ¬ìŠ¤í„° ì¬í• ë‹¹ ì™„ë£Œ")

    # pub_datetime_parsed ì»¬ëŸ¼ ì œê±° (ì„ì‹œ ì»¬ëŸ¼)
    df = df.drop(columns=["pub_datetime_parsed"])

    # Google Sheets ì—…ë°ì´íŠ¸
    print("\nğŸ“¤ Google Sheets ì—…ë°ì´íŠ¸ ì¤‘...")
    print(f"   - ì—…ë°ì´íŠ¸í•  í–‰: {len(df)}ê°œ")

    # í—¤ë” + ë°ì´í„° ì¤€ë¹„
    headers = df.columns.tolist()
    values = [headers] + df.fillna("").astype(str).values.tolist()

    # ê¸°ì¡´ ë°ì´í„° ì „ì²´ ì‚­ì œ í›„ ìƒˆ ë°ì´í„° ì“°ê¸°
    worksheet.clear()
    worksheet.update(values, value_input_option="USER_ENTERED")

    print(f"   âœ… Google Sheets ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("âœ… Migration ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - ì „ì²´ ê¸°ì‚¬: {len(df)}ê°œ")
    print(f"   - article_no: 1 ~ {len(df)} (pub_datetime ê¸°ì¤€ ì •ë ¬)")
    print(f"   - cluster_id: {total_clusters}ê°œ í´ëŸ¬ìŠ¤í„° ì¬í• ë‹¹")
    print(f"   - ë‚ ì§œ ë²”ìœ„: {df['pub_datetime'].min()} ~ {df['pub_datetime'].max()}")
    print("="*60)


if __name__ == "__main__":
    try:
        migrate_cumulative_ids()
    except Exception as e:
        print(f"\nâŒ Migration ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
