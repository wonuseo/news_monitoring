"""
recluster_all.py - ì „ì²´ ê¸°ì‚¬ ì¬í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤í¬ë¦½íŠ¸

Google Sheetsì˜ total_result ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹¤í–‰.

ì²˜ë¦¬ íë¦„:
  Step 1: Google Sheets total_result ë¡œë“œ
  Step 2: ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì´ˆê¸°í™” (+ ì¼ë°˜ê¸°ì‚¬ LLM ê²°ê³¼ëŠ” ê¸°ë³¸ ìœ ì§€)
  Step 3: detect_similar_articles(enable_llm_borderline=True)
  Step 4: summarize_clusters()
  Step 5: classify_press_releases() (--skip_llm_classify ì‹œ ìŠ¤í‚µ)
  Step 6: classify_llm() ì¼ë°˜ê¸°ì‚¬ ë¶„ë¥˜ (--skip_general_llm_classify ì‹œ ìŠ¤í‚µ)
  Step 7: verify_and_regroup_sources() (--skip_source_verify ì‹œ ìŠ¤í‚µ)
  Step 8: summarize_clusters() (ìƒˆ í† í”½ ê·¸ë£¹ ìš”ì•½)
  Step 9: ì¼ê´€ì„± ê²€ì¦
  Step 10: Google Sheets ì—…ë°ì´íŠ¸ + CSV ë°±ì—…

Usage:
  python scripts/recluster_all.py                     # ì „ì²´ ì¬í´ëŸ¬ìŠ¤í„°ë§
  python scripts/recluster_all.py --dry_run            # Sheets ì—…ë°ì´íŠ¸ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°
  python scripts/recluster_all.py --skip_llm_classify  # ë³´ë„ìë£Œ LLM ë¶„ë¥˜ ìŠ¤í‚µ
  python scripts/recluster_all.py --skip_general_llm_classify  # ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ ìŠ¤í‚µ
  python scripts/recluster_all.py --force_general_llm_reclassify  # ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ê°’ ì´ˆê¸°í™” í›„ ì¬ë¶„ë¥˜
  python scripts/recluster_all.py --skip_source_verify # source ê²€ì¦ ìŠ¤í‚µ
  python scripts/recluster_all.py --sheets_id ID       # íŠ¹ì • Sheets ID ì‚¬ìš©
  python scripts/recluster_all.py --chunk_size 50      # LLM ë¶„ë¥˜ ì²­í¬ í¬ê¸°
  python scripts/recluster_all.py --max_workers 3      # ë³‘ë ¬ ì›Œì»¤ ìˆ˜
"""

import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.modules.export.sheets import connect_sheets, sync_to_sheets, filter_total_result_by_date
from src.modules.processing.press_release_detector import detect_similar_articles, summarize_clusters
from src.modules.analysis.classify_press_releases import classify_press_releases
from src.modules.analysis.classify_llm import classify_llm
from src.modules.analysis.source_verifier import verify_and_regroup_sources


def load_total_result_from_sheets(spreadsheet) -> pd.DataFrame:
    """Google Sheets total_result íƒ­ì„ DataFrameìœ¼ë¡œ ë¡œë“œ."""
    try:
        worksheet = spreadsheet.worksheet("total_result")
        records = worksheet.get_all_records()
        if not records:
            print("  â„¹ï¸  total_result ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        df = pd.DataFrame(records)
        print(f"ğŸ“‚ Sheets total_resultì—ì„œ {len(df)}ê°œ ê¸°ì‚¬ ë¡œë“œ")
        return df
    except Exception as e:
        print(f"âŒ total_result ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def reset_cluster_fields(df: pd.DataFrame, preserve_existing_llm: bool = True) -> pd.DataFrame:
    """í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì´ˆê¸°í™”. í•„ìš” ì‹œ ê¸°ì¡´ ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ ê²°ê³¼ëŠ” ìœ ì§€."""
    df = df.copy()

    # í•­ìƒ ì¬ìƒì„±í•  í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ
    df["source"] = "ì¼ë°˜ê¸°ì‚¬"
    df["cluster_id"] = ""
    df["cluster_summary"] = ""

    llm_cols = [
        "brand_relevance",
        "brand_relevance_query_keywords",
        "sentiment_stage",
        "danger_level",
        "issue_category",
        "news_category",
        "news_keyword_summary",
        "classified_at",
    ]

    if preserve_existing_llm:
        # ì—†ëŠ” ì»¬ëŸ¼ë§Œ ìƒì„±í•˜ê³  ê¸°ì¡´ ê°’ì€ ìœ ì§€ (classify_llmê°€ classified_at ê¸°ì¤€ìœ¼ë¡œ ìë™ ìŠ¤í‚µ)
        for col in llm_cols:
            if col not in df.columns:
                df[col] = ""
        reused_count = int(df["classified_at"].astype(str).str.strip().ne("").sum()) if "classified_at" in df.columns else 0
        print(
            f"ğŸ”„ {len(df)}ê°œ ê¸°ì‚¬ì˜ í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì´ˆê¸°í™” ì™„ë£Œ "
            f"(ê¸°ì¡´ LLM ë¶„ë¥˜ ìœ ì§€: {reused_count}ê°œ)"
        )
    else:
        for col in llm_cols:
            df[col] = ""
        print(f"ğŸ”„ {len(df)}ê°œ ê¸°ì‚¬ì˜ í´ëŸ¬ìŠ¤í„°ë§/ì†ŒìŠ¤ íŒì • í•„ë“œ ì´ˆê¸°í™” ì™„ë£Œ (LLM ë¶„ë¥˜ ê°•ì œ ì´ˆê¸°í™”)")

    return df


def validate_consistency(df: pd.DataFrame) -> pd.DataFrame:
    """ì¼ê´€ì„± ê²€ì¦: ê°™ì€ cluster_id â†’ ê°™ì€ cluster_summary, ê°™ì€ source."""
    df = df.copy()
    fixes = 0

    if "cluster_id" not in df.columns:
        return df

    clustered = df["cluster_id"].astype(str).str.strip() != ""

    # Rule 1: source=="ì¼ë°˜ê¸°ì‚¬" â†’ cluster_id="", cluster_summary=""
    general_mask = df["source"] == "ì¼ë°˜ê¸°ì‚¬"
    r1 = (general_mask & clustered).sum()
    if r1 > 0:
        df.loc[general_mask, "cluster_id"] = ""
        df.loc[general_mask, "cluster_summary"] = ""
        fixes += r1
        print(f"  ì¼ê´€ì„± Rule 1: ì¼ë°˜ê¸°ì‚¬ {r1}ê±´ì˜ cluster_id/cluster_summary ì´ˆê¸°í™”")

    # Refresh clustered mask
    clustered = df["cluster_id"].astype(str).str.strip() != ""

    # Rule 2: ê°™ì€ cluster_id â†’ ê°™ì€ source (ì²« ë²ˆì§¸ ê°’ ì „íŒŒ)
    if clustered.any():
        for cid, cgroup in df[clustered].groupby("cluster_id"):
            sources = cgroup["source"].unique()
            if len(sources) > 1:
                target_source = cgroup["source"].iloc[0]
                mismatch = clustered & (df["cluster_id"] == cid) & (df["source"] != target_source)
                fix_count = mismatch.sum()
                if fix_count > 0:
                    df.loc[mismatch, "source"] = target_source
                    fixes += fix_count

    # Rule 3: ê°™ì€ cluster_id â†’ ê°™ì€ cluster_summary (ì²« ë¹„ì–´ìˆì§€ ì•Šì€ ê°’ ì „íŒŒ)
    if "cluster_summary" in df.columns and clustered.any():
        for cid, cgroup in df[clustered].groupby("cluster_id"):
            summaries = cgroup["cluster_summary"].dropna().astype(str)
            summaries = summaries[summaries.str.strip() != ""]
            if len(summaries) > 0:
                first_val = summaries.iloc[0]
                mismatch = clustered & (df["cluster_id"] == cid) & (df["cluster_summary"] != first_val)
                fix_count = mismatch.sum()
                if fix_count > 0:
                    df.loc[mismatch, "cluster_summary"] = first_val
                    fixes += fix_count

    if fixes > 0:
        print(f"  âœ… ì¼ê´€ì„± ê²€ì¦: ì´ {fixes}ê±´ ìˆ˜ì •")
    else:
        print(f"  âœ… ì¼ê´€ì„± ê²€ì¦: ë¶ˆì¼ì¹˜ ì—†ìŒ")

    return df


def print_summary(df: pd.DataFrame) -> None:
    """ì¬í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥."""
    print("\n" + "=" * 60)
    print("ğŸ“Š ì¬í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    print(f"  ì „ì²´ ê¸°ì‚¬: {len(df)}ê°œ")

    if "source" in df.columns:
        source_dist = df["source"].value_counts().to_dict()
        print(f"  Source ë¶„í¬:")
        for src, cnt in source_dist.items():
            print(f"    - {src}: {cnt}ê°œ")

    if "cluster_id" in df.columns:
        clustered = df[df["cluster_id"].astype(str).str.strip() != ""]
        n_clusters = clustered["cluster_id"].nunique()
        print(f"  í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}ê°œ ({len(clustered)}ê°œ ê¸°ì‚¬)")

        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
        if n_clusters > 0:
            sizes = clustered.groupby("cluster_id").size()
            print(f"    í‰ê·  í¬ê¸°: {sizes.mean():.1f}, ìµœëŒ€: {sizes.max()}, ìµœì†Œ: {sizes.min()}")

    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="ì „ì²´ ê¸°ì‚¬ ì¬í´ëŸ¬ìŠ¤í„°ë§")
    parser.add_argument("--dry_run", action="store_true", help="Sheets ì—…ë°ì´íŠ¸ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°")
    parser.add_argument("--skip_llm_classify", action="store_true", help="ë³´ë„ìë£Œ LLM ë¶„ë¥˜ ìŠ¤í‚µ")
    parser.add_argument("--skip_general_llm_classify", action="store_true", help="ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ ìŠ¤í‚µ")
    parser.add_argument("--force_general_llm_reclassify", action="store_true", help="ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ê°’ ì´ˆê¸°í™” í›„ ì¬ë¶„ë¥˜")
    parser.add_argument("--skip_source_verify", action="store_true", help="source ê²€ì¦ + ì£¼ì œ ê·¸ë£¹í™” ìŠ¤í‚µ")
    parser.add_argument("--sheets_id", type=str, default=None, help="íŠ¹ì • Google Sheets ID")
    parser.add_argument("--chunk_size", type=int, default=50, help="LLM ë¶„ë¥˜ ì²­í¬ í¬ê¸°")
    parser.add_argument("--max_workers", type=int, default=3, help="ë³‘ë ¬ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--outdir", type=str, default="data", help="CSV ë°±ì—… ë””ë ‰í† ë¦¬")
    args = parser.parse_args()

    # .env ë¡œë“œ
    env_path = PROJECT_ROOT / ".env"
    if env_path.exists():
        load_dotenv(env_path)

    openai_key = os.getenv("OPENAI_API_KEY")
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH")
    sheet_id = args.sheets_id or os.getenv("GOOGLE_SHEET_ID")

    if not creds_path or not sheet_id:
        print("âŒ GOOGLE_SHEETS_CREDENTIALS_PATH ë˜ëŠ” GOOGLE_SHEET_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    if not openai_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # ì „ì²´ ì§„í–‰ë¥  ë°” (ì´ 10ê°œ Step)
    total_steps = 10
    print("\n" + "=" * 60)
    print("ğŸ”„ ì „ì²´ ê¸°ì‚¬ ì¬í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    print("=" * 60)
    overall_pbar = tqdm(total=total_steps, desc="ì „ì²´ ì§„í–‰", unit="step", position=0)

    # â”€â”€â”€ Step 1: Google Sheets total_result ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“‹ Step 1/10: Google Sheets total_result ë¡œë“œ")
    spreadsheet = connect_sheets(creds_path, sheet_id)
    if spreadsheet is None:
        print("âŒ Google Sheets ì—°ê²° ì‹¤íŒ¨")
        sys.exit(1)

    df = load_total_result_from_sheets(spreadsheet)
    if df.empty:
        print("âŒ total_resultê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)

    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = {"title", "description", "link", "query"}
    missing = required_cols - set(df.columns)
    if missing:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        sys.exit(1)
    overall_pbar.update(1)

    # â”€â”€â”€ Step 2: ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì´ˆê¸°í™” (+ LLM ì¬ì‚¬ìš© ì˜µì…˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.force_general_llm_reclassify:
        print("\nğŸ“‹ Step 2/10: ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§/ì†ŒìŠ¤íŒì • í•„ë“œ ì´ˆê¸°í™” (ì¼ë°˜ê¸°ì‚¬ LLM ê°•ì œ ì¬ë¶„ë¥˜)")
    else:
        print("\nğŸ“‹ Step 2/10: ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§ í•„ë“œ ì´ˆê¸°í™” (ê¸°ì¡´ ì¼ë°˜ê¸°ì‚¬ LLM ê²°ê³¼ ì¬ì‚¬ìš©)")
    df = reset_cluster_fields(df, preserve_existing_llm=not args.force_general_llm_reclassify)
    overall_pbar.update(1)

    # â”€â”€â”€ Step 3: TF-IDF + LLM ê²½ê³„ì„  í´ëŸ¬ìŠ¤í„°ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“‹ Step 3/10: TF-IDF ìœ ì‚¬ë„ + LLM ê²½ê³„ì„  í´ëŸ¬ìŠ¤í„°ë§")
    df = detect_similar_articles(
        df,
        spreadsheet=None,  # cumulative numbering ì—†ì´ 1ë¶€í„° ì‹œì‘
        enable_llm_borderline=True,
        openai_key=openai_key,
        group_our_brands=True,
    )
    overall_pbar.update(1)

    # â”€â”€â”€ Step 4: í´ëŸ¬ìŠ¤í„° ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“‹ Step 4/10: í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìƒì„±")
    df = summarize_clusters(df, openai_key=openai_key)
    overall_pbar.update(1)

    # â”€â”€â”€ Step 5: ë³´ë„ìë£Œ LLM ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.skip_llm_classify:
        print("\nğŸ“‹ Step 5/10: ë³´ë„ìë£Œ ëŒ€í‘œ ê¸°ì‚¬ LLM ë¶„ë¥˜")
        df, pr_stats = classify_press_releases(
            df,
            openai_key=openai_key,
            chunk_size=args.chunk_size,
            max_workers=args.max_workers,
        )
        print(f"  PR ë¶„ë¥˜ í†µê³„: {pr_stats}")
    else:
        print("\nğŸ“‹ Step 5/10: ë³´ë„ìë£Œ LLM ë¶„ë¥˜ ìŠ¤í‚µ (--skip_llm_classify)")
    overall_pbar.update(1)

    # â”€â”€â”€ Step 6: ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.skip_general_llm_classify:
        print("\nğŸ“‹ Step 6/10: ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜")
        df, llm_stats = classify_llm(
            df,
            openai_key=openai_key,
            chunk_size=args.chunk_size,
            dry_run=False,  # recluster_allì˜ dry_runì€ Sheets ì—…ë°ì´íŠ¸ë§Œ ì œì–´
            max_competitor_classify=0,
            max_workers=args.max_workers,
        )
        print(f"  ì¼ë°˜ê¸°ì‚¬ ë¶„ë¥˜ í†µê³„: {llm_stats}")
    else:
        print("\nğŸ“‹ Step 6/10: ì¼ë°˜ê¸°ì‚¬ LLM ë¶„ë¥˜ ìŠ¤í‚µ (--skip_general_llm_classify)")
    overall_pbar.update(1)

    # â”€â”€â”€ Step 7: Source ê²€ì¦ + ì£¼ì œ ê·¸ë£¹í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.skip_source_verify:
        print("\nğŸ“‹ Step 7/10: Source ê²€ì¦ + ì£¼ì œ ê·¸ë£¹í™”")
        df, sv_stats = verify_and_regroup_sources(df, openai_key=openai_key)
        print(f"  SV í†µê³„: {sv_stats}")
    else:
        print("\nğŸ“‹ Step 7/10: Source ê²€ì¦ ìŠ¤í‚µ (--skip_source_verify)")
    overall_pbar.update(1)

    # â”€â”€â”€ Step 8: ìƒˆ í† í”½ ê·¸ë£¹ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“‹ Step 8/10: ìƒˆ í† í”½ ê·¸ë£¹ ìš”ì•½ ìƒì„±")
    df = summarize_clusters(df, openai_key=openai_key)
    overall_pbar.update(1)

    # â”€â”€â”€ Step 9: ì¼ê´€ì„± ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“‹ Step 9/10: ì¼ê´€ì„± ê²€ì¦")
    df = validate_consistency(df)
    overall_pbar.update(1)

    # ê²°ê³¼ ìš”ì•½
    print_summary(df)

    # â”€â”€â”€ Step 10: Google Sheets ì—…ë°ì´íŠ¸ + CSV ë°±ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.dry_run:
        print("\nğŸƒ Step 10/10: DRY RUN ëª¨ë“œ - Sheets ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        print("\nğŸ“‹ Step 10/10: Google Sheets ì—…ë°ì´íŠ¸")
        df_upload = filter_total_result_by_date(df)
        sync_result = sync_to_sheets(
            df_upload,
            spreadsheet,
            "total_result",
            force_update_existing=True,
        )
        print(f"  Sheets ì—…ë°ì´íŠ¸ ê²°ê³¼: {sync_result}")
    overall_pbar.update(1)

    # CSV ë°±ì—…
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = outdir / f"recluster_result_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ CSV ë°±ì—… ì €ì¥: {csv_path}")

    overall_pbar.close()
    print("\nâœ… ì¬í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
