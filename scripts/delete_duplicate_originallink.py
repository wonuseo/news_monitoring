#!/usr/bin/env python3
"""Delete duplicate rows in Google Sheets based on originallink column."""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

import argparse
import os
import time
from typing import Iterable, List, Optional, Tuple

from dotenv import load_dotenv

from src.modules.export.sheets import connect_sheets

ORIGINALLINK_COLUMN_PRIORITY = [
    "originallink",
    "original_link",
    "original link",
]
BRAND_RELEVANCE_COLUMN_PRIORITY = [
    "brand_relevance",
    "brand relevance",
]
MAX_ERROR_MESSAGES = 3


def find_column_index(headers: Iterable[str], candidates: List[str]) -> Optional[int]:
    normalized_headers = [
        header.strip().lower() if isinstance(header, str) else "" for header in headers
    ]
    for candidate in candidates:
        candidate_lower = candidate.lower()
        for idx, header in enumerate(normalized_headers):
            if header == candidate_lower:
                return idx
    return None


def normalize_link(value: str) -> str:
    return value.strip()


def has_non_empty_cell(row: List[str], col_idx: Optional[int]) -> bool:
    if col_idx is None or len(row) <= col_idx:
        return False
    return bool(str(row[col_idx]).strip())


def pick_keeper(group_rows: List[Tuple[int, List[str]]], brand_col_idx: Optional[int], keep: str) -> int:
    brand_rows = [
        row_idx for row_idx, row in group_rows if has_non_empty_cell(row, brand_col_idx)
    ]
    if brand_rows:
        return brand_rows[0] if keep == "first" else brand_rows[-1]
    return group_rows[0][0] if keep == "first" else group_rows[-1][0]


def find_duplicate_rows(
    data_rows: List[List[str]], link_col_idx: int, brand_col_idx: Optional[int], keep: str
) -> Tuple[List[int], int, int]:
    grouped: dict[str, List[Tuple[int, List[str]]]] = {}
    for row_idx, row in enumerate(data_rows, start=2):
        link = normalize_link(row[link_col_idx]) if len(row) > link_col_idx else ""
        if not link:
            continue
        grouped.setdefault(link, []).append((row_idx, row))

    rows_to_delete: List[int] = []
    duplicate_count = 0
    kept_by_brand = 0

    for group_rows in grouped.values():
        if len(group_rows) <= 1:
            continue

        keeper = pick_keeper(group_rows, brand_col_idx, keep)
        if any(has_non_empty_cell(row, brand_col_idx) for _, row in group_rows):
            kept_by_brand += 1

        for row_idx, _ in group_rows:
            if row_idx != keeper:
                rows_to_delete.append(row_idx)
                duplicate_count += 1

    return rows_to_delete, duplicate_count, kept_by_brand


def make_ranges(row_indices: List[int]) -> List[Tuple[int, int]]:
    if not row_indices:
        return []

    sorted_rows = sorted(row_indices)
    ranges: List[Tuple[int, int]] = []
    start = sorted_rows[0]
    end = start

    for row in sorted_rows[1:]:
        if row == end + 1:
            end = row
        else:
            ranges.append((start, end))
            start = row
            end = row

    ranges.append((start, end))
    return ranges


def delete_rows_with_ranges(worksheet, row_indices: List[int], dry_run: bool) -> Tuple[int, bool]:
    if not row_indices:
        print("  âœ… ì¤‘ë³µ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, False

    print(f"  ğŸ” ì¤‘ë³µ í–‰ {len(row_indices)}ê°œ ë°œê²¬")
    if dry_run:
        print("  ğŸ§ª dry run ëª¨ë“œ, ì‹¤ì œ ì‚­ì œëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return len(row_indices), False

    sheet_id = getattr(worksheet, "_properties", {}).get("sheetId") or getattr(
        worksheet, "id", None
    )
    spreadsheet = getattr(worksheet, "parent", None) or getattr(
        worksheet, "spreadsheet", None
    )

    deleted = 0
    error_count = 0

    if sheet_id is None or spreadsheet is None:
        print("  âš ï¸ batch deleteë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë‹¨ì¼ ì‚­ì œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        for row_idx in sorted(row_indices, reverse=True):
            try:
                worksheet.delete_rows(row_idx)
                deleted += 1
                time.sleep(0.2)
            except Exception as exc:
                error_count += 1
                print(f"    âš ï¸ ì‚­ì œ ì‹¤íŒ¨: row {row_idx} -> {exc}")
                if error_count >= MAX_ERROR_MESSAGES:
                    print("    âš ï¸ ì˜¤ë¥˜ê°€ 3íšŒ ë°œìƒí•˜ì—¬ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    return deleted, True
        return deleted, False

    for start, end in reversed(make_ranges(row_indices)):
        if error_count >= MAX_ERROR_MESSAGES:
            print("    âš ï¸ ì˜¤ë¥˜ê°€ 3íšŒ ë°œìƒí•˜ì—¬ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        request = {
            "requests": [
                {
                    "deleteDimension": {
                        "range": {
                            "sheetId": sheet_id,
                            "dimension": "ROWS",
                            "startIndex": start - 1,
                            "endIndex": end,
                        }
                    }
                }
            ]
        }
        row_count = end - start + 1
        try:
            spreadsheet.batch_update(request)
            deleted += row_count
            print(f"    ğŸ—‘ï¸ {start}-{end} ì‚­ì œ ì™„ë£Œ ({row_count}í–‰)")
            time.sleep(0.2)
        except Exception as exc:
            error_count += 1
            print(f"    âš ï¸ {start}-{end} ì‚­ì œ ì‹¤íŒ¨: {exc}")
            if error_count >= MAX_ERROR_MESSAGES:
                print("    âš ï¸ ì˜¤ë¥˜ê°€ 3íšŒ ë°œìƒí•˜ì—¬ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return deleted, True

    return deleted, error_count >= MAX_ERROR_MESSAGES


def cleanup_duplicates(worksheet, keep: str, dry_run: bool) -> Tuple[int, bool]:
    all_rows = worksheet.get_all_values()
    if not all_rows:
        print(f"  â„¹ï¸  '{worksheet.title}' ì‹œíŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return 0, False

    headers = all_rows[0]
    link_col_idx = find_column_index(headers, ORIGINALLINK_COLUMN_PRIORITY)
    if link_col_idx is None:
        print(
            "  âš ï¸  originallink ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"(ì‹œë„: {', '.join(ORIGINALLINK_COLUMN_PRIORITY)})"
        )
        return 0, False

    brand_col_idx = find_column_index(headers, BRAND_RELEVANCE_COLUMN_PRIORITY)
    if brand_col_idx is None:
        print(
            "  âš ï¸  brand_relevance ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
            "ì¤‘ë³µ ë‚´ ë³´ì¡´ ìš°ì„ ìˆœìœ„ëŠ” í–‰ ìˆœì„œë¡œë§Œ ê²°ì •ë©ë‹ˆë‹¤."
        )

    data_rows = all_rows[1:]
    if not data_rows:
        print(f"  â„¹ï¸  '{worksheet.title}'ì— ë°ì´í„° í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, False

    row_indices, duplicate_count, kept_by_brand = find_duplicate_rows(
        data_rows, link_col_idx, brand_col_idx, keep
    )
    if duplicate_count == 0:
        print("  âœ… originallink ê¸°ì¤€ ì¤‘ë³µì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, False

    print(
        f"  - ê¸°ì¤€ ì»¬ëŸ¼: '{headers[link_col_idx]}' / keep='{keep}' / duplicates={duplicate_count}"
    )
    if kept_by_brand:
        print(f"  - brand_relevance ë³´ì¡´ ì ìš© ê·¸ë£¹: {kept_by_brand}ê°œ")
    deleted, aborted = delete_rows_with_ranges(worksheet, row_indices, dry_run)
    if deleted:
        print(f"  âœ… '{worksheet.title}'ì—ì„œ {deleted}ê°œ í–‰ ì²˜ë¦¬ ì™„ë£Œ")
    return deleted, aborted


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Google Sheetsì—ì„œ originallink ê¸°ì¤€ ì¤‘ë³µ í–‰ì„ ì‚­ì œí•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--worksheets",
        default="total_result",
        help="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì›Œí¬ì‹œíŠ¸ ëª©ë¡ (ê¸°ë³¸: total_result).",
    )
    parser.add_argument(
        "--keep",
        choices=["first", "last"],
        default="first",
        help="brand_relevance ìš°ì„  í›„ ë™ë¥ ì¼ ë•Œ ì–´ë–¤ í–‰ì„ ë‚¨ê¸¸ì§€ ì„ íƒ (ê¸°ë³¸: first).",
    )
    parser.add_argument(
        "--credentials",
        default=None,
        help="ì„œë¹„ìŠ¤ ê³„ì • JSON ê²½ë¡œ (.envì˜ GOOGLE_SHEETS_CREDENTIALS_PATH ìš°ì„ ).",
    )
    parser.add_argument(
        "--sheet-id",
        default=None,
        help="Google Sheets ID (.envì˜ GOOGLE_SHEET_ID ëŒ€ì‹  ì‚¬ìš©).",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‚­ì œ ì—†ì´ ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
    )
    args = parser.parse_args()

    load_dotenv()

    creds_path = (
        args.credentials
        or os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH")
        or "service-account.json"
    )
    sheet_id = args.sheet_id or os.getenv("GOOGLE_SHEET_ID")
    if not sheet_id:
        parser.error("Google Sheets ID (.env ë˜ëŠ” --sheet-id) ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    worksheet_names = [name.strip() for name in args.worksheets.split(",") if name.strip()]

    print("originallink ì¤‘ë³µ ì œê±° ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print(f"  - ëŒ€ìƒ ì›Œí¬ì‹œíŠ¸: {', '.join(worksheet_names)}")
    print(f"  - keep ì •ì±…: brand_relevance ìš°ì„  + {args.keep} tie-break")
    print(f"  - dry run: {'ì˜ˆ' if args.dry_run else 'ì•„ë‹ˆì˜¤'}")

    spreadsheet = connect_sheets(creds_path, sheet_id)
    if not spreadsheet:
        raise SystemExit(1)

    total_deleted = 0
    aborted = False
    for sheet_name in worksheet_names:
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
        except Exception as exc:
            print(f"  âš ï¸ ì›Œí¬ì‹œíŠ¸ '{sheet_name}'ì„(ë¥¼) ì—´ ìˆ˜ ì—†ìŒ: {exc}")
            continue

        deleted, sheet_aborted = cleanup_duplicates(worksheet, args.keep, args.dry_run)
        total_deleted += deleted
        if sheet_aborted:
            aborted = True
            break

    if args.dry_run:
        print(f"ì „ì²´ ì˜ˆìƒ ì‚­ì œ: {total_deleted}ê°œ")
    else:
        print(f"ì „ì²´ ì‚­ì œ ì™„ë£Œ: {total_deleted}ê°œ")
    if aborted:
        print("âš ï¸ ì˜¤ë¥˜ íšŸìˆ˜ í•œë„ ë„ë‹¬ë¡œ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
