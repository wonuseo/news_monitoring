#!/usr/bin/env python3
"""
fix_incorrect_clusters.py - ì˜ëª» ë¬¶ì¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ì •

íŠ¹ì • cluster_idë¥¼ í•´ì²´í•˜ì—¬ ì¼ë°˜ê¸°ì‚¬ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  python fix_incorrect_clusters.py --cluster_ids "ì¡°ì„ í˜¸í…”_t00001,ë¡¯ë°í˜¸í…”_t00001"
  python fix_incorrect_clusters.py --cluster_ids "ì¡°ì„ í˜¸í…”_t00001" --dry_run
"""

import os
import argparse
import pandas as pd
from dotenv import load_dotenv

from src.modules.export.sheets import connect_sheets, sync_to_sheets


def fix_clusters(df: pd.DataFrame, cluster_ids: list) -> pd.DataFrame:
    """
    ì§€ì •ëœ cluster_idë¥¼ í•´ì²´í•˜ì—¬ ì¼ë°˜ê¸°ì‚¬ë¡œ ë˜ëŒë¦¼.

    Args:
        df: ì „ì²´ ë°ì´í„° DataFrame
        cluster_ids: í•´ì²´í•  cluster_id ë¦¬ìŠ¤íŠ¸

    Returns:
        ìˆ˜ì •ëœ DataFrame
    """
    df = df.copy()

    for cluster_id in cluster_ids:
        mask = df["cluster_id"] == cluster_id
        affected_count = mask.sum()

        if affected_count == 0:
            print(f"  âš ï¸  '{cluster_id}' í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # ê¸°ì‚¬ ì •ë³´ ì¶œë ¥
        print(f"\n  ğŸ“Œ '{cluster_id}' í´ëŸ¬ìŠ¤í„° í•´ì²´ ({affected_count}ê°œ ê¸°ì‚¬)")
        for idx, row in df[mask].iterrows():
            print(f"    - {row.get('title', 'N/A')[:60]}...")

        # cluster_id ì œê±°, sourceë¥¼ ì¼ë°˜ê¸°ì‚¬ë¡œ ë³€ê²½
        df.loc[mask, "cluster_id"] = ""
        df.loc[mask, "source"] = "ì¼ë°˜ê¸°ì‚¬"
        df.loc[mask, "press_release_group"] = ""

    return df


def main():
    parser = argparse.ArgumentParser(description="ì˜ëª» ë¬¶ì¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ì •")
    parser.add_argument("--cluster_ids", type=str, required=True,
                        help="í•´ì²´í•  cluster_id (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 'ì¡°ì„ í˜¸í…”_t00001,ë¡¯ë°í˜¸í…”_t00001')")
    parser.add_argument("--dry_run", action="store_true",
                        help="ë¯¸ë¦¬ë³´ê¸°ë§Œ (Sheets ì—…ë°ì´íŠ¸ ì•ˆ í•¨)")
    parser.add_argument("--sheets_id", type=str, default=None,
                        help="Google Sheets ID (.env ëŒ€ì‹  ì‚¬ìš©)")
    args = parser.parse_args()

    load_dotenv()

    # cluster_ids íŒŒì‹±
    cluster_ids = [cid.strip() for cid in args.cluster_ids.split(",")]
    print(f"\ní•´ì²´ ëŒ€ìƒ í´ëŸ¬ìŠ¤í„°: {cluster_ids}")

    # Google Sheets ì—°ê²°
    creds_path = os.getenv("GOOGLE_SHEETS_CREDENTIALS_PATH", "service-account.json")
    sheet_id = args.sheets_id or os.getenv("GOOGLE_SHEET_ID")

    if not os.path.exists(creds_path) or not sheet_id:
        print("\nâŒ Google Sheets ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("  .envì— GOOGLE_SHEETS_CREDENTIALS_PATH, GOOGLE_SHEET_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return

    spreadsheet = connect_sheets(creds_path, sheet_id)
    if not spreadsheet:
        return

    # total_result ë¡œë“œ
    print("\nì´ ê²°ê³¼ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        worksheet = spreadsheet.worksheet("total_result")
        records = worksheet.get_all_records()
    except Exception as e:
        print(f"âŒ total_result ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    if not records:
        print("âŒ total_result ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(records)
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")

    # ë³€ê²½ ì „ ìƒíƒœ
    print(f"\në³€ê²½ ì „ source ë¶„í¬:")
    print(df["source"].value_counts().to_string())

    # í´ëŸ¬ìŠ¤í„° ìˆ˜ì •
    print("\n" + "=" * 80)
    df_fixed = fix_clusters(df, cluster_ids)
    print("=" * 80)

    # ë³€ê²½ í›„ ìƒíƒœ
    print(f"\në³€ê²½ í›„ source ë¶„í¬:")
    print(df_fixed["source"].value_counts().to_string())

    # Sheets ì—…ë°ì´íŠ¸
    if args.dry_run:
        print("\n[DRY RUN] Sheets ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print("\nGoogle Sheets ì—…ë°ì´íŠ¸ ì¤‘...")
    result = sync_to_sheets(
        df_fixed, spreadsheet, "total_result",
        force_update_existing=True,
    )
    print(f"âœ… ì™„ë£Œ: ì¶”ê°€ {result.get('added', 0)}ê°œ, "
          f"ì—…ë°ì´íŠ¸ {result.get('updated', 0)}ê°œ, "
          f"ê±´ë„ˆëœ€ {result.get('skipped', 0)}ê°œ")

    # ë¡œì»¬ CSV ë°±ì—…ë„ ì—…ë°ì´íŠ¸
    csv_path = "../data/result.csv"
    if os.path.exists(csv_path):
        print(f"\në¡œì»¬ CSV ë°±ì—… ì—…ë°ì´íŠ¸: {csv_path}")
        df_csv = pd.read_csv(csv_path, encoding="utf-8-sig")

        # source, cluster_id, press_release_group ì—…ë°ì´íŠ¸ (link ê¸°ì¤€ merge)
        update_cols = ["link", "source", "cluster_id", "press_release_group"]
        df_updates = df_fixed[update_cols].copy()

        df_csv = df_csv.drop(columns=["source", "cluster_id", "press_release_group"], errors="ignore")
        df_csv = df_csv.merge(df_updates, on="link", how="left")
        df_csv.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(df_csv)}ê°œ ê¸°ì‚¬")

    print("\nâœ… í´ëŸ¬ìŠ¤í„° ìˆ˜ì • ì™„ë£Œ!")


if __name__ == "__main__":
    main()
